{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding-Based Zero-Shot Classification (v2 - Improved)\n",
    "\n",
    "**Approach**: Multi-prototype semantic similarity using sentence embeddings\n",
    "\n",
    "This approach improves upon rule-based matching by:\n",
    "1. Creating **rich input text** from CV positions (title + company + description)\n",
    "2. Embedding all texts using sentence-transformers (multilingual model)\n",
    "3. Creating **multiple prototypes per label** via K-Means clustering (captures diversity)\n",
    "4. Computing cosine similarity between input and all prototypes\n",
    "5. Predicting label with highest similarity to any of its prototypes\n",
    "6. Storing top-k similar labels for explainability\n",
    "\n",
    "**Key Improvements over v1**:\n",
    "- **More context**: Uses position description (first 200 chars) for richer embeddings\n",
    "- **Multiple prototypes**: 3 sub-prototypes per label capture semantic diversity\n",
    "- **Zero-shot**: No training, pure similarity-based classification\n",
    "- **Multilingual**: Handles German/French/English seamlessly\n",
    "- **Explainability**: Top-k most similar labels with confidence scores\n",
    "\n",
    "**Model**: `paraphrase-multilingual-MiniLM-L12-v2` (384-dimensional embeddings)\n",
    "\n",
    "**Training Data**: Lookup tables (10,145 dept + 9,428 seniority examples) → 11×3 + 6×3 prototypes  \n",
    "**Validation Data**: 478 annotated LinkedIn CVs (loaded only for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Import data loaders and models\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data.loader import load_label_lists, load_inference_dataset, load_evaluation_dataset\n",
    "from src.models.embedding_classifier import EmbeddingClassifier, create_domain_classifier, create_seniority_classifier\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "RESULTS_DIR = Path('./results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data (Lookup Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding fix...\n",
      "  Deduplication: 10145 -> 10145 (removed 0 duplicates)\n",
      "  Deduplication: 9428 -> 9428 (removed 0 duplicates)\n",
      "Department lookup: 10,145 examples\n",
      "Seniority lookup:  9,428 examples\n",
      "\n",
      "Unique departments: 11\n",
      "Unique seniority levels: 5\n"
     ]
    }
   ],
   "source": [
    "# Load lookup tables\n",
    "dept_df, sen_df = load_label_lists(DATA_DIR, max_per_class=None)\n",
    "\n",
    "print(f\"Department lookup: {len(dept_df):,} examples\")\n",
    "print(f\"Seniority lookup:  {len(sen_df):,} examples\")\n",
    "print(f\"\\nUnique departments: {dept_df['label'].nunique()}\")\n",
    "print(f\"Unique seniority levels: {sen_df['label'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Embedding Classifier\n",
    "\n",
    "**Approach**: Pure zero-shot classification using semantic similarity\n",
    "\n",
    "**Key Steps**:\n",
    "1. Create consistent input text from CV positions (robust handling of missing values)\n",
    "2. Generate embeddings for input texts and lookup table entries\n",
    "3. Aggregate lookup embeddings per label into prototypes (mean)\n",
    "4. Compute cosine similarity between input and all label prototypes\n",
    "5. Select label with highest similarity (optional: threshold-based fallback)\n",
    "6. Store top-k similarities for explainability\n",
    "\n",
    "**Implementation Details**:\n",
    "- Model: `paraphrase-multilingual-MiniLM-L12-v2` (multilingual support)\n",
    "- Caching: Store prototypes locally for faster re-runs\n",
    "- Evaluation: Accuracy, macro-F1, weighted-F1, precision, recall\n",
    "- Missing \"Professional\" seniority: Added manually (found in EDA, missing in CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Create RICH input text from position data with more context\n",
    "def create_input_text(row):\n",
    "    parts = []\n",
    "    \n",
    "    # 1. Title (most important)\n",
    "    if 'title' in row and pd.notna(row['title']) and row['title'].strip():\n",
    "        parts.append(row['title'])\n",
    "    \n",
    "    # 2. Company context\n",
    "    if 'company' in row and pd.notna(row['company']) and row['company'].strip():\n",
    "        parts.append(f\"at {row['company']}\")\n",
    "    \n",
    "    # 3. Position description (NEW: first 200 chars for additional context)\n",
    "    # This is the KEY improvement - captures job responsibilities/skills\n",
    "    if 'text' in row and pd.notna(row['text']) and row['text'].strip():\n",
    "        # 'text' column often contains position description\n",
    "        desc = row['text'][:200].strip()  # Limit to avoid noise\n",
    "        if desc and desc not in ' '.join(parts):  # Avoid duplication\n",
    "            parts.append(desc)\n",
    "    \n",
    "    if not parts:\n",
    "        return \"Unknown Position\"\n",
    "    \n",
    "    return ' '.join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Multiple Prototypes per Label with K-Means Clustering\n",
    "\n",
    "**New Approach**: Instead of 1 mean prototype per label, create **3 sub-prototypes** via K-Means.\n",
    "This captures semantic diversity within each label (e.g., \"Software Engineer\" vs \"Backend Developer\").\n",
    "\n",
    "Prototypes are cached to disk for faster re-runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Cache directory: results/embedding_cache\n",
      "Department cache: False\n",
      "Seniority cache: False\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Cache directory\n",
    "CACHE_DIR = RESULTS_DIR / 'embedding_cache'\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DEPT_CACHE = CACHE_DIR / 'dept_prototypes.pkl'\n",
    "SEN_CACHE = CACHE_DIR / 'sen_prototypes.pkl'\n",
    "\n",
    "# Model for embeddings\n",
    "MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")\n",
    "print(f\"Department cache: {DEPT_CACHE.exists()}\")\n",
    "print(f\"Seniority cache: {SEN_CACHE.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache deleted\n",
      "Cache directory ready: results/embedding_cache\n"
     ]
    }
   ],
   "source": [
    "# Force cache rebuild to ensure latest changes are applied\n",
    "import shutil\n",
    "\n",
    "if CACHE_DIR.exists():\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "    print(\"Cache deleted\")\n",
    "    \n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Cache directory ready: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 3 prototypes per label (model: paraphrase-multilingual-MiniLM-L12-v2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c21112872c40b59058e5ec7f42daec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54334a7f2c24ec4a506f3d11b47f15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55d9c569623465f8b1db76cfc1a1e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec55dc02bf64b0ab407f3d522ae68fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fc8cc00837437982424b657398b75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf9ac9cca38400c9371b47042d5bcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e78f7124174268b38249ee9c9d3046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/526 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c601da175bc64cac9ab1a3a0681c7817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38453776dd4c4e2c9baeed42827eb61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a0f41f890b482e86e33b8207a26631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Marketing: 4295 examples → 3 prototypes\n",
      "  Project Management: 201 examples → 3 prototypes\n",
      "  Administrative: 83 examples → 3 prototypes\n",
      "  Business Development: 620 examples → 3 prototypes\n",
      "  Consulting: 167 examples → 3 prototypes\n",
      "  Human Resources: 31 examples → 3 prototypes\n",
      "  Information Technology: 1305 examples → 3 prototypes\n",
      "  Other: 42 examples → 3 prototypes\n",
      "  Purchasing: 40 examples → 3 prototypes\n",
      "  Sales: 3328 examples → 3 prototypes\n",
      "  Customer Support: 33 examples → 3 prototypes\n",
      "Saved to cache: dept_prototypes.pkl\n",
      "\n",
      "Department prototypes: 11 labels\n"
     ]
    }
   ],
   "source": [
    "def build_label_prototypes(label_df, cache_path, model_name=MODEL_NAME, n_prototypes=3, force_rebuild=False):\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # Check cache\n",
    "    if cache_path.exists() and not force_rebuild:\n",
    "        print(f\"Loading prototypes from cache: {cache_path.name}\")\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    print(f\"Building {n_prototypes} prototypes per label (model: {model_name})...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    prototypes = {}\n",
    "    labels = label_df['label'].unique()\n",
    "    \n",
    "    for label in labels:\n",
    "        # Get all examples for this label\n",
    "        examples = label_df[label_df['label'] == label]['text'].tolist()\n",
    "        \n",
    "        # Embed all examples\n",
    "        embeddings = model.encode(examples, convert_to_numpy=True, normalize_embeddings=True, \n",
    "                                batch_size=32, show_progress_bar=False)\n",
    "        \n",
    "        # K-Means clustering to find n_prototypes centers\n",
    "        if len(examples) >= n_prototypes:\n",
    "            kmeans = KMeans(n_clusters=n_prototypes, random_state=42, n_init=10)\n",
    "            kmeans.fit(embeddings)\n",
    "            # Use cluster centers as prototypes\n",
    "            label_prototypes = []\n",
    "            for center in kmeans.cluster_centers_:\n",
    "                # Normalize each prototype\n",
    "                center_normalized = center / np.linalg.norm(center)\n",
    "                label_prototypes.append(center_normalized)\n",
    "        else:\n",
    "            # Fallback: if too few examples, use mean + add slight variations\n",
    "            mean_prototype = np.mean(embeddings, axis=0)\n",
    "            mean_prototype = mean_prototype / np.linalg.norm(mean_prototype)\n",
    "            label_prototypes = [mean_prototype]  # Just use single prototype\n",
    "        \n",
    "        prototypes[label] = label_prototypes\n",
    "        print(f\"  {label}: {len(examples)} examples → {len(label_prototypes)} prototypes\")\n",
    "    \n",
    "    # Save to cache\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(prototypes, f)\n",
    "    print(f\"Saved to cache: {cache_path.name}\\n\")\n",
    "    \n",
    "    return prototypes\n",
    "\n",
    "\n",
    "# Build department prototypes\n",
    "dept_prototypes = build_label_prototypes(dept_df, DEPT_CACHE)\n",
    "\n",
    "print(f\"Department prototypes: {len(dept_prototypes)} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 3 prototypes per label (model: paraphrase-multilingual-MiniLM-L12-v2)...\n",
      "  Junior: 409 examples → 3 prototypes\n",
      "  Senior: 3733 examples → 3 prototypes\n",
      "  Lead: 3546 examples → 3 prototypes\n",
      "  Management: 756 examples → 3 prototypes\n",
      "  Director: 984 examples → 3 prototypes\n",
      "  Professional: 3 examples → 3 prototypes\n",
      "Saved to cache: sen_prototypes.pkl\n",
      "\n",
      "Seniority prototypes: 6 labels\n",
      "Labels: ['Junior', 'Senior', 'Lead', 'Management', 'Director', 'Professional']\n"
     ]
    }
   ],
   "source": [
    "# Build seniority prototypes\n",
    "# CRITICAL: Missing \"Professional\" label (found in EDA, missing in CSV)\n",
    "ALL_SENIORITY_LABELS = ['Junior', 'Professional', 'Senior', 'Lead', 'Management', 'Director']\n",
    "\n",
    "# For missing \"Professional\", create synthetic examples\n",
    "sen_df_extended = sen_df.copy()\n",
    "professional_examples = pd.DataFrame({\n",
    "    'text': ['Professional', 'Professional Position', 'Professional Role'],\n",
    "    'label': ['Professional'] * 3\n",
    "})\n",
    "sen_df_extended = pd.concat([sen_df_extended, professional_examples], ignore_index=True)\n",
    "\n",
    "sen_prototypes = build_label_prototypes(sen_df_extended, SEN_CACHE)\n",
    "\n",
    "print(f\"Seniority prototypes: {len(sen_prototypes)} labels\")\n",
    "print(f\"Labels: {list(sen_prototypes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation dataset...\n",
      "Evaluation samples: 478\n",
      "Columns: ['cv_id', 'title', 'company', 'text', 'department', 'seniority']\n",
      "\n",
      "Sample data:\n",
      "                     title             company              department  \\\n",
      "0                Prokurist   Depot4Design GmbH                   Other   \n",
      "1      Solutions Architect  Computer Solutions  Information Technology   \n",
      "2  Medizintechnik Beratung           Udo Weber              Consulting   \n",
      "\n",
      "      seniority  \n",
      "0    Management  \n",
      "1  Professional  \n",
      "2  Professional  \n"
     ]
    }
   ],
   "source": [
    "# Load evaluation dataset\n",
    "print(\"Loading evaluation dataset...\")\n",
    "eval_df = load_evaluation_dataset(DATA_DIR)\n",
    "\n",
    "print(f\"Evaluation samples: {len(eval_df)}\")\n",
    "print(f\"Columns: {list(eval_df.columns)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(eval_df[['title', 'company', 'department', 'seniority']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_topk(input_text, prototypes, model, top_k=3):\n",
    "    # Embed input\n",
    "    input_embedding = model.encode(input_text, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Compute similarity to each label (max over all prototypes)\n",
    "    label_similarities = {}\n",
    "    for label, label_prototypes in prototypes.items():\n",
    "        # Compute similarity to all prototypes for this label\n",
    "        similarities = [np.dot(input_embedding, proto) for proto in label_prototypes]\n",
    "        # Take MAX similarity across prototypes\n",
    "        max_sim = max(similarities)\n",
    "        label_similarities[label] = max_sim\n",
    "    \n",
    "    # Sort by similarity\n",
    "    sorted_labels = sorted(label_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Top-k results\n",
    "    topk_dict = dict(sorted_labels[:top_k])\n",
    "    top_pred = sorted_labels[0][0]\n",
    "    top_conf = sorted_labels[0][1]\n",
    "    \n",
    "    return top_pred, top_conf, topk_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev/Test Split\n",
    "\n",
    "Split annotated data for threshold tuning (dev) and final evaluation (test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotated samples: 478\n",
      "Dev set (threshold tuning): 334 samples (69.9%)\n",
      "Test set (final evaluation): 144 samples (30.1%)\n",
      "\n",
      "Dev set department distribution:\n",
      "department\n",
      "Administrative              6\n",
      "Business Development       12\n",
      "Consulting                 20\n",
      "Customer Support            4\n",
      "Human Resources            10\n",
      "Information Technology     38\n",
      "Marketing                  13\n",
      "Other                     175\n",
      "Project Management         22\n",
      "Purchasing                  8\n",
      "Sales                      26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dev set seniority distribution:\n",
      "seniority\n",
      "Director         19\n",
      "Junior            7\n",
      "Lead             62\n",
      "Management       98\n",
      "Professional    118\n",
      "Senior           30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split annotated data: 70% dev (for threshold tuning), 30% test (for final evaluation)\n",
    "dev_df, test_df = train_test_split(eval_df, test_size=0.3, random_state=42, stratify=eval_df['department'])\n",
    "\n",
    "print(f\"Total annotated samples: {len(eval_df)}\")\n",
    "print(f\"Dev set (threshold tuning): {len(dev_df)} samples ({len(dev_df)/len(eval_df)*100:.1f}%)\")\n",
    "print(f\"Test set (final evaluation): {len(test_df)} samples ({len(test_df)/len(eval_df)*100:.1f}%)\")\n",
    "print(f\"\\nDev set department distribution:\")\n",
    "print(dev_df['department'].value_counts().sort_index())\n",
    "print(f\"\\nDev set seniority distribution:\")\n",
    "print(dev_df['seniority'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold Tuning on Dev Set\n",
    "\n",
    "Find optimal similarity threshold for each task.\n",
    "If max_similarity < threshold → use fallback label (Other/Professional), else use top-1 prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating input texts for dev set...\n",
      "✓ Created input texts for 334 dev samples\n"
     ]
    }
   ],
   "source": [
    "# Create input texts for dev set\n",
    "print(\"Creating input texts for dev set...\")\n",
    "dev_df['input_text'] = dev_df.apply(create_input_text, axis=1)\n",
    "print(f\"✓ Created input texts for {len(dev_df)} dev samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "# Load model for predictions\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "print(f\"Model loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions and similarities for dev set...\n",
      "✓ Computed predictions for 334 dev samples\n",
      "\n",
      "Department max similarity stats:\n",
      "  Mean: 0.599\n",
      "  Min:  0.276\n",
      "  Max:  0.857\n",
      "\n",
      "Seniority max similarity stats:\n",
      "  Mean: 0.599\n",
      "  Min:  0.201\n",
      "  Max:  0.894\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and max similarities for dev set\n",
    "print(\"Computing predictions and similarities for dev set...\")\n",
    "\n",
    "dev_dept_preds = []\n",
    "dev_dept_sims = []  # max similarity scores\n",
    "dev_sen_preds = []\n",
    "dev_sen_sims = []\n",
    "\n",
    "for _, row in dev_df.iterrows():\n",
    "    # Department\n",
    "    dept_pred, dept_conf, _ = predict_with_topk(row['input_text'], dept_prototypes, model, top_k=1)\n",
    "    dev_dept_preds.append(dept_pred)\n",
    "    dev_dept_sims.append(dept_conf)\n",
    "    \n",
    "    # Seniority\n",
    "    sen_pred, sen_conf, _ = predict_with_topk(row['input_text'], sen_prototypes, model, top_k=1)\n",
    "    dev_sen_preds.append(sen_pred)\n",
    "    dev_sen_sims.append(sen_conf)\n",
    "\n",
    "dev_df['dept_pred_raw'] = dev_dept_preds  # predictions without threshold\n",
    "dev_df['dept_max_sim'] = dev_dept_sims\n",
    "dev_df['sen_pred_raw'] = dev_sen_preds\n",
    "dev_df['sen_max_sim'] = dev_sen_sims\n",
    "\n",
    "print(f\"✓ Computed predictions for {len(dev_df)} dev samples\")\n",
    "print(f\"\\nDepartment max similarity stats:\")\n",
    "print(f\"  Mean: {np.mean(dev_dept_sims):.3f}\")\n",
    "print(f\"  Min:  {np.min(dev_dept_sims):.3f}\")\n",
    "print(f\"  Max:  {np.max(dev_dept_sims):.3f}\")\n",
    "print(f\"\\nSeniority max similarity stats:\")\n",
    "print(f\"  Mean: {np.mean(dev_sen_sims):.3f}\")\n",
    "print(f\"  Min:  {np.min(dev_sen_sims):.3f}\")\n",
    "print(f\"  Max:  {np.max(dev_sen_sims):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Department Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for optimal department threshold...\n",
      "================================================================================\n",
      "\n",
      "Threshold  Accuracy   F1-Macro   F1-Weighted \n",
      "--------------------------------------------------------------------------------\n",
      "0.20       0.335      0.392      0.324       \n",
      "0.22       0.335      0.392      0.324       \n",
      "0.24       0.335      0.392      0.324       \n",
      "0.26       0.335      0.392      0.324       \n",
      "0.28       0.338      0.393      0.329       \n",
      "0.30       0.338      0.393      0.329       \n",
      "0.32       0.338      0.393      0.329       \n",
      "0.34       0.335      0.375      0.331       \n",
      "0.36       0.338      0.378      0.335       \n",
      "0.38       0.344      0.380      0.344       \n",
      "0.40       0.353      0.384      0.357       \n",
      "0.42       0.377      0.394      0.388       \n",
      "0.44       0.404      0.415      0.422       \n",
      "0.46       0.422      0.426      0.441       \n",
      "0.48       0.434      0.427      0.455       \n",
      "0.50       0.446      0.432      0.468       \n",
      "0.52       0.467      0.439      0.484        ← BEST\n",
      "0.54       0.476      0.433      0.489       \n",
      "0.56       0.482      0.402      0.486       \n",
      "0.58       0.497      0.398      0.497       \n",
      "0.60       0.509      0.406      0.502       \n",
      "\n",
      "================================================================================\n",
      "✓ Best Department Threshold: 0.52 (F1-Macro: 0.439)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Grid search for best department threshold\n",
    "print(\"Grid search for optimal department threshold...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "DEPT_FALLBACK = \"Other\"\n",
    "thresholds = np.arange(0.20, 0.62, 0.02)  # 0.20 to 0.60 in 0.02 steps\n",
    "\n",
    "dept_threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold: if max_sim < threshold, use fallback, else use raw prediction\n",
    "    dept_preds_thresh = [\n",
    "        DEPT_FALLBACK if sim < threshold else pred\n",
    "        for pred, sim in zip(dev_df['dept_pred_raw'], dev_df['dept_max_sim'])\n",
    "    ]\n",
    "    \n",
    "    # Compute metrics\n",
    "    dept_true = dev_df['department'].tolist()\n",
    "    acc = accuracy_score(dept_true, dept_preds_thresh)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(dept_true, dept_preds_thresh, average='macro', zero_division=0)\n",
    "    _, _, f1_weighted, _ = precision_recall_fscore_support(dept_true, dept_preds_thresh, average='weighted', zero_division=0)\n",
    "    \n",
    "    dept_threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "dept_results_df = pd.DataFrame(dept_threshold_results)\n",
    "\n",
    "# Find best threshold (by macro F1)\n",
    "best_idx = dept_results_df['f1_macro'].idxmax()\n",
    "best_dept_threshold = dept_results_df.loc[best_idx, 'threshold']\n",
    "best_dept_f1 = dept_results_df.loc[best_idx, 'f1_macro']\n",
    "\n",
    "print(f\"\\n{'Threshold':<10} {'Accuracy':<10} {'F1-Macro':<10} {'F1-Weighted':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in dept_results_df.iterrows():\n",
    "    marker = \" ← BEST\" if row['threshold'] == best_dept_threshold else \"\"\n",
    "    print(f\"{row['threshold']:<10.2f} {row['accuracy']:<10.3f} {row['f1_macro']:<10.3f} {row['f1_weighted']:<12.3f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Best Department Threshold: {best_dept_threshold:.2f} (F1-Macro: {best_dept_f1:.3f})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Seniority Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for optimal seniority threshold...\n",
      "================================================================================\n",
      "\n",
      "Threshold  Accuracy   F1-Macro   F1-Weighted \n",
      "--------------------------------------------------------------------------------\n",
      "0.20       0.407      0.352      0.414       \n",
      "0.22       0.407      0.352      0.414       \n",
      "0.24       0.410      0.355      0.419       \n",
      "0.26       0.410      0.355      0.419       \n",
      "0.28       0.410      0.355      0.419       \n",
      "0.30       0.413      0.358      0.425       \n",
      "0.32       0.413      0.358      0.426       \n",
      "0.34       0.413      0.358      0.426       \n",
      "0.36       0.419      0.363      0.435       \n",
      "0.38       0.428      0.370      0.449       \n",
      "0.40       0.428      0.372      0.451       \n",
      "0.42       0.443      0.383      0.471       \n",
      "0.44       0.458      0.394      0.491       \n",
      "0.46       0.464      0.397      0.501       \n",
      "0.48       0.476      0.406      0.514       \n",
      "0.50       0.491      0.417      0.525       \n",
      "0.52       0.503      0.415      0.534       \n",
      "0.54       0.512      0.418      0.536        ← BEST\n",
      "0.56       0.503      0.406      0.521       \n",
      "0.58       0.506      0.385      0.520       \n",
      "0.60       0.506      0.379      0.515       \n",
      "\n",
      "================================================================================\n",
      "✓ Best Seniority Threshold: 0.54 (F1-Macro: 0.418)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Grid search for best seniority threshold\n",
    "print(\"Grid search for optimal seniority threshold...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "SEN_FALLBACK = \"Professional\"\n",
    "\n",
    "sen_threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold: if max_sim < threshold, use fallback, else use raw prediction\n",
    "    sen_preds_thresh = [\n",
    "        SEN_FALLBACK if sim < threshold else pred\n",
    "        for pred, sim in zip(dev_df['sen_pred_raw'], dev_df['sen_max_sim'])\n",
    "    ]\n",
    "    \n",
    "    # Compute metrics\n",
    "    sen_true = dev_df['seniority'].tolist()\n",
    "    acc = accuracy_score(sen_true, sen_preds_thresh)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(sen_true, sen_preds_thresh, average='macro', zero_division=0)\n",
    "    _, _, f1_weighted, _ = precision_recall_fscore_support(sen_true, sen_preds_thresh, average='weighted', zero_division=0)\n",
    "    \n",
    "    sen_threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "sen_results_df = pd.DataFrame(sen_threshold_results)\n",
    "\n",
    "# Find best threshold (by macro F1)\n",
    "best_idx = sen_results_df['f1_macro'].idxmax()\n",
    "best_sen_threshold = sen_results_df.loc[best_idx, 'threshold']\n",
    "best_sen_f1 = sen_results_df.loc[best_idx, 'f1_macro']\n",
    "\n",
    "print(f\"\\n{'Threshold':<10} {'Accuracy':<10} {'F1-Macro':<10} {'F1-Weighted':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in sen_results_df.iterrows():\n",
    "    marker = \" ← BEST\" if row['threshold'] == best_sen_threshold else \"\"\n",
    "    print(f\"{row['threshold']:<10.2f} {row['accuracy']:<10.3f} {row['f1_macro']:<10.3f} {row['f1_weighted']:<12.3f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Best Seniority Threshold: {best_sen_threshold:.2f} (F1-Macro: {best_sen_f1:.3f})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation on Test Set\n",
    "\n",
    "Apply optimized thresholds to test set for final performance assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test set...\n",
      "✓ Computed predictions for 144 test samples\n",
      "  Applied thresholds: Dept=0.52, Sen=0.54\n"
     ]
    }
   ],
   "source": [
    "# Create input texts for test set\n",
    "print(\"Preparing test set...\")\n",
    "test_df['input_text'] = test_df.apply(create_input_text, axis=1)\n",
    "\n",
    "# Get predictions and similarities for test set\n",
    "test_dept_preds = []\n",
    "test_dept_sims = []\n",
    "test_sen_preds = []\n",
    "test_sen_sims = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    # Department\n",
    "    dept_pred, dept_conf, _ = predict_with_topk(row['input_text'], dept_prototypes, model, top_k=1)\n",
    "    test_dept_preds.append(dept_pred)\n",
    "    test_dept_sims.append(dept_conf)\n",
    "    \n",
    "    # Seniority\n",
    "    sen_pred, sen_conf, _ = predict_with_topk(row['input_text'], sen_prototypes, model, top_k=1)\n",
    "    test_sen_preds.append(sen_pred)\n",
    "    test_sen_sims.append(sen_conf)\n",
    "\n",
    "# Apply optimized thresholds\n",
    "test_dept_preds_final = [\n",
    "    DEPT_FALLBACK if sim < best_dept_threshold else pred\n",
    "    for pred, sim in zip(test_dept_preds, test_dept_sims)\n",
    "]\n",
    "\n",
    "test_sen_preds_final = [\n",
    "    SEN_FALLBACK if sim < best_sen_threshold else pred\n",
    "    for pred, sim in zip(test_sen_preds, test_sen_sims)\n",
    "]\n",
    "\n",
    "print(f\"✓ Computed predictions for {len(test_df)} test samples\")\n",
    "print(f\"  Applied thresholds: Dept={best_dept_threshold:.2f}, Sen={best_sen_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Test Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL TEST SET RESULTS (with optimized thresholds)\n",
      "================================================================================\n",
      "\n",
      "Optimized Thresholds:\n",
      "  Department:  0.52 (Fallback: 'Other')\n",
      "  Seniority:   0.54 (Fallback: 'Professional')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Department Accuracy:       0.451\n",
      "Department F1 (macro):     0.301\n",
      "Department F1 (weighted):  0.476\n",
      "\n",
      "Seniority Accuracy:        0.458\n",
      "Seniority F1 (macro):      0.360\n",
      "Seniority F1 (weighted):   0.499\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute final test metrics\n",
    "dept_true_test = test_df['department'].tolist()\n",
    "sen_true_test = test_df['seniority'].tolist()\n",
    "\n",
    "# Department metrics\n",
    "dept_accuracy = accuracy_score(dept_true_test, test_dept_preds_final)\n",
    "dept_precision, dept_recall, dept_f1, _ = precision_recall_fscore_support(\n",
    "    dept_true_test, test_dept_preds_final, average='macro', zero_division=0\n",
    ")\n",
    "_, _, dept_f1_weighted, _ = precision_recall_fscore_support(\n",
    "    dept_true_test, test_dept_preds_final, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "# Seniority metrics\n",
    "sen_accuracy = accuracy_score(sen_true_test, test_sen_preds_final)\n",
    "sen_precision, sen_recall, sen_f1, _ = precision_recall_fscore_support(\n",
    "    sen_true_test, test_sen_preds_final, average='macro', zero_division=0\n",
    ")\n",
    "_, _, sen_f1_weighted, _ = precision_recall_fscore_support(\n",
    "    sen_true_test, test_sen_preds_final, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST SET RESULTS (with optimized thresholds)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOptimized Thresholds:\")\n",
    "print(f\"  Department:  {best_dept_threshold:.2f} (Fallback: '{DEPT_FALLBACK}')\")\n",
    "print(f\"  Seniority:   {best_sen_threshold:.2f} (Fallback: '{SEN_FALLBACK}')\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"Department Accuracy:       {dept_accuracy:.3f}\")\n",
    "print(f\"Department F1 (macro):     {dept_f1:.3f}\")\n",
    "print(f\"Department F1 (weighted):  {dept_f1_weighted:.3f}\")\n",
    "print()\n",
    "print(f\"Seniority Accuracy:        {sen_accuracy:.3f}\")\n",
    "print(f\"Seniority F1 (macro):      {sen_f1:.3f}\")\n",
    "print(f\"Seniority F1 (weighted):   {sen_f1_weighted:.3f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Classification Report (Department - Test Set):\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        Administrative       0.08      0.33      0.12         3\n",
      "  Business Development       0.20      0.40      0.27         5\n",
      "            Consulting       0.35      0.88      0.50         8\n",
      "      Customer Support       0.00      0.00      0.00         2\n",
      "       Human Resources       0.00      0.00      0.00         5\n",
      "Information Technology       0.67      0.24      0.35        17\n",
      "             Marketing       0.33      0.60      0.43         5\n",
      "                 Other       0.74      0.52      0.61        75\n",
      "    Project Management       0.43      0.33      0.38         9\n",
      "            Purchasing       0.20      0.25      0.22         4\n",
      "                 Sales       0.42      0.45      0.43        11\n",
      "\n",
      "              accuracy                           0.45       144\n",
      "             macro avg       0.31      0.36      0.30       144\n",
      "          weighted avg       0.57      0.45      0.48       144\n",
      "\n",
      "\n",
      "Detailed Classification Report (Seniority - Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Director       0.28      0.62      0.38         8\n",
      "      Junior       0.00      0.00      0.00         3\n",
      "        Lead       0.65      0.37      0.47        41\n",
      "  Management       0.89      0.60      0.72        40\n",
      "Professional       0.45      0.47      0.46        43\n",
      "      Senior       0.10      0.22      0.13         9\n",
      "\n",
      "    accuracy                           0.46       144\n",
      "   macro avg       0.39      0.38      0.36       144\n",
      "weighted avg       0.59      0.46      0.50       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed classification reports\n",
    "print(\"\\nDetailed Classification Report (Department - Test Set):\")\n",
    "print(classification_report(dept_true_test, test_dept_preds_final, zero_division=0))\n",
    "\n",
    "print(\"\\nDetailed Classification Report (Seniority - Test Set):\")\n",
    "print(classification_report(sen_true_test, test_sen_preds_final, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Real-World (Annotated LinkedIn CVs - Test Set):\n",
      "  Optimized Thresholds: Dept=0.52, Sen=0.54\n",
      "--------------------------------------------------------------------------------\n",
      "Department Accuracy:       0.451\n",
      "Department F1 (macro):     0.301\n",
      "Department F1 (weighted):  0.476\n",
      "\n",
      "Seniority Accuracy:        0.458\n",
      "Seniority F1 (macro):      0.360\n",
      "Seniority F1 (weighted):   0.499\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save results with optimized thresholds to JSON\n",
    "results = {\n",
    "    'approach': 'embedding_zero_shot_v2',\n",
    "    'model': MODEL_NAME,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'n_prototypes': 3,\n",
    "        'multi_prototype_clustering': 'KMeans',\n",
    "        'input_text_enrichment': 'title + company + description (200 chars)',\n",
    "        'text_normalization': False\n",
    "    },\n",
    "    'threshold_tuning': {\n",
    "        'dev_samples': len(dev_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'dept_threshold': float(best_dept_threshold),\n",
    "        'dept_fallback': DEPT_FALLBACK,\n",
    "        'sen_threshold': float(best_sen_threshold),\n",
    "        'sen_fallback': SEN_FALLBACK,\n",
    "        'tuning_metric': 'f1_macro',\n",
    "        'threshold_range': '0.20-0.60 (step=0.02)'\n",
    "    },\n",
    "    'department': {\n",
    "        'accuracy': float(dept_accuracy),\n",
    "        'precision_macro': float(dept_precision),\n",
    "        'recall_macro': float(dept_recall),\n",
    "        'f1_macro': float(dept_f1),\n",
    "        'f1_weighted': float(dept_f1_weighted)\n",
    "    },\n",
    "    'seniority': {\n",
    "        'accuracy': float(sen_accuracy),\n",
    "        'precision_macro': float(sen_precision),\n",
    "        'recall_macro': float(sen_recall),\n",
    "        'f1_macro': float(sen_f1),\n",
    "        'f1_weighted': float(sen_f1_weighted)\n",
    "    },\n",
    "    'notes': 'Zero-shot with multi-prototypes (K-Means), richer input context, and optimized similarity thresholds via grid search on dev set.'\n",
    "}\n",
    "\n",
    "output_path = RESULTS_DIR / 'embedding_zeroshot_v2_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Real-World (Annotated LinkedIn CVs - Test Set):\")\n",
    "print(f\"  Optimized Thresholds: Dept={best_dept_threshold:.2f}, Sen={best_sen_threshold:.2f}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Department Accuracy:       {dept_accuracy:.3f}\")\n",
    "print(f\"Department F1 (macro):     {dept_f1:.3f}\")\n",
    "print(f\"Department F1 (weighted):  {dept_f1_weighted:.3f}\")\n",
    "print(f\"\\nSeniority Accuracy:        {sen_accuracy:.3f}\")\n",
    "print(f\"Seniority F1 (macro):      {sen_f1:.3f}\")\n",
    "print(f\"Seniority F1 (weighted):   {sen_f1_weighted:.3f}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
