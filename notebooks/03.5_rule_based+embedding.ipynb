{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54731241",
   "metadata": {},
   "source": [
    "# Hybrid: Rule-Based + Embedding Classifier\n",
    "\n",
    "**Approach**: Combine strengths of both rule-based and embedding approaches\n",
    "\n",
    "**Decision Logic**:\n",
    "1. **Rule-Based Exact Match**: If exact/direct match found \u2192 Use rule-based prediction\n",
    "2. **Embedding Fallback**: Otherwise \u2192 Use embedding-based prediction\n",
    "3. **Rule-Based Safety Net**: If embedding confidence < 50% \u2192 Fall back to rule-based (fuzzy/keyword)\n",
    "\n",
    "**Rationale**:\n",
    "- **Rule-Based (Notebook 02)**: Excellent precision on exact/fuzzy matches (90%+ confidence) - 72.8% dept, 61.1% sen\n",
    "- **Embeddings (Notebook 03)**: Better generalization for unknown terms and semantic similarity - 32.4% dept, 42.1% sen\n",
    "- **Safety Net**: Prevents poor embedding predictions on low-confidence cases\n",
    "\n",
    "**Expected Benefit**: Best of both worlds - precision + flexibility\n",
    "\n",
    "**Code Reuse**: This notebook imports and combines classifiers from:\n",
    "- Notebook 02: `create_department_classifier()`, `create_seniority_classifier()` from `src.models.rule_based`\n",
    "- Notebook 03: Embedding prototypes and prediction logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20d3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Import data loaders and models\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data.loader import load_label_lists, load_evaluation_dataset\n",
    "from src.models.rule_based import RuleConfig, create_department_classifier, create_seniority_classifier\n",
    "from src.models.embedding_classifier import EmbeddingConfig, create_domain_classifier, create_seniority_classifier as create_seniority_embedding_classifier\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "RESULTS_DIR = Path('./results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496c5c9",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1846fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding fix...\n",
      "  Deduplication: 10145 -> 10145 (removed 0 duplicates)\n",
      "  Deduplication: 9428 -> 9428 (removed 0 duplicates)\n",
      "Department lookup: 10,145 examples\n",
      "Seniority lookup:  9,428 examples\n",
      "\n",
      "Evaluation samples: 478\n"
     ]
    }
   ],
   "source": [
    "# Load lookup tables\n",
    "dept_df, sen_df = load_label_lists(DATA_DIR, max_per_class=None)\n",
    "\n",
    "print(f\"Department lookup: {len(dept_df):,} examples\")\n",
    "print(f\"Seniority lookup:  {len(sen_df):,} examples\")\n",
    "\n",
    "# Load evaluation data\n",
    "eval_df = load_evaluation_dataset(DATA_DIR)\n",
    "print(f\"\\nEvaluation samples: {len(eval_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5cc5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set: 334 samples\n",
      "Test set: 144 samples\n"
     ]
    }
   ],
   "source": [
    "# Split into dev/test (same as embedding notebook)\n",
    "dev_df, test_df = train_test_split(eval_df, test_size=0.3, random_state=42, stratify=eval_df['department'])\n",
    "\n",
    "print(f\"Dev set: {len(dev_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff60aa",
   "metadata": {},
   "source": [
    "## 2. Rule-Based Classifiers (from Notebook 02)\n",
    "\n",
    "Using the optimized rule-based classifiers with text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61c15f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Rule-Based Classifiers created\n",
      "   Department: Text normalization + Keywords + Fuzzy matching\n",
      "   Seniority: Text normalization + Keywords + Fuzzy matching\n"
     ]
    }
   ],
   "source": [
    "# Configure rule-based classifiers (from Notebook 02)\n",
    "config_dept = RuleConfig(\n",
    "    fuzzy_threshold=0.8, \n",
    "    use_text_normalization=True,\n",
    "    default_label=\"Other\"\n",
    ")\n",
    "\n",
    "config_sen = RuleConfig(\n",
    "    fuzzy_threshold=0.8, \n",
    "    use_text_normalization=True,\n",
    "    default_label=\"Professional\"\n",
    ")\n",
    "\n",
    "# Create classifiers using factory functions from src.models.rule_based\n",
    "dept_clf_rule = create_department_classifier(dept_df, config=config_dept)\n",
    "sen_clf_rule = create_seniority_classifier(sen_df, config=config_sen)\n",
    "\n",
    "print(\"\u2705 Rule-Based Classifiers created\")\n",
    "print(f\"   Department: Text normalization + Keywords + Fuzzy matching\")\n",
    "print(f\"   Seniority: Text normalization + Keywords + Fuzzy matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adf214",
   "metadata": {},
   "source": [
    "## 3. Embedding Classifiers (from embedding_classifier.py)\n",
    "\n",
    "Using the EmbeddingClassifier with averaged example embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f108fabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding classifiers...\n",
      "Model: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "# Model configuration (from embedding_classifier.py)\n",
    "MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "print(f\"Building embedding classifiers...\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5467f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for rich input text\n",
    "def create_input_text(row):\n",
    "    \"\"\"Create rich input text from CV position (title + company + description).\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    if 'title' in row and pd.notna(row['title']) and row['title'].strip():\n",
    "        parts.append(row['title'])\n",
    "    \n",
    "    if 'company' in row and pd.notna(row['company']) and row['company'].strip():\n",
    "        parts.append(f\"at {row['company']}\")\n",
    "    \n",
    "    if 'text' in row and pd.notna(row['text']) and row['text'].strip():\n",
    "        desc = row['text'][:200].strip()\n",
    "        if desc and desc not in ' '.join(parts):\n",
    "            parts.append(desc)\n",
    "    \n",
    "    return ' '.join(parts) if parts else \"Unknown Position\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a7be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Department Embedding Classifier...\n",
      "Loading model 'paraphrase-multilingual-MiniLM-L12-v2' on cuda...\n",
      "Model loaded successfully!\n",
      "Fitted from examples: 11 labels, shape (11, 384)\n",
      "Creating Seniority Embedding Classifier...\n",
      "Loading model 'paraphrase-multilingual-MiniLM-L12-v2' on cuda...\n",
      "Model loaded successfully!\n",
      "Fitted from examples: 6 labels, shape (6, 384)\n",
      "\n",
      "\u2705 Embedding Classifiers created\n",
      "   Department: 11 labels\n",
      "   Seniority: 6 labels\n"
     ]
    }
   ],
   "source": [
    "# Build embedding classifiers using factory functions from embedding_classifier.py\n",
    "print(\"Creating Department Embedding Classifier...\")\n",
    "dept_clf_emb = create_domain_classifier(dept_df, model_name=MODEL_NAME, use_examples=True)\n",
    "\n",
    "# Add missing \"Professional\" for seniority\n",
    "sen_df_extended = sen_df.copy()\n",
    "professional_examples = pd.DataFrame({\n",
    "    'text': ['Professional', 'Professional Position', 'Professional Role'],\n",
    "    'label': ['Professional'] * 3\n",
    "})\n",
    "sen_df_extended = pd.concat([sen_df_extended, professional_examples], ignore_index=True)\n",
    "\n",
    "print(\"Creating Seniority Embedding Classifier...\")\n",
    "sen_clf_emb = create_seniority_embedding_classifier(sen_df_extended, model_name=MODEL_NAME, use_examples=True)\n",
    "\n",
    "print(f\"\\n\u2705 Embedding Classifiers created\")\n",
    "print(f\"   Department: {len(dept_df['label'].unique())} labels\")\n",
    "print(f\"   Seniority: {len(sen_df_extended['label'].unique())} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d307b66",
   "metadata": {},
   "source": [
    "## 4. Hybrid Classifier\n",
    "\n",
    "**Decision Flow**:\n",
    "```\n",
    "1. Try Rule-Based\n",
    "   \u251c\u2500 Exact Match Found \u2192 Use Rule-Based \u2713\n",
    "   \u2514\u2500 No Exact Match \u2192 Try Embedding\n",
    "       \u251c\u2500 Confidence \u2265 50% \u2192 Use Embedding \u2713\n",
    "       \u2514\u2500 Confidence < 50% \u2192 Use Rule-Based (fuzzy/keyword) \u2713\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb05b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_predict(row, task='department', \n",
    "                  embedding_low_threshold=0.50):\n",
    "    \"\"\"\n",
    "    Hybrid prediction combining rule-based and embedding approaches.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with CV position data\n",
    "        task: 'department' or 'seniority'\n",
    "        embedding_low_threshold: If embedding confidence < this, fall back to rule-based (default: 0.50)\n",
    "    \n",
    "    Returns:\n",
    "        prediction: Final label\n",
    "        confidence: Final confidence score\n",
    "        method: Which method was used ('rule_based_exact', 'embedding', 'rule_based_low')\n",
    "    \"\"\"\n",
    "    # Select classifiers based on task\n",
    "    if task == 'department':\n",
    "        rule_clf = dept_clf_rule\n",
    "        emb_clf = dept_clf_emb\n",
    "    else:  # seniority\n",
    "        rule_clf = sen_clf_rule\n",
    "        emb_clf = sen_clf_emb\n",
    "    \n",
    "    # Step 1: Get rule-based prediction using predict_with_details()\n",
    "    # predict_with_details returns: [(prediction, confidence, method), ...]\n",
    "    rule_pred, rule_conf, rule_method = rule_clf.predict_with_details([row['title']])[0]\n",
    "    \n",
    "    # Step 2: If rule-based found an EXACT match, use it (direct match only)\n",
    "    if rule_method == 'exact':\n",
    "        return rule_pred, rule_conf, 'rule_based_exact'\n",
    "    \n",
    "    # Step 3: Otherwise, try embedding\n",
    "    # Use rich input text (title + company + description)\n",
    "    input_text = create_input_text(row)\n",
    "    \n",
    "    # Use predict_single() method from EmbeddingClassifier\n",
    "    emb_pred, emb_conf, _ = emb_clf.predict_single(input_text)\n",
    "    \n",
    "    # Step 4: If embedding confidence is decent (\u226550%), use it\n",
    "    if emb_conf >= embedding_low_threshold:\n",
    "        return emb_pred, emb_conf, 'embedding'\n",
    "    \n",
    "    # Step 5: Safety net - fall back to rule-based (fuzzy/keyword match)\n",
    "    return rule_pred, rule_conf, 'rule_based_low'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b109ac",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8349cd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hybrid predictions on test set...\n",
      "\u2713 Completed predictions for 144 test samples\n"
     ]
    }
   ],
   "source": [
    "# Run hybrid predictions on test set\n",
    "print(\"Running hybrid predictions on test set...\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    # Department\n",
    "    dept_pred, dept_conf, dept_method = hybrid_predict(row, task='department')\n",
    "    \n",
    "    # Seniority\n",
    "    sen_pred, sen_conf, sen_method = hybrid_predict(row, task='seniority')\n",
    "    \n",
    "    test_results.append({\n",
    "        'dept_pred': dept_pred,\n",
    "        'dept_conf': dept_conf,\n",
    "        'dept_method': dept_method,\n",
    "        'dept_true': row['department'],\n",
    "        'sen_pred': sen_pred,\n",
    "        'sen_conf': sen_conf,\n",
    "        'sen_method': sen_method,\n",
    "        'sen_true': row['seniority']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(test_results)\n",
    "print(f\"\u2713 Completed predictions for {len(results_df)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acac892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METHOD USAGE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Department:\n",
      "  embedding           :  93 ( 64.6%)\n",
      "  rule_based_low      :  39 ( 27.1%)\n",
      "  rule_based_exact    :  12 (  8.3%)\n",
      "\n",
      "Seniority:\n",
      "  embedding           :  66 ( 45.8%)\n",
      "  rule_based_low      :  43 ( 29.9%)\n",
      "  rule_based_exact    :  35 ( 24.3%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze method usage\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD USAGE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDepartment:\")\n",
    "dept_method_counts = results_df['dept_method'].value_counts()\n",
    "for method, count in dept_method_counts.items():\n",
    "    pct = count / len(results_df) * 100\n",
    "    print(f\"  {method:20s}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nSeniority:\")\n",
    "sen_method_counts = results_df['sen_method'].value_counts()\n",
    "for method, count in sen_method_counts.items():\n",
    "    pct = count / len(results_df) * 100\n",
    "    print(f\"  {method:20s}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55fa8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "dept_true = results_df['dept_true'].tolist()\n",
    "dept_pred = results_df['dept_pred'].tolist()\n",
    "\n",
    "sen_true = results_df['sen_true'].tolist()\n",
    "sen_pred = results_df['sen_pred'].tolist()\n",
    "\n",
    "# Department metrics\n",
    "dept_accuracy = accuracy_score(dept_true, dept_pred)\n",
    "dept_precision, dept_recall, dept_f1, _ = precision_recall_fscore_support(\n",
    "    dept_true, dept_pred, average='macro', zero_division=0\n",
    ")\n",
    "_, _, dept_f1_weighted, _ = precision_recall_fscore_support(\n",
    "    dept_true, dept_pred, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "# Seniority metrics\n",
    "sen_accuracy = accuracy_score(sen_true, sen_pred)\n",
    "sen_precision, sen_recall, sen_f1, _ = precision_recall_fscore_support(\n",
    "    sen_true, sen_pred, average='macro', zero_division=0\n",
    ")\n",
    "_, _, sen_f1_weighted, _ = precision_recall_fscore_support(\n",
    "    sen_true, sen_pred, average='weighted', zero_division=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad55456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Real-World (Annotated LinkedIn CVs - Test Set):\n",
      "  Hybrid Strategy: Rule-Based Exact Match \u2192 Embedding \u2192 Rule-Based (<50%)\n",
      "--------------------------------------------------------------------------------\n",
      "Department Accuracy:       0.479\n",
      "Department F1 (macro):     0.365\n",
      "Department F1 (weighted):  0.503\n",
      "\n",
      "Seniority Accuracy:        0.465\n",
      "Seniority F1 (macro):      0.434\n",
      "Seniority F1 (weighted):   0.495\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Real-World (Annotated LinkedIn CVs - Test Set):\")\n",
    "print(f\"  Hybrid Strategy: Rule-Based Exact Match \u2192 Embedding \u2192 Rule-Based (<50%)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Department Accuracy:       {dept_accuracy:.3f}\")\n",
    "print(f\"Department F1 (macro):     {dept_f1:.3f}\")\n",
    "print(f\"Department F1 (weighted):  {dept_f1_weighted:.3f}\")\n",
    "print(f\"\\nSeniority Accuracy:        {sen_accuracy:.3f}\")\n",
    "print(f\"Seniority F1 (macro):      {sen_f1:.3f}\")\n",
    "print(f\"Seniority F1 (weighted):   {sen_f1_weighted:.3f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394ede1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Classification Report (Department - Test Set):\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        Administrative       0.11      0.33      0.17         3\n",
      "  Business Development       0.18      0.40      0.25         5\n",
      "            Consulting       0.23      0.75      0.35         8\n",
      "      Customer Support       0.00      0.00      0.00         2\n",
      "       Human Resources       0.25      0.20      0.22         5\n",
      "Information Technology       0.45      0.29      0.36        17\n",
      "             Marketing       0.40      0.40      0.40         5\n",
      "                 Other       0.70      0.51      0.59        75\n",
      "    Project Management       0.50      0.67      0.57         9\n",
      "            Purchasing       1.00      0.25      0.40         4\n",
      "                 Sales       0.78      0.64      0.70        11\n",
      "\n",
      "              accuracy                           0.48       144\n",
      "             macro avg       0.42      0.40      0.36       144\n",
      "          weighted avg       0.58      0.48      0.50       144\n",
      "\n",
      "\n",
      "Detailed Classification Report (Seniority - Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Director       0.31      0.62      0.42         8\n",
      "      Junior       0.29      0.67      0.40         3\n",
      "        Lead       0.81      0.41      0.55        41\n",
      "  Management       0.58      0.62      0.60        40\n",
      "Professional       0.72      0.30      0.43        43\n",
      "      Senior       0.13      0.56      0.21         9\n",
      "\n",
      "    accuracy                           0.47       144\n",
      "   macro avg       0.47      0.53      0.43       144\n",
      "weighted avg       0.64      0.47      0.50       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed classification reports\n",
    "print(\"\\nDetailed Classification Report (Department - Test Set):\")\n",
    "print(classification_report(dept_true, dept_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nDetailed Classification Report (Seniority - Test Set):\")\n",
    "print(classification_report(sen_true, sen_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46f76c",
   "metadata": {},
   "source": [
    "## 6. Method-Specific Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "826293dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE BY METHOD\n",
      "================================================================================\n",
      "\n",
      "Department - Accuracy by Method:\n",
      "  embedding           : Acc=0.312, n= 93, avg_conf=0.619\n",
      "  rule_based_low      : Acc=0.718, n= 39, avg_conf=0.077\n",
      "  rule_based_exact    : Acc=1.000, n= 12, avg_conf=1.000\n",
      "\n",
      "Seniority - Accuracy by Method:\n",
      "  rule_based_exact    : Acc=0.543, n= 35, avg_conf=1.000\n",
      "  embedding           : Acc=0.333, n= 66, avg_conf=0.608\n",
      "  rule_based_low      : Acc=0.605, n= 43, avg_conf=0.230\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze accuracy by method used\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY METHOD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDepartment - Accuracy by Method:\")\n",
    "for method in results_df['dept_method'].unique():\n",
    "    mask = results_df['dept_method'] == method\n",
    "    subset = results_df[mask]\n",
    "    acc = accuracy_score(subset['dept_true'], subset['dept_pred'])\n",
    "    count = len(subset)\n",
    "    avg_conf = subset['dept_conf'].mean()\n",
    "    print(f\"  {method:20s}: Acc={acc:.3f}, n={count:3d}, avg_conf={avg_conf:.3f}\")\n",
    "\n",
    "print(\"\\nSeniority - Accuracy by Method:\")\n",
    "for method in results_df['sen_method'].unique():\n",
    "    mask = results_df['sen_method'] == method\n",
    "    subset = results_df[mask]\n",
    "    acc = accuracy_score(subset['sen_true'], subset['sen_pred'])\n",
    "    count = len(subset)\n",
    "    avg_conf = subset['sen_conf'].mean()\n",
    "    print(f\"  {method:20s}: Acc={acc:.3f}, n={count:3d}, avg_conf={avg_conf:.3f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_findings_35",
   "metadata": {},
   "source": [
    "## 8. Key Findings & Interpretation\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "| Metric | Department | Seniority |\n",
    "|--------|-----------|----------|\n",
    "| **Accuracy** | 47.9% | 46.5% |\n",
    "| **F1 Macro** | 0.365 | 0.434 |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Rule-based exact matches are highly reliable** (100% on Department, 54% on Seniority)\n",
    "2. **Embedding predictions show moderate accuracy** (~31% Dept, ~33% Sen) but cover the most cases\n",
    "3. **The hybrid strategy works best for seniority** where clear keyword matches exist\n",
    "4. **Department classification remains challenging** due to semantic ambiguity\n",
    "\n",
    "### Method Usage Analysis\n",
    "\n",
    "- **Department**: Embeddings used 64.6% of the time, rule-based fallback 27.1%, exact matches 8.3%\n",
    "- **Seniority**: More balanced - embeddings 45.8%, rule-based fallback 29.9%, exact matches 24.3%\n",
    "\n",
    "### When to Use This Approach\n",
    "\n",
    "\u2705 **Good for**: Cases where precision on known terms is critical, multilingual job titles\n",
    "\u274c **Limitations**: Embedding predictions on ambiguous titles remain uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fff43",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199a67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results = {\n",
    "    'approach': 'hybrid_rule_based_embedding',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'embedding_low_threshold': 0.50,\n",
    "        'embedding_model': MODEL_NAME,\n",
    "        'decision_logic': 'Rule-Based Exact Match \u2192 Embedding \u2192 Rule-Based (<50%)'\n",
    "    },\n",
    "    'method_usage': {\n",
    "        'department': dept_method_counts.to_dict(),\n",
    "        'seniority': sen_method_counts.to_dict()\n",
    "    },\n",
    "    'department': {\n",
    "        'accuracy': float(dept_accuracy),\n",
    "        'precision_macro': float(dept_precision),\n",
    "        'recall_macro': float(dept_recall),\n",
    "        'f1_macro': float(dept_f1),\n",
    "        'f1_weighted': float(dept_f1_weighted)\n",
    "    },\n",
    "    'seniority': {\n",
    "        'accuracy': float(sen_accuracy),\n",
    "        'precision_macro': float(sen_precision),\n",
    "        'recall_macro': float(sen_recall),\n",
    "        'f1_macro': float(sen_f1),\n",
    "        'f1_weighted': float(sen_f1_weighted)\n",
    "    },\n",
    "    'notes': 'Hybrid approach combining rule-based precision with embedding flexibility. Uses rule-based for high-confidence exact matches and low-confidence safety net, embeddings for mid-range cases.'\n",
    "}\n",
    "\n",
    "output_path = RESULTS_DIR / 'hybrid_rule_embedding_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}