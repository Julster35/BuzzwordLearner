{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7662c161",
   "metadata": {},
   "source": [
    "# 05. Pseudo-Labeling with Transformers\n",
    "\n",
    "This notebook covers:\n",
    "1. Generating pseudo-labels for unannotated CVs using the embedding baseline.\n",
    "2. Filtering for high-confidence predictions (Silver Data).\n",
    "3. Combining Gold (lookups) + Silver (pseudo-labels) data.\n",
    "4. Fine-tuning Transformer classifiers on the combined dataset.\n",
    "5. Comprehensive evaluation on annotated CVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d502a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from src.data.loader import load_label_lists, load_inference_dataset, load_evaluation_dataset, balance_dataset, prepare_dataset\n",
    "from src.models.embedding_classifier import EmbeddingClassifier, create_domain_classifier, create_seniority_classifier\n",
    "from src.models.transformer_classifier import TransformerClassifier\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e9423",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fad73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding fix...\n",
      "  Deduplication: 10145 -> 10145 (removed 0 duplicates)\n",
      "  Deduplication: 9428 -> 9428 (removed 0 duplicates)\n",
      "Gold-labeled data (lookup tables):\n",
      "  Department: 10,145 examples\n",
      "  Seniority:  9,428 examples\n"
     ]
    }
   ],
   "source": [
    "# Load lookup tables (gold labels)\n",
    "dept_df, sen_df = load_label_lists(DATA_DIR)\n",
    "\n",
    "print(f\"Gold-labeled data (lookup tables):\\n  Department: {len(dept_df):,} examples\\n  Seniority:  {len(sen_df):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33d0c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unannotated LinkedIn CVs: 314 positions\n",
      "These will be pseudo-labeled using the embedding baseline\n"
     ]
    }
   ],
   "source": [
    "# Load unannotated CVs for pseudo-labeling\n",
    "inference_df = load_inference_dataset(DATA_DIR)\n",
    "\n",
    "print(f\"\\nUnannotated LinkedIn CVs: {len(inference_df)} positions\")\n",
    "print(\"These will be pseudo-labeled using the embedding baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7e864",
   "metadata": {},
   "source": [
    "## 2. Generate Pseudo-Labels Using Embedding Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179408b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding classifiers for pseudo-labeling...\n",
      "Loading model 'paraphrase-multilingual-MiniLM-L12-v2' on cuda...\n",
      "Model loaded successfully!\n",
      "Fitted from examples: 11 labels, shape (11, 384)\n",
      "Loading model 'paraphrase-multilingual-MiniLM-L12-v2' on cuda...\n",
      "Model loaded successfully!\n",
      "Fitted from examples: 5 labels, shape (5, 384)\n",
      "‚úÖ Embedding classifiers ready\n"
     ]
    }
   ],
   "source": [
    "# Create embedding classifiers using factory functions (same as notebook 03)\n",
    "print(\"Creating embedding classifiers for pseudo-labeling...\")\n",
    "dept_emb = create_domain_classifier(dept_df, use_examples=True)\n",
    "sen_emb = create_seniority_classifier(sen_df, use_examples=True)\n",
    "print(\"Embedding classifiers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0660de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating pseudo-labels for unannotated CVs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6809ad4c2f04fa79aef6ea6ac6656d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26510e9632b47feb0cf884076555764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 314 pseudo-labels\n",
      "\n",
      "Confidence statistics:\n",
      "  Dept - Mean: 0.575, Median: 0.592\n",
      "  Sen  - Mean: 0.568, Median: 0.587\n"
     ]
    }
   ],
   "source": [
    "# Generate pseudo-labels with confidence scores\n",
    "print(\"\\nGenerating pseudo-labels for unannotated CVs...\")\n",
    "inference_texts = inference_df['text'].tolist()\n",
    "\n",
    "dept_results = dept_emb.predict_with_confidence(inference_texts)\n",
    "dept_pseudo_labels = [res[0] for res in dept_results]\n",
    "dept_conf = [res[1] for res in dept_results]\n",
    "sen_results = sen_emb.predict_with_confidence(inference_texts)\n",
    "sen_pseudo_labels = [res[0] for res in sen_results]\n",
    "sen_conf = [res[1] for res in sen_results]\n",
    "\n",
    "inference_df['pseudo_dept'] = dept_pseudo_labels\n",
    "inference_df['dept_conf'] = dept_conf\n",
    "inference_df['pseudo_sen'] = sen_pseudo_labels\n",
    "inference_df['sen_conf'] = sen_conf\n",
    "\n",
    "print(f\"Generated {len(inference_df)} pseudo-labels\")\n",
    "print(f\"    Confidence statistics:\\n  Dept - Mean: {np.mean(dept_conf):.3f}, Median: {np.median(dept_conf):.3f}\")\n",
    "print(f\"    Sen  - Mean: {np.mean(sen_conf):.3f}, Median: {np.median(sen_conf):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ecec4",
   "metadata": {},
   "source": [
    "## 3. Filter for High-Confidence Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-confidence pseudo-labels (>0.85):\n",
      "  Department: 2 / 314 (0.6%)\n",
      "  Seniority:  0 / 314 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Filter department pseudo-labels\n",
    "CONFIDENCE_THRESHOLD = 0.85\n",
    "\n",
    "dept_silver = inference_df[inference_df['dept_conf'] >= CONFIDENCE_THRESHOLD][['text', 'pseudo_dept']].copy()\n",
    "dept_silver.columns = ['title', 'label']\n",
    "\n",
    "sen_silver = inference_df[inference_df['sen_conf'] >= CONFIDENCE_THRESHOLD][['text', 'pseudo_sen']].copy()\n",
    "sen_silver.columns = ['title', 'label']\n",
    "\n",
    "print(f\"High-confidence pseudo-labels (>{CONFIDENCE_THRESHOLD}):\")\n",
    "print(f\"    Department: {len(dept_silver)} / {len(inference_df)} ({len(dept_silver)/len(inference_df):.1%})\")\n",
    "print(f\"  Seniority:  {len(sen_silver)} / {len(inference_df)} ({len(sen_silver)/len(inference_df):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e63fa3",
   "metadata": {},
   "source": [
    "## 4. Combine Gold + Silver Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0df01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined datasets:\n",
      "\n",
      "Department:\n",
      "  Gold:   10,145\n",
      "  Silver: 2\n",
      "  Total:  10,147\n",
      "\n",
      "Seniority:\n",
      "  Gold:   9,428\n",
      "  Silver: 0\n",
      "  Total:  9,428\n"
     ]
    }
   ],
   "source": [
    "# Combine lookup tables (gold) with pseudo-labeled CVs (silver)\n",
    "dept_gold_df = dept_df[['text', 'label']].rename(columns={'text': 'title'})\n",
    "dept_combined = pd.concat([dept_gold_df, dept_silver], ignore_index=True)\n",
    "\n",
    "sen_gold_df = sen_df[['text', 'label']].rename(columns={'text': 'title'})\n",
    "sen_combined = pd.concat([sen_gold_df, sen_silver], ignore_index=True)\n",
    "\n",
    "print(f\"Combined datasets:\\n\")\n",
    "print(f\"Department:\\n  Gold:   {len(dept_gold_df):,}\\n  Silver: {len(dept_silver):,}\\n  Total:  {len(dept_combined):,}\")\n",
    "print(f\"Seniority:\\n  Gold:   {len(sen_gold_df):,}\\n  Silver: {len(sen_silver):,}\\n  Total:  {len(sen_combined):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d4627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing combined department data...\n",
      "Balancing: 10147 -> 7120 samples\n",
      "  Class distribution: {'Marketing': 1000, 'Sales': 1000, 'Information Technology': 1000, 'Business Development': 620, 'Project Management': 500, 'Consulting': 500, 'Administrative': 500, 'Other': 500, 'Purchasing': 500, 'Customer Support': 500, 'Human Resources': 500}\n",
      "\n",
      "Balancing combined seniority data...\n",
      "Balancing: 9428 -> 4240 samples\n",
      "  Class distribution: {'Senior': 1000, 'Lead': 1000, 'Director': 984, 'Management': 756, 'Junior': 500}\n",
      "\n",
      "Balanced Department Total: 7,120\n",
      "Balanced Seniority Total:  4,240\n"
     ]
    }
   ],
   "source": [
    "# Apply Data Balancing\n",
    "print(\"Balancing combined department data...\")\n",
    "dept_balanced, _ = balance_dataset(dept_combined, min_samples=500, max_samples=1000)\n",
    "\n",
    "print(\"\\nBalancing combined seniority data...\")\n",
    "sen_balanced, _ = balance_dataset(sen_combined, min_samples=500, max_samples=1000)\n",
    "\n",
    "print(f\"Balanced Department Total: {len(dept_balanced):,}\")\n",
    "print(f\"Balanced Seniority Total:  {len(sen_balanced):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb3514",
   "metadata": {},
   "source": [
    "## 5. Train/Val Split on Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department split:\n",
      "  Train: 5,696\n",
      "  Val:   1,424\n",
      "\n",
      "Seniority split:\n",
      "  Train: 3,392\n",
      "  Val:   848\n"
     ]
    }
   ],
   "source": [
    "# Split combined data (80/20)\n",
    "dept_train_texts, dept_val_texts, dept_train_labels, dept_val_labels = train_test_split(\n",
    "    dept_balanced['title'].tolist(), \n",
    "    dept_balanced['label'].tolist(), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "sen_train_texts, sen_val_texts, sen_train_labels, sen_val_labels = train_test_split(\n",
    "    sen_balanced['title'].tolist(), \n",
    "    sen_balanced['label'].tolist(), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Department split:\\n  Train: {len(dept_train_texts):,}\\n  Val:   {len(dept_val_texts):,}\")\n",
    "print(f\"Seniority split:\\n  Train: {len(sen_train_texts):,}\\n  Val:   {len(sen_val_texts):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393c90e",
   "metadata": {},
   "source": [
    "## 6. Create Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa403ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department: 11 classes\n",
      "Seniority: 5 classes\n"
     ]
    }
   ],
   "source": [
    "# Department label mappings\n",
    "dept_unique_labels = sorted(dept_combined['label'].unique())\n",
    "dept_label2id = {label: i for i, label in enumerate(dept_unique_labels)}\n",
    "dept_id2label = {i: label for label, i in dept_label2id.items()}\n",
    "\n",
    "# Seniority label mappings\n",
    "sen_unique_labels = sorted(sen_combined['label'].unique())\n",
    "sen_label2id = {label: i for i, label in enumerate(sen_unique_labels)}\n",
    "sen_id2label = {i: label for label, i in sen_label2id.items()}\n",
    "\n",
    "print(f\"Department: {len(dept_label2id)} classes\")\n",
    "print(f\"Seniority: {len(sen_label2id)} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557ec84",
   "metadata": {},
   "source": [
    "## 7. Train Department Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "üöÄ Training department classifier on combined data...\n",
      "Training on 5696 examples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/1068 01:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.190561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.055764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.049578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "\n",
      "‚úÖ Department classifier (pseudo) training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize department classifier\n",
    "dept_clf = TransformerClassifier(\n",
    "    num_labels=len(dept_label2id),\n",
    "    label2id=dept_label2id, \n",
    "    id2label=dept_id2label\n",
    ")\n",
    "\n",
    "print(f\"Training department classifier on combined data...\")\n",
    "dept_clf.train(\n",
    "    texts=dept_train_texts, \n",
    "    labels=[dept_label2id[l] for l in dept_train_labels], \n",
    "    val_texts=dept_val_texts,\n",
    "    val_labels=[dept_label2id[l] for l in dept_val_labels],\n",
    "    output_dir=\"./models/dept_transformer_pseudo\",\n",
    "    epochs=3,\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "print(\"Department classifier (pseudo) training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05c231",
   "metadata": {},
   "source": [
    "## 8. Train Seniority Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482d0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "üöÄ Training seniority classifier on combined data...\n",
      "Training on 3392 examples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='636' max='636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [636/636 01:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>0.285258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>0.095368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.080650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "\n",
      "‚úÖ Seniority classifier (pseudo) training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize seniority classifier\n",
    "sen_clf = TransformerClassifier(\n",
    "    num_labels=len(sen_label2id),\n",
    "    label2id=sen_label2id, \n",
    "    id2label=sen_id2label\n",
    ")\n",
    "\n",
    "print(f\"Training seniority classifier on combined data...\")\n",
    "sen_clf.train(\n",
    "    texts=sen_train_texts, \n",
    "    labels=[sen_label2id[l] for l in sen_train_labels], \n",
    "    val_texts=sen_val_texts,\n",
    "    val_labels=[sen_label2id[l] for l in sen_val_labels],\n",
    "    output_dir=\"./models/sen_transformer_pseudo\",\n",
    "    epochs=3,\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "print(\"Seniority classifier (pseudo) training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c2a3b",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d3f4a",
   "metadata": {},
   "source": [
    "### 9.1 In-Distribution Evaluation (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2048563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IN-DISTRIBUTION EVALUATION (Validation Set)\n",
      "============================================================\n",
      "Department Accuracy (Val): 0.9937\n",
      "Seniority Accuracy (Val):  0.9800\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Department In-Distribution Evaluation\n",
    "dept_val_preds = dept_clf.predict_labels(dept_val_texts)\n",
    "dept_id_acc = accuracy_score(dept_val_labels, dept_val_preds)\n",
    "sen_val_preds = sen_clf.predict_labels(sen_val_texts)\n",
    "sen_id_acc = accuracy_score(sen_val_labels, sen_val_preds)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IN-DISTRIBUTION EVALUATION (Validation Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Department Accuracy (Val): {dept_id_acc:.4f}\")\n",
    "print(f\"Seniority Accuracy (Val):  {sen_id_acc:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e0a1b",
   "metadata": {},
   "source": [
    "### 9.2 Real-World Evaluation (Annotated CVs)\n",
    "\n",
    "‚ö†Ô∏è **LOADING ANNOTATED DATA FOR EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a2b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded 478 annotated CV positions for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Load annotated dataset for evaluation\n",
    "eval_df = load_evaluation_dataset(DATA_DIR)\n",
    "\n",
    "print(f\"Loaded {len(eval_df)} annotated CV positions for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b3c4d",
   "metadata": {},
   "source": [
    "#### Department Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEPARTMENT CLASSIFICATION RESULTS (Real-World)\n",
      "============================================================\n",
      "Accuracy:          0.3159\n",
      "Precision (macro): 0.4979\n",
      "Recall (macro):    0.4856\n",
      "F1-score (macro):  0.4009\n",
      "F1-score (wtd):    0.2392\n",
      "============================================================\n",
      "\n",
      "Per-Class F1 Scores (Department):\n",
      "  Sales                         : 0.7429\n",
      "  Project Management            : 0.7119\n",
      "  Human Resources               : 0.5333\n",
      "  Purchasing                    : 0.4800\n",
      "  Marketing                     : 0.4762\n",
      "  Consulting                    : 0.4390\n",
      "  Information Technology        : 0.3434\n",
      "  Customer Support              : 0.2857\n",
      "  Business Development          : 0.2703\n",
      "  Administrative                : 0.1111\n",
      "  Other                         : 0.0158\n"
     ]
    }
   ],
   "source": [
    "# Predict on evaluation set\n",
    "eval_titles = eval_df['title'].tolist()\n",
    "dept_predictions = dept_clf.predict_labels(eval_titles)\n",
    "\n",
    "# Ground truth\n",
    "dept_true = eval_df['department'].tolist()\n",
    "\n",
    "# Calculate metrics\n",
    "dept_accuracy = accuracy_score(dept_true, dept_predictions)\n",
    "dept_precision, dept_recall, dept_f1, _ = precision_recall_fscore_support(\n",
    "    dept_true, dept_predictions, average='macro', zero_division=0\n",
    ")\n",
    "dept_weighted_f1 = precision_recall_fscore_support(\n",
    "    dept_true, dept_predictions, average='weighted', zero_division=0\n",
    ")[2]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEPARTMENT CLASSIFICATION RESULTS (Real-World)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:          {dept_accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {dept_precision:.4f}\")\n",
    "print(f\"Recall (macro):    {dept_recall:.4f}\")\n",
    "print(f\"F1-score (macro):  {dept_f1:.4f}\")\n",
    "print(f\"F1-score (wtd):    {dept_weighted_f1:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Per-class F1 scores\n",
    "dept_report = classification_report(dept_true, dept_predictions, output_dict=True, zero_division=0)\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores (Department):\")\n",
    "dept_f1_scores = {label: metrics['f1-score'] for label, metrics in dept_report.items() \n",
    "                  if label not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "for label, f1 in sorted(dept_f1_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {label:<30}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c7d8e",
   "metadata": {},
   "source": [
    "#### Seniority Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c2d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SENIORITY CLASSIFICATION RESULTS (Real-World)\n",
      "============================================================\n",
      "Accuracy:          0.4749\n",
      "Precision (macro): 0.4123\n",
      "Recall (macro):    0.6183\n",
      "F1-score (macro):  0.4388\n",
      "F1-score (wtd):    0.4363\n",
      "============================================================\n",
      "\n",
      "Per-Class F1 Scores (Seniority):\n",
      "  Management                    : 0.7823\n",
      "  Director                      : 0.7143\n",
      "  Lead                          : 0.6474\n",
      "  Senior                        : 0.3366\n",
      "  Junior                        : 0.1519\n",
      "  Professional                  : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Predict on evaluation set\n",
    "sen_predictions = sen_clf.predict_labels(eval_titles)\n",
    "\n",
    "# Ground truth\n",
    "sen_true = eval_df['seniority'].tolist()\n",
    "\n",
    "# Calculate metrics\n",
    "sen_accuracy = accuracy_score(sen_true, sen_predictions)\n",
    "sen_precision, sen_recall, sen_f1, _ = precision_recall_fscore_support(\n",
    "    sen_true, sen_predictions, average='macro', zero_division=0\n",
    ")\n",
    "sen_weighted_f1 = precision_recall_fscore_support(\n",
    "    sen_true, sen_predictions, average='weighted', zero_division=0\n",
    ")[2]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENIORITY CLASSIFICATION RESULTS (Real-World)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:          {sen_accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {sen_precision:.4f}\")\n",
    "print(f\"Recall (macro):    {sen_recall:.4f}\")\n",
    "print(f\"F1-score (macro):  {sen_f1:.4f}\")\n",
    "print(f\"F1-score (wtd):    {sen_weighted_f1:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Per-class F1 scores\n",
    "sen_report = classification_report(sen_true, sen_predictions, output_dict=True, zero_division=0)\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores (Seniority):\")\n",
    "sen_f1_scores = {label: metrics['f1-score'] for label, metrics in sen_report.items() \n",
    "                 if label not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "for label, f1 in sorted(sen_f1_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {label:<30}: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
