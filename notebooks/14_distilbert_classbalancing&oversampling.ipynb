{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c2de45",
   "metadata": {},
   "source": [
    "Zelle 1: Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b339560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install -U transformers datasets accelerate scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9cd347",
   "metadata": {},
   "source": [
    "Zelle 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c84e4",
   "metadata": {},
   "source": [
    "Zelle 3: Pfade + Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPT_CSV = \"../data/department-v2.csv\"\n",
    "CV_ANN   = \"../data/linkedin-cvs-annotated.json\"\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "MAX_LEN = 32\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97db37b",
   "metadata": {},
   "source": [
    "Zelle 4: Trainingsdaten laden (Lookup CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_df = pd.read_csv(DEPT_CSV)\n",
    "dept_df = dept_df.dropna(subset=[\"text\", \"label\"]).copy()\n",
    "dept_df[\"text\"] = dept_df[\"text\"].astype(str).str.strip()\n",
    "dept_df[\"label\"] = dept_df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Train rows:\", len(dept_df))\n",
    "display(dept_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992200d2",
   "metadata": {},
   "source": [
    "Zelle 5: Eval-Daten laden (annotated JSON) und flatten + ACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402196f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CV_ANN, \"r\", encoding=\"utf-8\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "# ann: list[list[dict]] -> flatten\n",
    "positions = [p for cv in ann for p in cv]\n",
    "eval_df = pd.DataFrame(positions)\n",
    "\n",
    "# ACTIVE\n",
    "eval_df[\"status\"] = eval_df[\"status\"].astype(str).str.upper()\n",
    "eval_df = eval_df[eval_df[\"status\"] == \"ACTIVE\"].copy()\n",
    "\n",
    "# position -> title\n",
    "eval_df[\"title\"] = eval_df[\"position\"].astype(str).str.strip()\n",
    "\n",
    "# labels clean\n",
    "eval_df[\"department\"] = eval_df[\"department\"].astype(str).str.strip()\n",
    "\n",
    "eval_df = eval_df[[\"title\", \"department\"]].dropna().copy()\n",
    "\n",
    "print(\"Eval ACTIVE rows:\", len(eval_df))\n",
    "display(eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c523d",
   "metadata": {},
   "source": [
    "Zelle 6: Klassenverteilung (Train vs Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dist(series, name):\n",
    "    vc = series.value_counts()\n",
    "    df = pd.DataFrame({\"count\": vc, \"pct\": (vc / vc.sum() * 100).round(2)})\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    display(df)\n",
    "\n",
    "show_dist(dept_df[\"label\"], \"Department TRAIN (lookup)\")\n",
    "show_dist(eval_df[\"department\"], \"Department EVAL (annotated ACTIVE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea9e13",
   "metadata": {},
   "source": [
    "Zelle 7: LabelEncoder + Train/Val Split (nur aus Lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "dept_df[\"y\"] = le.fit_transform(dept_df[\"label\"])\n",
    "\n",
    "# stratified split nur aus Lookup\n",
    "train_part, val_part = train_test_split(\n",
    "    dept_df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=dept_df[\"y\"]\n",
    ")\n",
    "\n",
    "print(\"Train split:\", len(train_part), \"Val split:\", len(val_part))\n",
    "print(\"Num classes:\", len(le.classes_))\n",
    "print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c5601",
   "metadata": {},
   "source": [
    "Zelle 8: Class Weights berechnen + “nach balancing” anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_int, num_classes):\n",
    "    counts = np.bincount(y_int, minlength=num_classes)\n",
    "    total = counts.sum()\n",
    "    weights = total / (num_classes * counts)  # N / (K * n_c)\n",
    "    return counts, weights\n",
    "\n",
    "def oversample_to_median(df, label_col=\"y\", random_state=42):\n",
    "    vc = df[label_col].value_counts()\n",
    "    target = int(vc.median())  # moderat\n",
    "\n",
    "    parts = []\n",
    "    for cls, n in vc.items():\n",
    "        df_c = df[df[label_col] == cls]\n",
    "        if n < target:\n",
    "            df_c = df_c.sample(target, replace=True, random_state=random_state)\n",
    "        parts.append(df_c)\n",
    "\n",
    "    return pd.concat(parts).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Oversampling NUR Train-Split\n",
    "train_part_os = oversample_to_median(train_part, label_col=\"y\", random_state=SEED)\n",
    "\n",
    "print(\"\\n--- Train BEFORE oversampling ---\")\n",
    "display(train_part[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\n--- Train AFTER oversampling (median target) ---\")\n",
    "display(train_part_os[\"label\"].value_counts())\n",
    "\n",
    "# Class weights auf dem oversampleten Train berechnen (oder alternativ auf original train_part)\n",
    "counts, weights = compute_class_weights(train_part_os[\"y\"].values, len(le.classes_))\n",
    "\n",
    "balance_df = pd.DataFrame({\n",
    "    \"class\": le.classes_,\n",
    "    \"count\": counts,\n",
    "    \"pct\": (counts / counts.sum() * 100).round(2),\n",
    "    \"weight\": weights.round(4),\n",
    "})\n",
    "balance_df[\"count_x_weight\"] = (balance_df[\"count\"] * balance_df[\"weight\"]).round(4)\n",
    "\n",
    "print(\"\\n--- Oversampled train distribution + class weights (Loss-Balancing) ---\")\n",
    "display(balance_df.sort_values(\"count\", ascending=False))\n",
    "print(\"Sum(count_x_weight):\", balance_df[\"count_x_weight\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0fedd",
   "metadata": {},
   "source": [
    "Zelle 9: Weighted Trainer (CrossEntropy mit weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab52c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a1ef8",
   "metadata": {},
   "source": [
    "Zelle 10: Tokenizer + Dataset Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def make_ds(df, text_col, y_col=None):\n",
    "    data = {\"text\": df[text_col].astype(str).tolist()}\n",
    "    if y_col is not None:\n",
    "        data[\"labels\"] = df[y_col].astype(int).tolist()\n",
    "    ds = Dataset.from_dict(data)\n",
    "\n",
    "    def tok(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "    return ds.map(tok, batched=True)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_ds = make_ds(train_part_os, \"text\", \"y\")\n",
    "val_ds   = make_ds(val_part, \"text\", \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de521f4",
   "metadata": {},
   "source": [
    "Zelle 11: Training-Runner (für LR Sweep) mit Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d18e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(lr, max_epochs=20, batch_size=64, patience=3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(le.classes_)\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./out_dept_lr{lr}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "\n",
    "        learning_rate=lr,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.06,\n",
    "        weight_decay=0.01,\n",
    "\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=max_epochs,\n",
    "\n",
    "        logging_steps=50,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "            \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        }\n",
    "\n",
    "    cw = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=cw,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    best_val = trainer.evaluate()\n",
    "    return trainer, best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca7c46",
   "metadata": {},
   "source": [
    "Zelle 12: LR Sweep (nur auf Val aus Lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb413af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [3e-5, 2e-5]\n",
    "results = []\n",
    "trainers = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    print(f\"\\n### LR = {lr} ###\")\n",
    "    trainer, m = run_train(lr=lr, max_epochs=20, batch_size=64, patience=3)\n",
    "    trainers[lr] = trainer\n",
    "    results.append({\n",
    "        \"lr\": lr,\n",
    "        \"val_f1_macro\": m.get(\"eval_f1_macro\"),\n",
    "        \"val_accuracy\": m.get(\"eval_accuracy\"),\n",
    "        \"val_f1_weighted\": m.get(\"eval_f1_weighted\"),\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"val_f1_macro\", ascending=False)\n",
    "display(res_df)\n",
    "\n",
    "best_lr = res_df.iloc[0][\"lr\"]\n",
    "print(\"Best LR:\", best_lr)\n",
    "best_trainer = trainers[best_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f2d9d",
   "metadata": {},
   "source": [
    "Zelle 13: Finale Evaluation auf eval_df (nur messen, kein Training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fded9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df Labels auf Train-Labelset filtern\n",
    "eval_use = eval_df[eval_df[\"department\"].isin(set(le.classes_))].copy()\n",
    "print(\"Eval after label filter:\", len(eval_use))\n",
    "\n",
    "# encode eval labels\n",
    "y_eval = le.transform(eval_use[\"department\"].astype(str))\n",
    "\n",
    "# dataset for eval\n",
    "eval_use_ds = Dataset.from_dict({\"text\": eval_use[\"title\"].astype(str).tolist(), \"labels\": y_eval.tolist()})\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "eval_use_ds = eval_use_ds.map(tok, batched=True)\n",
    "\n",
    "pred = best_trainer.predict(eval_use_ds)\n",
    "pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "pred_labels = le.inverse_transform(pred_ids)\n",
    "\n",
    "y_true = eval_use[\"department\"].astype(str).values\n",
    "y_pred = pred_labels.astype(str)\n",
    "\n",
    "print(\"\\n=== FINAL EVAL on eval_df (Department) ===\")\n",
    "print(\"Accuracy       :\", accuracy_score(y_true, y_pred))\n",
    "print(\"Macro Precision:\", precision_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "print(\"Macro Recall   :\", recall_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "print(\"Macro F1       :\", f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "print(\"Weighted F1    :\", f1_score(y_true, y_pred, average=\"weighted\", zero_division=0))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "# optional: ein paar predictions anschauen\n",
    "out_preview = eval_use[[\"title\", \"department\"]].copy()\n",
    "out_preview[\"pred\"] = y_pred\n",
    "out_preview[\"correct\"] = out_preview[\"department\"] == out_preview[\"pred\"]\n",
    "display(out_preview.head(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
