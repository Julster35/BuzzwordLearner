{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - TF-IDF + Logistic Regression (Semi-Supervised)\n",
    "\n",
    "**Approach**: Train a linear classifier on TF-IDF features from lookup tables **and pseudo-labeled LinkedIn CVs**.\n",
    "\n",
    "This approach uses a classic machine learning pipeline:\n",
    "1. Load Gold (CSV) and Silver (Pseudo-labeled JSON) data.\n",
    "2. Split Gold data into train/val set.\n",
    "3. Train on **Gold Train + Silver**.\n",
    "4. Evaluate on **Gold Val** (In-Distribution) and **Annotated JSON** (Real-World)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data.loader import load_label_lists, load_inference_dataset, load_evaluation_dataset, balance_dataset\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "RESULTS_DIR = Path('./results')\n",
    "MODELS_DIR = Path('../models')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding fix...\n",
      "Deduplicating department labels...\n",
      "  Deduplication: 10145 -> 10145 (removed 0 duplicates)\n",
      "  Capping: 10145 -> 2597 (max 500 per class)\n",
      "Deduplicating seniority labels...\n",
      "  Deduplication: 9428 -> 9428 (removed 0 duplicates)\n",
      "  Capping: 9428 -> 2409 (max 500 per class)\n",
      "ðŸ¥ˆ Loaded 314 dept and 314 seniority silver labels\n",
      "\n",
      "ðŸ“Š Gold Department: 2,597 examples\n",
      "ðŸ“Š Gold Seniority:  2,409 examples\n"
     ]
    }
   ],
   "source": [
    "# Load lookup tables (Gold)\n",
    "dept_df, sen_df = load_label_lists(DATA_DIR)\n",
    "\n",
    "# Load pseudo-labeled data (Silver)\n",
    "silver_path = DATA_DIR / \"processed/unannotated_pseudo_labeled.csv\"\n",
    "if silver_path.exists():\n",
    "    silver_df = pd.read_csv(silver_path)\n",
    "    # Silver DF has 'title' instead of 'text'\n",
    "    dept_silver = silver_df[silver_df['dept_pseudo'].notna()][['title', 'dept_pseudo']].copy()\n",
    "    dept_silver = dept_silver.rename(columns={'dept_pseudo': 'label', 'title': 'text'})\n",
    "    \n",
    "    sen_silver = silver_df[silver_df['sen_pseudo'].notna()][['title', 'sen_pseudo']].copy()\n",
    "    sen_silver = sen_silver.rename(columns={'sen_pseudo': 'label', 'title': 'text'})\n",
    "    \n",
    "    print(f\"ðŸ¥ˆ Loaded {len(dept_silver)} dept and {len(sen_silver)} seniority silver labels\")\n",
    "else:\n",
    "    dept_silver = pd.DataFrame(columns=['text', 'label'])\n",
    "    sen_silver = pd.DataFrame(columns=['text', 'label'])\n",
    "    print(\"âš ï¸ Silver data not found\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Gold Department: {len(dept_df):,} examples\")\n",
    "print(f\"ðŸ“Š Gold Seniority:  {len(sen_df):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department: 2077 gold + 314 silver\n",
      "Seniority:  1927 gold + 314 silver\n",
      "Balancing: 2391 -> 5500 samples\n",
      "  Class distribution: {'Sales': 500, 'Business Development': 500, 'Marketing': 500, 'Information Technology': 500, 'Project Management': 500, 'Consulting': 500, 'Administrative': 500, 'Other': 500, 'Purchasing': 500, 'Human Resources': 500, 'Customer Support': 500}\n",
      "Balancing: 2241 -> 2500 samples\n",
      "  Class distribution: {'Management': 500, 'Senior': 500, 'Lead': 500, 'Director': 500, 'Junior': 500}\n",
      "\n",
      "Balanced Department: 5500 examples\n",
      "Balanced Seniority:  2500 examples\n"
     ]
    }
   ],
   "source": [
    "# Split Gold into train/val\n",
    "dept_train_gold_texts, dept_val_texts, dept_train_gold_labels, dept_val_labels = train_test_split(\n",
    "    dept_df['text'].tolist(),\n",
    "    dept_df['label'].tolist(),\n",
    "    test_size=0.2, random_state=42, stratify=dept_df['label']\n",
    ")\n",
    "\n",
    "sen_train_gold_texts, sen_val_texts, sen_train_gold_labels, sen_val_labels = train_test_split(\n",
    "    sen_df['text'].tolist(),\n",
    "    sen_df['label'].tolist(),\n",
    "    test_size=0.2, random_state=42, stratify=sen_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Department: {len(dept_train_gold_texts)} gold + {len(dept_silver)} silver\")\n",
    "print(f\"Seniority:  {len(sen_train_gold_texts)} gold + {len(sen_silver)} silver\")\n",
    "\n",
    "# Combine with Silver for training\n",
    "dept_train_texts = dept_train_gold_texts + dept_silver['text'].tolist()\n",
    "dept_train_labels = dept_train_gold_labels + dept_silver['label'].tolist()\n",
    "\n",
    "sen_train_texts = sen_train_gold_texts + sen_silver['text'].tolist()\n",
    "sen_train_labels = sen_train_gold_labels + sen_silver['label'].tolist()\n",
    "\n",
    "# Apply Tier 1 Data Balancing\n",
    "dept_train_df = pd.DataFrame({'text': dept_train_texts, 'label': dept_train_labels})\n",
    "dept_train_df, dept_weights = balance_dataset(dept_train_df, min_samples=500, max_samples=2000, return_weights=True)\n",
    "dept_train_texts = dept_train_df['text'].tolist()\n",
    "dept_train_labels = dept_train_df['label'].tolist()\n",
    "\n",
    "sen_train_df = pd.DataFrame({'text': sen_train_texts, 'label': sen_train_labels})\n",
    "sen_train_df, sen_weights = balance_dataset(sen_train_df, min_samples=500, max_samples=2000, return_weights=True)\n",
    "sen_train_texts = sen_train_df['text'].tolist()\n",
    "sen_train_labels = sen_train_df['label'].tolist()\n",
    "\n",
    "print(f\"\\nBalanced Department: {len(dept_train_texts)} examples\")\n",
    "print(f\"Balanced Seniority:  {len(sen_train_texts)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Department Classifier: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department TF-IDF features: 3,012\n",
      "Training matrix shape: (5500, 3012)\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer for departments\n",
    "dept_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "dept_X_train = dept_vectorizer.fit_transform(dept_train_texts)\n",
    "dept_X_val = dept_vectorizer.transform(dept_val_texts)\n",
    "\n",
    "print(f\"Department TF-IDF features: {dept_X_train.shape[1]:,}\")\n",
    "print(f\"Training matrix shape: {dept_X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Department Logistic Regression...\n",
      "\n",
      "âœ… Department Logistic Regression trained\n",
      "   Validation accuracy (In-Distribution): 0.9327\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression classifier\n",
    "print(\"Training Department Logistic Regression...\")\n",
    "dept_logreg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "dept_logreg.fit(dept_X_train, dept_train_labels)\n",
    "\n",
    "# Evaluate on validation set (In-distribution)\n",
    "dept_val_preds = dept_logreg.predict(dept_X_val)\n",
    "dept_id_acc = accuracy_score(dept_val_labels, dept_val_preds)\n",
    "print(f\"\\nâœ… Department Logistic Regression trained\")\n",
    "print(f\"   Validation accuracy (In-Distribution): {dept_id_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seniority Classifier: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seniority TF-IDF features: 1,550\n",
      "Training matrix shape: (2500, 1550)\n"
     ]
    }
   ],
   "source": [
    "sen_vectorizer = TfidfVectorizer(\n",
    "    max_features=3000, \n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "sen_X_train = sen_vectorizer.fit_transform(sen_train_texts)\n",
    "sen_X_val = sen_vectorizer.transform(sen_val_texts)\n",
    "\n",
    "print(f\"Seniority TF-IDF features: {sen_X_train.shape[1]:,}\")\n",
    "print(f\"Training matrix shape: {sen_X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Seniority Logistic Regression...\n",
      "\n",
      "âœ… Seniority Logistic Regression trained\n",
      "   Validation accuracy (In-Distribution): 0.9523\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Seniority Logistic Regression...\")\n",
    "sen_logreg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "sen_logreg.fit(sen_X_train, sen_train_labels)\n",
    "\n",
    "sen_val_preds = sen_logreg.predict(sen_X_val)\n",
    "sen_id_acc = accuracy_score(sen_val_labels, sen_val_preds)\n",
    "print(f\"\\nâœ… Seniority Logistic Regression trained\")\n",
    "print(f\"   Validation accuracy (In-Distribution): {sen_id_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation on Annotated Dataset (Real-World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loaded 478 annotated CV positions for evaluation\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS (TF-IDF + Logistic Regression)\n",
      "============================================================\n",
      "Department - In-Dist: 0.9327, Real-World: 0.3891\n",
      "Seniority  - In-Dist: 0.9523, Real-World: 0.4163\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "eval_df = load_evaluation_dataset(DATA_DIR)\n",
    "eval_titles = eval_df['title'].tolist()\n",
    "print(f\"ðŸ“Š Loaded {len(eval_df)} annotated CV positions for evaluation\")\n",
    "\n",
    "# Department Evaluation\n",
    "eval_dept_X = dept_vectorizer.transform(eval_titles)\n",
    "dept_predictions = dept_logreg.predict(eval_dept_X)\n",
    "dept_true = eval_df['department'].tolist()\n",
    "dept_accuracy = accuracy_score(dept_true, dept_predictions)\n",
    "dept_precision, dept_recall, dept_f1, _ = precision_recall_fscore_support(dept_true, dept_predictions, average='macro', zero_division=0)\n",
    "dept_weighted_f1 = precision_recall_fscore_support(dept_true, dept_predictions, average='weighted', zero_division=0)[2]\n",
    "dept_report = classification_report(dept_true, dept_predictions, output_dict=True, zero_division=0)\n",
    "dept_f1_scores = {label: metrics['f1-score'] for label, metrics in dept_report.items() if label not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "\n",
    "# Seniority Evaluation\n",
    "eval_sen_X = sen_vectorizer.transform(eval_titles)\n",
    "sen_predictions = sen_logreg.predict(eval_sen_X)\n",
    "sen_true = eval_df['seniority'].tolist()\n",
    "sen_accuracy = accuracy_score(sen_true, sen_predictions)\n",
    "sen_precision, sen_recall, sen_f1, _ = precision_recall_fscore_support(sen_true, sen_predictions, average='macro', zero_division=0)\n",
    "sen_weighted_f1 = precision_recall_fscore_support(sen_true, sen_predictions, average='weighted', zero_division=0)[2]\n",
    "sen_report = classification_report(sen_true, sen_predictions, output_dict=True, zero_division=0)\n",
    "sen_f1_scores = {label: metrics['f1-score'] for label, metrics in sen_report.items() if label not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS (TF-IDF + Logistic Regression)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Department - In-Dist: {dept_id_acc:.4f}, Real-World: {dept_accuracy:.4f}\")\n",
    "print(f\"Seniority  - In-Dist: {sen_id_acc:.4f}, Real-World: {sen_accuracy:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to: results\\tfidf_logreg_results.json\n",
      "âœ… Models saved to models/ directory\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"approach\": \"TF-IDF + Logistic Regression\",\n",
    "    \"department\": {\n",
    "        \"in_distribution_accuracy\": float(dept_id_acc),\n",
    "        \"real_world_accuracy\": float(dept_accuracy),\n",
    "        \"real_world_precision\": float(dept_precision),\n",
    "        \"real_world_recall\": float(dept_recall),\n",
    "        \"real_world_f1_macro\": float(dept_f1),\n",
    "        \"real_world_f1_weighted\": float(dept_weighted_f1),\n",
    "        \"per_class_f1\": {k: float(v) for k, v in dept_f1_scores.items()}\n",
    "    },\n",
    "    \"seniority\": {\n",
    "        \"in_distribution_accuracy\": float(sen_id_acc),\n",
    "        \"real_world_accuracy\": float(sen_accuracy),\n",
    "        \"real_world_precision\": float(sen_precision),\n",
    "        \"real_world_recall\": float(sen_recall),\n",
    "        \"real_world_f1_macro\": float(sen_f1),\n",
    "        \"real_world_f1_weighted\": float(sen_weighted_f1),\n",
    "        \"per_class_f1\": {k: float(v) for k, v in sen_f1_scores.items()}\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"training_samples\": len(dept_train_texts) + len(sen_train_texts),\n",
    "        \"dept_train_samples\": len(dept_train_texts),\n",
    "        \"dept_vocab_size\": len(dept_vectorizer.vocabulary_),\n",
    "        \"sen_train_samples\": len(sen_train_texts),\n",
    "        \"sen_vocab_size\": len(sen_vectorizer.vocabulary_),\n",
    "        \"max_features\": {\"department\": 5000, \"seniority\": 3000},\n",
    "        \"ngram_range\": \"(1, 2)\",\n",
    "        \"C\": 1.0\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "output_path = RESULTS_DIR / 'tfidf_logreg_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to: {output_path}\")\n",
    "\n",
    "# Save models\n",
    "with open(MODELS_DIR / 'tfidf_logreg_department.pkl', 'wb') as f:\n",
    "    pickle.dump({'vectorizer': dept_vectorizer, 'classifier': dept_logreg}, f)\n",
    "with open(MODELS_DIR / 'tfidf_logreg_seniority.pkl', 'wb') as f:\n",
    "    pickle.dump({'vectorizer': sen_vectorizer, 'classifier': sen_logreg}, f)\n",
    "print(\"âœ… Models saved to models/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
