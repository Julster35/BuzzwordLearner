{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - DistilBERT Improvements\n",
    "\n",
    "Dieses Notebook setzt die vorgeschlagenen Verbesserungen um:\n",
    "1) Zwei-stufiges Fine-Tuning (Lookup -> CV-Train)\n",
    "2) Backbone Vergleich (andere Modelle)\n",
    "3) Label Smoothing und max_length Tests\n",
    "4) Input Varianten (title vs text vs title+history)\n",
    "5) Silver Data Augmentation (optional)\n",
    "\n",
    "Hinweis: Einige Experimente trainieren auf annotierten CVs.\n",
    "Das ist nicht mehr strikt zero-shot, sondern supervised domain adaptation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Wir importieren alle benoetigten Bibliotheken, definieren Seeds und Basis-Konfigurationen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('/Users/batuklkn/Desktop/GustAbgabe/BuzzwordLearner/data')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "BASE_MODEL = 'distilbert-base-multilingual-cased'\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    'model_name': BASE_MODEL,\n",
    "    'epochs': 3,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'use_class_weights': False\n",
    "}\n",
    "\n",
    "RUN_TWO_STAGE = True\n",
    "RUN_BACKBONE_TESTS = True\n",
    "RUN_LABEL_SMOOTHING = True\n",
    "RUN_INPUT_VARIANTS = True\n",
    "RUN_SILVER = False\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "\n",
    "def _fix_encoding(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        if 'Ãƒ' in text:\n",
    "            return text.encode('latin-1').decode('utf-8', errors='ignore')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "\n",
    "def deduplicate_label_df(label_df, max_per_class=500):\n",
    "    label_df = label_df.copy()\n",
    "    label_df['text_normalized'] = label_df['text'].str.lower().str.strip()\n",
    "    original_count = len(label_df)\n",
    "    label_df = label_df.drop_duplicates(subset=['text_normalized', 'label'])\n",
    "    dedup_count = len(label_df)\n",
    "\n",
    "    if max_per_class is not None:\n",
    "        label_df = label_df.groupby('label', group_keys=False).apply(\n",
    "            lambda x: x.sample(min(len(x), max_per_class), random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    final_count = len(label_df)\n",
    "    label_df = label_df.drop(columns=['text_normalized'])\n",
    "\n",
    "    print(f\"  Deduplication: {original_count} -> {dedup_count} (removed {original_count - dedup_count} duplicates)\")\n",
    "    if max_per_class is not None:\n",
    "        print(f\"  Capping: {dedup_count} -> {final_count} (max {max_per_class} per class)\")\n",
    "\n",
    "    return label_df\n",
    "\n",
    "\n",
    "def load_linkedin_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_dataset(cvs, include_history=False):\n",
    "    records = []\n",
    "\n",
    "    for cv_idx, cv in enumerate(cvs):\n",
    "        if isinstance(cv, list):\n",
    "            positions = cv\n",
    "        else:\n",
    "            positions = cv.get('positions', cv) if isinstance(cv, dict) else []\n",
    "\n",
    "        active_positions = [p for p in positions if p.get('status') == 'ACTIVE']\n",
    "\n",
    "        if not active_positions:\n",
    "            continue\n",
    "\n",
    "        active = active_positions[0]\n",
    "\n",
    "        title = active.get('position', active.get('title', ''))\n",
    "        company = active.get('organization', active.get('companyName', ''))\n",
    "\n",
    "        record = {\n",
    "            'cv_id': cv_idx,\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'text': f\"{title} at {company}\".strip() if company else title,\n",
    "        }\n",
    "\n",
    "        if 'department' in active:\n",
    "            record['department'] = active['department']\n",
    "        if 'seniority' in active:\n",
    "            record['seniority'] = active['seniority']\n",
    "\n",
    "        if include_history:\n",
    "            past_positions = [p for p in positions if p.get('status') != 'ACTIVE']\n",
    "            record['history'] = ' | '.join([\n",
    "                p.get('position', p.get('title', '')) for p in past_positions\n",
    "            ])\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def load_label_lists(data_dir, fix_encoding=True, deduplicate=True, max_per_class=500):\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    department_df = pd.read_csv(data_path / 'department-v2.csv', encoding='utf-8')\n",
    "    seniority_df = pd.read_csv(data_path / 'seniority-v2.csv', encoding='utf-8')\n",
    "\n",
    "    if fix_encoding:\n",
    "        print('Applying encoding fix...')\n",
    "        department_df['text'] = department_df['text'].apply(_fix_encoding)\n",
    "        seniority_df['text'] = seniority_df['text'].apply(_fix_encoding)\n",
    "\n",
    "    if deduplicate:\n",
    "        print('Deduplicating department labels...')\n",
    "        department_df = deduplicate_label_df(department_df, max_per_class)\n",
    "        print('Deduplicating seniority labels...')\n",
    "        seniority_df = deduplicate_label_df(seniority_df, max_per_class)\n",
    "\n",
    "    return department_df, seniority_df\n",
    "\n",
    "\n",
    "def load_evaluation_dataset(data_dir):\n",
    "    data_path = Path(data_dir)\n",
    "    cvs = load_linkedin_data(str(data_path / 'linkedin-cvs-annotated.json'))\n",
    "    return prepare_dataset(cvs)\n",
    "\n",
    "\n",
    "class JobTitleDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            weight = torch.tensor(self.class_weights, device=logits.device, dtype=logits.dtype)\n",
    "            loss_fn = CrossEntropyLoss(weight=weight)\n",
    "        else:\n",
    "            loss_fn = CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "class TransformerClassifier:\n",
    "    def __init__(self, model_name='distilbert-base-multilingual-cased', num_labels=2, id2label=None, label2id=None):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.id2label = id2label or {}\n",
    "        self.label2id = label2id or {}\n",
    "\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "\n",
    "    def train(self, texts, labels, val_texts=None, val_labels=None, epochs=3, batch_size=16, learning_rate=1e-5, warmup_ratio=0.1, use_class_weights=False):\n",
    "        print(f\"Training on {len(texts)} examples...\")\n",
    "\n",
    "        train_encodings = self.tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "        train_dataset = JobTitleDataset(train_encodings, labels)\n",
    "\n",
    "        if val_texts is not None and val_labels is not None:\n",
    "            val_encodings = self.tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "            val_dataset = JobTitleDataset(val_encodings, val_labels)\n",
    "            eval_strategy = 'epoch'\n",
    "        else:\n",
    "            val_dataset = None\n",
    "            eval_strategy = 'no'\n",
    "\n",
    "        class_weights = None\n",
    "        if use_class_weights:\n",
    "            from sklearn.utils.class_weight import compute_class_weight\n",
    "            unique_labels = np.unique(labels)\n",
    "            class_weights = compute_class_weight('balanced', classes=unique_labels, y=labels)\n",
    "            print(f\"Using class weights: {dict(zip(unique_labels, class_weights))}\")\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=tmp_dir,\n",
    "                num_train_epochs=epochs,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                warmup_ratio=warmup_ratio,\n",
    "                weight_decay=0.01,\n",
    "                evaluation_strategy=eval_strategy,\n",
    "                save_strategy='no',\n",
    "                logging_strategy='no',\n",
    "                learning_rate=learning_rate,\n",
    "                report_to='none',\n",
    "                disable_tqdm=True\n",
    "            )\n",
    "\n",
    "            if class_weights is not None:\n",
    "                trainer = WeightedTrainer(\n",
    "                    class_weights=class_weights,\n",
    "                    model=self.model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=val_dataset\n",
    "                )\n",
    "            else:\n",
    "                trainer = Trainer(\n",
    "                    model=self.model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=val_dataset\n",
    "                )\n",
    "\n",
    "            trainer.train()\n",
    "\n",
    "        print('Training complete!')\n",
    "\n",
    "    def predict(self, texts, batch_size=32):\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts, padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().tolist())\n",
    "\n",
    "        return all_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten laden\n",
    "\n",
    "Wir laden Lookup-Tabellen und die annotierten CVs. Fuer Input-Varianten nutzen wir\n",
    "auch die CV-History.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup-Tabellen (Training)\n",
    "dept_df, sen_df = load_label_lists(\n",
    "    DATA_DIR,\n",
    "    fix_encoding=True,\n",
    "    deduplicate=True,\n",
    "    max_per_class=None\n",
    ")\n",
    "\n",
    "# Annotierte CVs (Evaluation)\n",
    "eval_df = load_evaluation_dataset(DATA_DIR)\n",
    "\n",
    "# Volle CVs mit History fuer Input-Experimente\n",
    "cvs_annotated = load_linkedin_data(DATA_DIR / 'linkedin-cvs-annotated.json')\n",
    "cv_df_full = prepare_dataset(cvs_annotated, include_history=True)\n",
    "\n",
    "# Titel + History kombinieren\n",
    "history = cv_df_full['history'].fillna('')\n",
    "cv_df_full['title_history'] = np.where(\n",
    "    history != '',\n",
    "    cv_df_full['title'].fillna('') + ' | ' + history,\n",
    "    cv_df_full['title'].fillna('')\n",
    ")\n",
    "\n",
    "print(f\"Department lookup: {len(dept_df):,} examples\")\n",
    "print(f\"Seniority lookup:  {len(sen_df):,} examples\")\n",
    "print(f\"Annotated CVs:     {len(eval_df):,} positions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hilfsfunktionen\n",
    "\n",
    "Wir kapseln wiederholte Logik fuer Splits, Mappings, Training und Evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_maps(label_series):\n",
    "    labels = sorted(label_series.unique())\n",
    "    label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "def split_by_cv_id(df, train_size=0.7, val_size=0.15):\n",
    "    cv_ids = df['cv_id'].unique()\n",
    "    train_ids, temp_ids = train_test_split(\n",
    "        cv_ids, test_size=1 - train_size, random_state=RANDOM_STATE\n",
    "    )\n",
    "    val_ids, test_ids = train_test_split(\n",
    "        temp_ids, test_size=0.5, random_state=RANDOM_STATE\n",
    "    )\n",
    "    train_df = df[df['cv_id'].isin(train_ids)].copy()\n",
    "    val_df = df[df['cv_id'].isin(val_ids)].copy()\n",
    "    test_df = df[df['cv_id'].isin(test_ids)].copy()\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def compute_metrics(true_ids, pred_ids, id2label):\n",
    "    acc = accuracy_score(true_ids, pred_ids)\n",
    "    precision, recall, f1_macro, _ = precision_recall_fscore_support(\n",
    "        true_ids, pred_ids, average='macro', zero_division=0\n",
    "    )\n",
    "    f1_weighted = precision_recall_fscore_support(\n",
    "        true_ids, pred_ids, average='weighted', zero_division=0\n",
    "    )[2]\n",
    "\n",
    "    labels = sorted(set(true_ids) | set(pred_ids))\n",
    "    _, _, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        true_ids, pred_ids, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    per_class_f1 = {id2label[i]: float(f1_per_class[idx]) for idx, i in enumerate(labels)}\n",
    "\n",
    "    return {\n",
    "        'accuracy': float(acc),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'per_class_f1': per_class_f1\n",
    "    }\n",
    "\n",
    "\n",
    "def print_summary(name, results):\n",
    "    in_acc = results['in_distribution']['accuracy']\n",
    "    in_f1 = results['in_distribution']['f1_macro']\n",
    "    if results.get('cv_test'):\n",
    "        rw_acc = results['cv_test']['accuracy']\n",
    "        rw_f1 = results['cv_test']['f1_macro']\n",
    "        print(f\"{name} | in-dist acc {in_acc:.4f} f1 {in_f1:.4f} | cv-test acc {rw_acc:.4f} f1 {rw_f1:.4f}\")\n",
    "    else:\n",
    "        print(f\"{name} | in-dist acc {in_acc:.4f} f1 {in_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zwei-stufiges Fine-Tuning\n",
    "\n",
    "Ziel: Erst auf Lookup-Tabellen trainieren (breite Pattern-Abdeckung),\n",
    "danach auf einem CV-Train-Split feinjustieren (Domain Adaptation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWO_STAGE_CONFIG = {\n",
    "    'stage1': {\n",
    "        'epochs': 2,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'warmup_ratio': 0.1,\n",
    "        'use_class_weights': False\n",
    "    },\n",
    "    'stage2': {\n",
    "        'epochs': 2,\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': 1e-5,\n",
    "        'warmup_ratio': 0.1,\n",
    "        'use_class_weights': False\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def two_stage_finetune(task_name, lookup_df, cv_df, label_col, config, cv_text_col='title'):\n",
    "    label2id, id2label = build_label_maps(lookup_df['label'])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        lookup_df['text'].tolist(),\n",
    "        lookup_df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=lookup_df['label']\n",
    "    )\n",
    "\n",
    "    y_train_ids = [label2id[l] for l in y_train]\n",
    "    y_val_ids = [label2id[l] for l in y_val]\n",
    "\n",
    "    clf = TransformerClassifier(\n",
    "        model_name=BASE_MODEL,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    clf.train(\n",
    "        texts=X_train,\n",
    "        labels=y_train_ids,\n",
    "        val_texts=X_val,\n",
    "        val_labels=y_val_ids,\n",
    "        epochs=config['stage1']['epochs'],\n",
    "        batch_size=config['stage1']['batch_size'],\n",
    "        learning_rate=config['stage1']['learning_rate'],\n",
    "        warmup_ratio=config['stage1']['warmup_ratio'],\n",
    "        use_class_weights=config['stage1']['use_class_weights']\n",
    "    )\n",
    "\n",
    "    stage1_pred = clf.predict(X_val)\n",
    "    stage1_metrics = compute_metrics(y_val_ids, stage1_pred, id2label)\n",
    "\n",
    "    cv_task = cv_df[cv_df[label_col].notna()].copy()\n",
    "    cv_task = cv_task[cv_task[label_col].isin(label2id.keys())]\n",
    "\n",
    "    cv_train, cv_val, cv_test = split_by_cv_id(cv_task)\n",
    "\n",
    "    cv_train_texts = cv_train[cv_text_col].fillna('').tolist()\n",
    "    cv_train_labels = [label2id[l] for l in cv_train[label_col].tolist()]\n",
    "\n",
    "    cv_val_texts = cv_val[cv_text_col].fillna('').tolist()\n",
    "    cv_val_labels = [label2id[l] for l in cv_val[label_col].tolist()]\n",
    "\n",
    "    clf.train(\n",
    "        texts=cv_train_texts,\n",
    "        labels=cv_train_labels,\n",
    "        val_texts=cv_val_texts,\n",
    "        val_labels=cv_val_labels,\n",
    "        epochs=config['stage2']['epochs'],\n",
    "        batch_size=config['stage2']['batch_size'],\n",
    "        learning_rate=config['stage2']['learning_rate'],\n",
    "        warmup_ratio=config['stage2']['warmup_ratio'],\n",
    "        use_class_weights=config['stage2']['use_class_weights']\n",
    "    )\n",
    "\n",
    "    stage2_pred = clf.predict(cv_val_texts)\n",
    "    stage2_metrics = compute_metrics(cv_val_labels, stage2_pred, id2label)\n",
    "\n",
    "    cv_test_texts = cv_test[cv_text_col].fillna('').tolist()\n",
    "    cv_test_labels = [label2id[l] for l in cv_test[label_col].tolist()]\n",
    "\n",
    "    cv_test_pred = clf.predict(cv_test_texts)\n",
    "    cv_test_metrics = compute_metrics(cv_test_labels, cv_test_pred, id2label)\n",
    "\n",
    "    results = {\n",
    "        'in_distribution': stage1_metrics,\n",
    "        'cv_val': stage2_metrics,\n",
    "        'cv_test': cv_test_metrics,\n",
    "        'cv_split_sizes': {\n",
    "            'train': len(cv_train),\n",
    "            'val': len(cv_val),\n",
    "            'test': len(cv_test)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if RUN_TWO_STAGE:\n",
    "    two_stage_results = {}\n",
    "\n",
    "    two_stage_results['department'] = two_stage_finetune(\n",
    "        task_name='department',\n",
    "        lookup_df=dept_df,\n",
    "        cv_df=cv_df_full,\n",
    "        label_col='department',\n",
    "        config=TWO_STAGE_CONFIG,\n",
    "        cv_text_col='title'\n",
    "    )\n",
    "    print_summary('Two-stage Department', two_stage_results['department'])\n",
    "\n",
    "    two_stage_results['seniority'] = two_stage_finetune(\n",
    "        task_name='seniority',\n",
    "        lookup_df=sen_df,\n",
    "        cv_df=cv_df_full,\n",
    "        label_col='seniority',\n",
    "        config=TWO_STAGE_CONFIG,\n",
    "        cv_text_col='title'\n",
    "    )\n",
    "    print_summary('Two-stage Seniority', two_stage_results['seniority'])\n",
    "else:\n",
    "    two_stage_results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backbone Vergleich\n",
    "\n",
    "Ziel: pruefen, ob ein anderes Modell (z.B. XLM-R) bessere Generalisierung liefert.\n",
    "Wir trainieren nur auf Lookup-Tabellen und evaluieren auf einem CV-Testsplit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONES = [\n",
    "    BASE_MODEL\n",
    "]\n",
    "\n",
    "BACKBONE_CONFIG = {\n",
    "    'epochs': 2,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'use_class_weights': False\n",
    "}\n",
    "\n",
    "MAX_SAMPLES = 2500  # optional: reduce for faster runs\n",
    "\n",
    "\n",
    "def sample_df(df, max_samples=None):\n",
    "    if max_samples is None or len(df) <= max_samples:\n",
    "        return df\n",
    "    return df.sample(max_samples, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def train_lookup_only(task_name, lookup_df, cv_df, label_col, model_name, config, max_samples=None):\n",
    "    label2id, id2label = build_label_maps(lookup_df['label'])\n",
    "    lookup_df = sample_df(lookup_df, max_samples=max_samples)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        lookup_df['text'].tolist(),\n",
    "        lookup_df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=lookup_df['label']\n",
    "    )\n",
    "\n",
    "    y_train_ids = [label2id[l] for l in y_train]\n",
    "    y_val_ids = [label2id[l] for l in y_val]\n",
    "\n",
    "    clf = TransformerClassifier(\n",
    "        model_name=model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    clf.train(\n",
    "        texts=X_train,\n",
    "        labels=y_train_ids,\n",
    "        val_texts=X_val,\n",
    "        val_labels=y_val_ids,\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        warmup_ratio=config['warmup_ratio'],\n",
    "        use_class_weights=config['use_class_weights']\n",
    "    )\n",
    "\n",
    "    in_dist_pred = clf.predict(X_val)\n",
    "    in_dist_metrics = compute_metrics(y_val_ids, in_dist_pred, id2label)\n",
    "\n",
    "    cv_task = cv_df[cv_df[label_col].notna()].copy()\n",
    "    cv_task = cv_task[cv_task[label_col].isin(label2id.keys())]\n",
    "    _, _, cv_test = split_by_cv_id(cv_task)\n",
    "\n",
    "    cv_test_texts = cv_test['title'].fillna('').tolist()\n",
    "    cv_test_labels = [label2id[l] for l in cv_test[label_col].tolist()]\n",
    "\n",
    "    cv_test_pred = clf.predict(cv_test_texts)\n",
    "    cv_test_metrics = compute_metrics(cv_test_labels, cv_test_pred, id2label)\n",
    "\n",
    "    return {\n",
    "        'in_distribution': in_dist_metrics,\n",
    "        'cv_test': cv_test_metrics\n",
    "    }\n",
    "\n",
    "\n",
    "if RUN_BACKBONE_TESTS:\n",
    "    backbone_results = {\n",
    "        'department': {},\n",
    "        'seniority': {}\n",
    "    }\n",
    "\n",
    "    for model_name in BACKBONES:\n",
    "        print(f\"\n",
    "Backbone: {model_name}\")\n",
    "\n",
    "        backbone_results['department'][model_name] = train_lookup_only(\n",
    "            task_name='department',\n",
    "            lookup_df=dept_df,\n",
    "            cv_df=cv_df_full,\n",
    "            label_col='department',\n",
    "            model_name=model_name,\n",
    "            config=BACKBONE_CONFIG,\n",
    "            max_samples=MAX_SAMPLES\n",
    "        )\n",
    "        print_summary(f\"Dept {model_name}\", backbone_results['department'][model_name])\n",
    "\n",
    "        backbone_results['seniority'][model_name] = train_lookup_only(\n",
    "            task_name='seniority',\n",
    "            lookup_df=sen_df,\n",
    "            cv_df=cv_df_full,\n",
    "            label_col='seniority',\n",
    "            model_name=model_name,\n",
    "            config=BACKBONE_CONFIG,\n",
    "            max_samples=MAX_SAMPLES\n",
    "        )\n",
    "        print_summary(f\"Sen {model_name}\", backbone_results['seniority'][model_name])\n",
    "else:\n",
    "    backbone_results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Label Smoothing und max_length\n",
    "\n",
    "Ziel: pruefen, ob Label Smoothing die Generalisierung verbessert, und ob eine\n",
    "andere Token-Laenge (64/128/256) sinnvoll ist. Dieses Experiment nutzt einen\n",
    "Custom Trainer, weil der vorhandene TransformerClassifier max_length fix auf 128 setzt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def train_with_label_smoothing(\n",
    "    lookup_df,\n",
    "    cv_df,\n",
    "    label_col,\n",
    "    model_name,\n",
    "    max_length=128,\n",
    "    label_smoothing=0.1,\n",
    "    epochs=2,\n",
    "    batch_size=16\n",
    "):\n",
    "    label2id, id2label = build_label_maps(lookup_df['label'])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        lookup_df['text'].tolist(),\n",
    "        lookup_df['label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=lookup_df['label']\n",
    "    )\n",
    "\n",
    "    y_train_ids = [label2id[l] for l in y_train]\n",
    "    y_val_ids = [label2id[l] for l in y_val]\n",
    "\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    train_enc = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
    "    val_enc = tokenizer(X_val, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "    train_ds = SimpleDataset(train_enc, y_train_ids)\n",
    "    val_ds = SimpleDataset(val_enc, y_val_ids)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        args = TrainingArguments(\n",
    "            output_dir=tmp_dir,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            evaluation_strategy='epoch',\n",
    "            save_strategy='no',\n",
    "            logging_strategy='no',\n",
    "            report_to='none',\n",
    "            disable_tqdm=True,\n",
    "            label_smoothing_factor=label_smoothing\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        preds = trainer.predict(val_ds).predictions\n",
    "\n",
    "    val_pred_ids = np.argmax(preds, axis=1).tolist()\n",
    "    in_dist = compute_metrics(y_val_ids, val_pred_ids, id2label)\n",
    "\n",
    "    cv_task = cv_df[cv_df[label_col].notna()].copy()\n",
    "    cv_task = cv_task[cv_task[label_col].isin(label2id.keys())]\n",
    "    _, _, cv_test = split_by_cv_id(cv_task)\n",
    "\n",
    "    cv_texts = cv_test['title'].fillna('').tolist()\n",
    "    cv_labels = [label2id[l] for l in cv_test[label_col].tolist()]\n",
    "    cv_enc = tokenizer(cv_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**cv_enc)\n",
    "        cv_pred_ids = torch.argmax(outputs.logits, dim=-1).cpu().tolist()\n",
    "\n",
    "    cv_test_metrics = compute_metrics(cv_labels, cv_pred_ids, id2label)\n",
    "\n",
    "    return {\n",
    "        'in_distribution': in_dist,\n",
    "        'cv_test': cv_test_metrics,\n",
    "        'max_length': max_length,\n",
    "        'label_smoothing': label_smoothing\n",
    "    }\n",
    "\n",
    "\n",
    "if RUN_LABEL_SMOOTHING:\n",
    "    ls_results = {'department': [], 'seniority': []}\n",
    "\n",
    "    MAX_LENGTHS = [64, 128, 256]\n",
    "    SMOOTHING = [0.0, 0.1]\n",
    "\n",
    "    for max_len in MAX_LENGTHS:\n",
    "        for ls in SMOOTHING:\n",
    "            print(f\"Label smoothing run: max_len={max_len}, ls={ls}\")\n",
    "            res = train_with_label_smoothing(\n",
    "                lookup_df=dept_df,\n",
    "                cv_df=cv_df_full,\n",
    "                label_col='department',\n",
    "                model_name=BASE_MODEL,\n",
    "                max_length=max_len,\n",
    "                label_smoothing=ls,\n",
    "                epochs=2,\n",
    "                batch_size=16\n",
    "            )\n",
    "            ls_results['department'].append(res)\n",
    "\n",
    "    for max_len in MAX_LENGTHS:\n",
    "        for ls in SMOOTHING:\n",
    "            print(f\"Label smoothing run: max_len={max_len}, ls={ls}\")\n",
    "            res = train_with_label_smoothing(\n",
    "                lookup_df=sen_df,\n",
    "                cv_df=cv_df_full,\n",
    "                label_col='seniority',\n",
    "                model_name=BASE_MODEL,\n",
    "                max_length=max_len,\n",
    "                label_smoothing=ls,\n",
    "                epochs=2,\n",
    "                batch_size=16\n",
    "            )\n",
    "            ls_results['seniority'].append(res)\n",
    "else:\n",
    "    ls_results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Input Varianten (title vs text vs title+history)\n",
    "\n",
    "Ziel: testen, ob mehr Kontext (company oder history) die Performance verbessert.\n",
    "Wir nutzen das zwei-stufige Setup und variieren die Eingabe fuer die CV-Phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INPUT_VARIANTS:\n",
    "    input_variants = {\n",
    "        'title': 'title',\n",
    "        'text': 'text',\n",
    "        'title_history': 'title_history'\n",
    "    }\n",
    "\n",
    "    input_variant_results = {\n",
    "        'department': {},\n",
    "        'seniority': {}\n",
    "    }\n",
    "\n",
    "    for name, col in input_variants.items():\n",
    "        print(f\"\n",
    "Input variant: {name}\")\n",
    "\n",
    "        input_variant_results['department'][name] = two_stage_finetune(\n",
    "            task_name='department',\n",
    "            lookup_df=dept_df,\n",
    "            cv_df=cv_df_full,\n",
    "            label_col='department',\n",
    "            config=TWO_STAGE_CONFIG,\n",
    "            cv_text_col=col\n",
    "        )\n",
    "        print_summary(f\"Department {name}\", input_variant_results['department'][name])\n",
    "\n",
    "        input_variant_results['seniority'][name] = two_stage_finetune(\n",
    "            task_name='seniority',\n",
    "            lookup_df=sen_df,\n",
    "            cv_df=cv_df_full,\n",
    "            label_col='seniority',\n",
    "            config=TWO_STAGE_CONFIG,\n",
    "            cv_text_col=col\n",
    "        )\n",
    "        print_summary(f\"Seniority {name}\", input_variant_results['seniority'][name])\n",
    "else:\n",
    "    input_variant_results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Silver Data Augmentation (optional)\n",
    "\n",
    "Ziel: mehr Trainingsdaten durch Pseudo-Labels. Diese Sektion erzeugt optional\n",
    "Silver Labels mit der Embedding-Baseline und trainiert danach DistilBERT.\n",
    "\n",
    "Standardmaessig ist RUN_SILVER = False, da es laenger dauert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_results = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ergebnisse speichern\n",
    "\n",
    "Wir speichern alle Resultate in einer JSON-Datei fuer spaetere Vergleiche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    'approach': 'DistilBERT Improvements',\n",
    "    'two_stage': two_stage_results,\n",
    "    'backbone': backbone_results,\n",
    "    'label_smoothing': ls_results,\n",
    "    'input_variants': input_variant_results,\n",
    "    'silver': silver_results,\n",
    "    'metadata': {\n",
    "        'base_model': BASE_MODEL,\n",
    "        'random_state': RANDOM_STATE\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "print('All results available in all_results.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
