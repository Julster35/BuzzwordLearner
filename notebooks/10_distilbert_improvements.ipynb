{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10 - DistilBERT Improvements\n",
        "\n",
        "Dieses Notebook setzt die vorgeschlagenen Verbesserungen um:\n",
        "1) Zwei-stufiges Fine-Tuning (Lookup -> CV-Train)\n",
        "2) Backbone Vergleich (andere Modelle)\n",
        "3) Label Smoothing und max_length Tests\n",
        "4) Input Varianten (title vs text vs title+history)\n",
        "5) Silver Data Augmentation (optional)\n",
        "\n",
        "Hinweis: Einige Experimente trainieren auf annotierten CVs.\n",
        "Das ist nicht mehr strikt zero-shot, sondern supervised domain adaptation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Wir importieren alle benoetigten Bibliotheken, definieren Seeds und Basis-Konfigurationen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "from src.data.loader import (\n",
        "    load_label_lists,\n",
        "    load_evaluation_dataset,\n",
        "    load_linkedin_data,\n",
        "    prepare_dataset,\n",
        "    load_inference_dataset,\n",
        "    balance_dataset\n",
        ")\n",
        "from src.models.transformer_classifier import TransformerClassifier\n",
        "from src.models.embedding_classifier import create_domain_classifier, create_seniority_classifier\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_DIR = Path('../data')\n",
        "RESULTS_DIR = Path('./results')\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "BASE_MODEL = 'distilbert-base-multilingual-cased'\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    'model_name': BASE_MODEL,\n",
        "    'epochs': 3,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 2e-5,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'use_class_weights': False\n",
        "}\n",
        "\n",
        "RUN_TWO_STAGE = True\n",
        "RUN_BACKBONE_TESTS = True\n",
        "RUN_LABEL_SMOOTHING = True\n",
        "RUN_INPUT_VARIANTS = True\n",
        "RUN_SILVER = False\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(RANDOM_STATE)\n",
        "\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Daten laden\n",
        "\n",
        "Wir laden Lookup-Tabellen und die annotierten CVs. Fuer Input-Varianten nutzen wir\n",
        "auch die CV-History.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lookup-Tabellen (Training)\n",
        "dept_df, sen_df = load_label_lists(\n",
        "    DATA_DIR,\n",
        "    fix_encoding=True,\n",
        "    deduplicate=True,\n",
        "    max_per_class=None\n",
        ")\n",
        "\n",
        "# Annotierte CVs (Evaluation)\n",
        "eval_df = load_evaluation_dataset(DATA_DIR)\n",
        "\n",
        "# Volle CVs mit History fuer Input-Experimente\n",
        "cvs_annotated = load_linkedin_data(DATA_DIR / 'linkedin-cvs-annotated.json')\n",
        "cv_df_full = prepare_dataset(cvs_annotated, include_history=True)\n",
        "\n",
        "# Titel + History kombinieren\n",
        "history = cv_df_full['history'].fillna('')\n",
        "cv_df_full['title_history'] = np.where(\n",
        "    history != '',\n",
        "    cv_df_full['title'].fillna('') + ' | ' + history,\n",
        "    cv_df_full['title'].fillna('')\n",
        ")\n",
        "\n",
        "print(f\"Department lookup: {len(dept_df):,} examples\")\n",
        "print(f\"Seniority lookup:  {len(sen_df):,} examples\")\n",
        "print(f\"Annotated CVs:     {len(eval_df):,} positions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Hilfsfunktionen\n",
        "\n",
        "Wir kapseln wiederholte Logik fuer Splits, Mappings, Training und Evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_label_maps(label_series):\n",
        "    labels = sorted(label_series.unique())\n",
        "    label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "    id2label = {idx: label for label, idx in label2id.items()}\n",
        "    return label2id, id2label\n",
        "\n",
        "\n",
        "def split_by_cv_id(df, train_size=0.7, val_size=0.15):\n",
        "    cv_ids = df['cv_id'].unique()\n",
        "    train_ids, temp_ids = train_test_split(\n",
        "        cv_ids, test_size=1 - train_size, random_state=RANDOM_STATE\n",
        "    )\n",
        "    val_ids, test_ids = train_test_split(\n",
        "        temp_ids, test_size=0.5, random_state=RANDOM_STATE\n",
        "    )\n",
        "    train_df = df[df['cv_id'].isin(train_ids)].copy()\n",
        "    val_df = df[df['cv_id'].isin(val_ids)].copy()\n",
        "    test_df = df[df['cv_id'].isin(test_ids)].copy()\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def compute_metrics(true_ids, pred_ids, id2label):\n",
        "    acc = accuracy_score(true_ids, pred_ids)\n",
        "    precision, recall, f1_macro, _ = precision_recall_fscore_support(\n",
        "        true_ids, pred_ids, average='macro', zero_division=0\n",
        "    )\n",
        "    f1_weighted = precision_recall_fscore_support(\n",
        "        true_ids, pred_ids, average='weighted', zero_division=0\n",
        "    )[2]\n",
        "\n",
        "    labels = sorted(set(true_ids) | set(pred_ids))\n",
        "    _, _, f1_per_class, _ = precision_recall_fscore_support(\n",
        "        true_ids, pred_ids, labels=labels, average=None, zero_division=0\n",
        "    )\n",
        "    per_class_f1 = {id2label[i]: float(f1_per_class[idx]) for idx, i in enumerate(labels)}\n",
        "\n",
        "    return {\n",
        "        'accuracy': float(acc),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_macro': float(f1_macro),\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'per_class_f1': per_class_f1\n",
        "    }\n",
        "\n",
        "\n",
        "def print_summary(name, results):\n",
        "    in_acc = results['in_distribution']['accuracy']\n",
        "    in_f1 = results['in_distribution']['f1_macro']\n",
        "    if results.get('cv_test'):\n",
        "        rw_acc = results['cv_test']['accuracy']\n",
        "        rw_f1 = results['cv_test']['f1_macro']\n",
        "        print(f\"{name} | in-dist acc {in_acc:.4f} f1 {in_f1:.4f} | cv-test acc {rw_acc:.4f} f1 {rw_f1:.4f}\")\n",
        "    else:\n",
        "        print(f\"{name} | in-dist acc {in_acc:.4f} f1 {in_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Zwei-stufiges Fine-Tuning\n",
        "\n",
        "Ziel: Erst auf Lookup-Tabellen trainieren (breite Pattern-Abdeckung),\n",
        "danach auf einem CV-Train-Split feinjustieren (Domain Adaptation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TWO_STAGE_CONFIG = {\n",
        "    'stage1': {\n",
        "        'epochs': 2,\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 2e-5,\n",
        "        'warmup_ratio': 0.1,\n",
        "        'use_class_weights': False\n",
        "    },\n",
        "    'stage2': {\n",
        "        'epochs': 2,\n",
        "        'batch_size': 8,\n",
        "        'learning_rate': 1e-5,\n",
        "        'warmup_ratio': 0.1,\n",
        "        'use_class_weights': False\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def two_stage_finetune(task_name, lookup_df, cv_df, label_col, config, output_dir, cv_text_col='title'):\n",
        "    label2id, id2label = build_label_maps(lookup_df['label'])\n",
        "\n",
        "    # Stage 1: Lookup split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        lookup_df['text'].tolist(),\n",
        "        lookup_df['label'].tolist(),\n",
        "        test_size=0.2,\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=lookup_df['label']\n",
        "    )\n",
        "\n",
        "    y_train_ids = [label2id[l] for l in y_train]\n",
        "    y_val_ids = [label2id[l] for l in y_val]\n",
        "\n",
        "    clf = TransformerClassifier(\n",
        "        model_name=BASE_MODEL,\n",
        "        num_labels=len(label2id),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    clf.train(\n",
        "        texts=X_train,\n",
        "        labels=y_train_ids,\n",
        "        val_texts=X_val,\n",
        "        val_labels=y_val_ids,\n",
        "        output_dir=f'{output_dir}/stage1',\n",
        "        epochs=config['stage1']['epochs'],\n",
        "        batch_size=config['stage1']['batch_size'],\n",
        "        learning_rate=config['stage1']['learning_rate'],\n",
        "        warmup_ratio=config['stage1']['warmup_ratio'],\n",
        "        use_class_weights=config['stage1']['use_class_weights']\n",
        "    )\n",
        "\n",
        "    stage1_pred = clf.predict(X_val)\n",
        "    stage1_metrics = compute_metrics(y_val_ids, stage1_pred, id2label)\n",
        "\n",
        "    # Stage 2: CV split by cv_id\n",
        "    cv_task = cv_df[cv_df[label_col].notna()].copy()\n",
        "    cv_task = cv_task[cv_task[label_col].isin(label2id.keys())]\n",
        "\n",
        "    cv_train, cv_val, cv_test = split_by_cv_id(cv_task)\n",
        "\n",
        "    cv_train_texts = cv_train[cv_text_col].fillna('').tolist()\n",
        "    cv_train_labels = [label2id[l] for l in cv_train[label_col].tolist()]\n",
        "\n",
        "    cv_val_texts = cv_val[cv_text_col].fillna('').tolist()\n",
        "    cv_val_labels = [label2id[l] for l in cv_val[label_col].tolist()]\n",
        "\n",
        "    clf.train(\n",
        "        texts=cv_train_texts,\n",
        "        labels=cv_train_labels,\n",
        "        val_texts=cv_val_texts,\n",
        "        val_labels=cv_val_labels,\n",
        "        output_dir=f'{output_dir}/stage2',\n",
        "        epochs=config['stage2']['epochs'],\n",
        "        batch_size=config['stage2']['batch_size'],\n",
        "        learning_rate=config['stage2']['learning_rate'],\n",
        "        warmup_ratio=config['stage2']['warmup_ratio'],\n",
        "        use_class_weights=config['stage2']['use_class_weights']\n",
        "    )\n",
        "\n",
        "    stage2_pred = clf.predict(cv_val_texts)\n",
        "    stage2_metrics = compute_metrics(cv_val_labels, stage2_pred, id2label)\n",
        "\n",
        "    # Final CV test evaluation\n",
        "    cv_test_texts = cv_test[cv_text_col].fillna('').tolist()\n",
        "    cv_test_labels = [label2id[l] for l in cv_test[label_col].tolist()]\n",
        "\n",
        "    cv_test_pred = clf.predict(cv_test_texts)\n",
        "    cv_test_metrics = compute_metrics(cv_test_labels, cv_test_pred, id2label)\n",
        "\n",
        "    results = {\n",
        "        'in_distribution': stage1_metrics,\n",
        "        'cv_val': stage2_metrics,\n",
        "        'cv_test': cv_test_metrics,\n",
        "        'cv_split_sizes': {\n",
        "            'train': len(cv_train),\n",
        "            'val': len(cv_val),\n",
        "            'test': len(cv_test)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if RUN_TWO_STAGE:\n",
        "    two_stage_results = {}\n",
        "\n",
        "    two_stage_results['department'] = two_stage_finetune(\n",
        "        task_name='department',\n",
        "        lookup_df=dept_df,\n",
        "        cv_df=cv_df_full,\n",
        "        label_col='department',\n",
        "        config=TWO_STAGE_CONFIG,\n",
        "        output_dir='./results/10_distilbert/two_stage/department',\n",
        "        cv_text_col='title'\n",
        "    )\n",
        "    print_summary('Two-stage Department', two_stage_results['department'])\n",
        "\n",
        "    two_stage_results['seniority'] = two_stage_finetune(\n",
        "        task_name='seniority',\n",
        "        lookup_df=sen_df,\n",
        "        cv_df=cv_df_full,\n",
        "        label_col='seniority',\n",
        "        config=TWO_STAGE_CONFIG,\n",
        "        output_dir='./results/10_distilbert/two_stage/seniority',\n",
        "        cv_text_col='title'\n",
        "    )\n",
        "    print_summary('Two-stage Seniority', two_stage_results['seniority'])\n",
        "else:\n",
        "    two_stage_results = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Backbone Vergleich\n",
        "\n",
        "Ziel: pruefen, ob ein anderes Modell (z.B. XLM-R) bessere Generalisierung liefert.\n",
        "Wir trainieren nur auf Lookup-Tabellen und evaluieren auf einem CV-Testsplit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BACKBONES = [\n",
        "    'distilbert-base-multilingual-cased',\n",
        "    'bert-base-multilingual-cased',\n",
        "    'xlm-roberta-base'\n",
        "]\n",
        "\n",
        "BACKBONE_CONFIG = {\n",
        "    'epochs': 2,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 2e-5,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'use_class_weights': False\n",
        "}\n",
        "\n",
        "MAX_SAMPLES = 2500  # optional: reduzieren fuer schnellere Runs\n",
        "\n",
        "\n",
        "def sample_df(df, max_samples=None):\n",
        "    if max_samples is None or len(df) <= max_samples:\n",
        "        return df\n",
        "    return df.sample(max_samples, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def train_lookup_only(task_name, lookup_df, cv_df, label_col, model_name, config, output_dir, max_samples=None):\n",
        "    label2id, id2label = build_label_maps(lookup_df['label'])\n",
        "    lookup_df = sample_df(lookup_df, max_samples=max_samples)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        lookup_df['text'].tolist(),\n",
        "        lookup_df['label'].tolist(),\n",
        "        test_size=0.2,\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=lookup_df['label']\n",
        "    )\n",
        "\n",
        "    y_train_ids = [label2id[l] for l in y_train]\n",
        "    y_val_ids = [label2id[l] for l in y_val]\n",
        "\n",
        "    clf = TransformerClassifier(\n",
        "        model_name=model_name,\n",
        "        num_labels=len(label2id),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    clf.train(\n",
        "        texts=X_train,\n",
        "        labels=y_train_ids,\n",
        "        val_texts=X_val,\n",
        "        val_labels=y_val_ids,\n",
        "        output_dir=output_dir,\n",
        "        epochs=config['epochs'],\n",
        "        batch_size=config['batch_size'],\n",
        "        learning_rate=config['learning_rate'],\n",
        "        warmup_ratio=config['warmup_ratio'],\n",
        "        use_class_weights=config['use_class_weights']\n",
        "    )\n",
        "\n",
        "    in_dist_pred = clf.predict(X_val)\n",
        "    in_dist_metrics = compute_metrics(y_val_ids, in_dist_pred, id2label)\n",
        "\n",
        "    # CV test split\n",
        "    cv_task = cv_df[cv_df[label_col].notna()].copy()\n",
        "    cv_task = cv_task[cv_task[label_col].isin(label2id.keys())]\n",
        "    _, _, cv_test = split_by_cv_id(cv_task)\n",
        "\n",
        "    cv_test_texts = cv_test['title'].fillna('').tolist()\n",
        "    cv_test_labels = [label2id[l] for l in cv_test[label_col].tolist()]\n",
        "\n",
        "    cv_test_pred = clf.predict(cv_test_texts)\n",
        "    cv_test_metrics = compute_metrics(cv_test_labels, cv_test_pred, id2label)\n",
        "\n",
        "    return {\n",
        "        'in_distribution': in_dist_metrics,\n",
        "        'cv_test': cv_test_metrics\n",
        "    }\n",
        "\n",
        "\n",
        "if RUN_BACKBONE_TESTS:\n",
        "    backbone_results = {\n",
        "        'department': {},\n",
        "        'seniority': {}\n",
        "    }\n",
        "\n",
        "    for model_name in BACKBONES:\n",
        "        print(f\"\n",
        "Backbone: {model_name}\")\n",
        "\n",
        "        backbone_results['department'][model_name] = train_lookup_only(\n",
        "            task_name='department',\n",
        "            lookup_df=dept_df,\n",
        "            cv_df=cv_df_full,\n",
        "            label_col='department',\n",
        "            model_name=model_name,\n",
        "            config=BACKBONE_CONFIG,\n",
        "            output_dir=f'./results/10_distilbert/backbone/{model_name}/department',\n",
        "            max_samples=MAX_SAMPLES\n",
        "        )\n",
        "        print_summary(f\"Dept {model_name}\", backbone_results['department'][model_name])\n",
        "\n",
        "        backbone_results['seniority'][model_name] = train_lookup_only(\n",
        "            task_name='seniority',\n",
        "            lookup_df=sen_df,\n",
        "            cv_df=cv_df_full,\n",
        "            label_col='seniority',\n",
        "            model_name=model_name,\n",
        "            config=BACKBONE_CONFIG,\n",
        "            output_dir=f'./results/10_distilbert/backbone/{model_name}/seniority',\n",
        "            max_samples=MAX_SAMPLES\n",
        "        )\n",
        "        print_summary(f\"Sen {model_name}\", backbone_results['seniority'][model_name])\n",
        "else:\n",
        "    backbone_results = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Label Smoothing und max_length\n",
        "\n",
        "Ziel: pruefen, ob Label Smoothing die Generalisierung verbessert, und ob eine\n",
        "andere Token-Laenge (64/128/256) sinnvoll ist. Dieses Experiment nutzt einen\n",
        "Custom Trainer, weil der vorhandene TransformerClassifier max_length fix auf 128 setzt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "def train_with_label_smoothing(\n",
        "    lookup_df,\n",
        "    cv_df,\n",
        "    label_col,\n",
        "    model_name,\n",
        "    output_dir,\n",
        "    max_length=128,\n",
        "    label_smoothing=0.1,\n",
        "    epochs=2,\n",
        "    batch_size=16\n",
        "):\n",
        "    label2id, id2label = build_label_maps(lookup_df['label'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        lookup_df['text'].tolist(),\n",
        "        lookup_df['label'].tolist(),\n",
        "        test_size=0.2,\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=lookup_df['label']\n",
        "    )\n",
        "\n",
        "    y_train_ids = [label2id[l] for l in y_train]\n",
        "    y_val_ids = [label2id[l] for l in y_val]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(label2id),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    train_enc = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
        "    val_enc = tokenizer(X_val, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    train_ds = SimpleDataset(train_enc, y_train_ids)\n",
        "    val_ds = SimpleDataset(val_enc, y_val_ids)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        logging_steps=50,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        report_to='none',\n",
        "        save_total_limit=2,\n",
        "        label_smoothing_factor=label_smoothing\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    # In-dist evaluation\n",
        "    preds = trainer.predict(val_ds).predictions\n",
        "    val_pred_ids = np.argmax(preds, axis=1).tolist()\n",
        "    in_dist = compute_metrics(y_val_ids, val_pred_ids, id2label)\n",
        "\n",
        "    # CV test evaluation\n",
        "    cv_task = cv_df[cv_df[label_col].notna()].copy()\n",
        "    cv_task = cv_task[cv_task[label_col].isin(label2id.keys())]\n",
        "    _, _, cv_test = split_by_cv_id(cv_task)\n",
        "\n",
        "    cv_texts = cv_test['title'].fillna('').tolist()\n",
        "    cv_labels = [label2id[l] for l in cv_test[label_col].tolist()]\n",
        "    cv_enc = tokenizer(cv_texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**cv_enc)\n",
        "        cv_pred_ids = torch.argmax(outputs.logits, dim=-1).cpu().tolist()\n",
        "\n",
        "    cv_test_metrics = compute_metrics(cv_labels, cv_pred_ids, id2label)\n",
        "\n",
        "    return {\n",
        "        'in_distribution': in_dist,\n",
        "        'cv_test': cv_test_metrics,\n",
        "        'max_length': max_length,\n",
        "        'label_smoothing': label_smoothing\n",
        "    }\n",
        "\n",
        "\n",
        "if RUN_LABEL_SMOOTHING:\n",
        "    ls_results = {'department': [], 'seniority': []}\n",
        "\n",
        "    MAX_LENGTHS = [64, 128, 256]\n",
        "    SMOOTHING = [0.0, 0.1]\n",
        "\n",
        "    for max_len in MAX_LENGTHS:\n",
        "        for ls in SMOOTHING:\n",
        "            print(f\"Label smoothing run: max_len={max_len}, ls={ls}\")\n",
        "            res = train_with_label_smoothing(\n",
        "                lookup_df=dept_df,\n",
        "                cv_df=cv_df_full,\n",
        "                label_col='department',\n",
        "                model_name=BASE_MODEL,\n",
        "                output_dir=f'./results/10_distilbert/label_smoothing/department/len_{max_len}_ls_{ls}',\n",
        "                max_length=max_len,\n",
        "                label_smoothing=ls,\n",
        "                epochs=2,\n",
        "                batch_size=16\n",
        "            )\n",
        "            ls_results['department'].append(res)\n",
        "\n",
        "    for max_len in MAX_LENGTHS:\n",
        "        for ls in SMOOTHING:\n",
        "            print(f\"Label smoothing run: max_len={max_len}, ls={ls}\")\n",
        "            res = train_with_label_smoothing(\n",
        "                lookup_df=sen_df,\n",
        "                cv_df=cv_df_full,\n",
        "                label_col='seniority',\n",
        "                model_name=BASE_MODEL,\n",
        "                output_dir=f'./results/10_distilbert/label_smoothing/seniority/len_{max_len}_ls_{ls}',\n",
        "                max_length=max_len,\n",
        "                label_smoothing=ls,\n",
        "                epochs=2,\n",
        "                batch_size=16\n",
        "            )\n",
        "            ls_results['seniority'].append(res)\n",
        "else:\n",
        "    ls_results = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Input Varianten (title vs text vs title+history)\n",
        "\n",
        "Ziel: testen, ob mehr Kontext (company oder history) die Performance verbessert.\n",
        "Wir nutzen das zwei-stufige Setup und variieren die Eingabe fuer die CV-Phase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_INPUT_VARIANTS:\n",
        "    input_variants = {\n",
        "        'title': 'title',\n",
        "        'text': 'text',\n",
        "        'title_history': 'title_history'\n",
        "    }\n",
        "\n",
        "    input_variant_results = {\n",
        "        'department': {},\n",
        "        'seniority': {}\n",
        "    }\n",
        "\n",
        "    for name, col in input_variants.items():\n",
        "        print(f\"\n",
        "Input variant: {name}\")\n",
        "\n",
        "        input_variant_results['department'][name] = two_stage_finetune(\n",
        "            task_name='department',\n",
        "            lookup_df=dept_df,\n",
        "            cv_df=cv_df_full,\n",
        "            label_col='department',\n",
        "            config=TWO_STAGE_CONFIG,\n",
        "            output_dir=f'./results/10_distilbert/input_variants/{name}/department',\n",
        "            cv_text_col=col\n",
        "        )\n",
        "        print_summary(f\"Department {name}\", input_variant_results['department'][name])\n",
        "\n",
        "        input_variant_results['seniority'][name] = two_stage_finetune(\n",
        "            task_name='seniority',\n",
        "            lookup_df=sen_df,\n",
        "            cv_df=cv_df_full,\n",
        "            label_col='seniority',\n",
        "            config=TWO_STAGE_CONFIG,\n",
        "            output_dir=f'./results/10_distilbert/input_variants/{name}/seniority',\n",
        "            cv_text_col=col\n",
        "        )\n",
        "        print_summary(f\"Seniority {name}\", input_variant_results['seniority'][name])\n",
        "else:\n",
        "    input_variant_results = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Silver Data Augmentation (optional)\n",
        "\n",
        "Ziel: mehr Trainingsdaten durch Pseudo-Labels. Diese Sektion erzeugt optional\n",
        "Silver Labels mit der Embedding-Baseline und trainiert danach DistilBERT.\n",
        "\n",
        "Standardmaessig ist RUN_SILVER = False, da es laenger dauert.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_SILVER:\n",
        "    silver_threshold = 0.85\n",
        "\n",
        "    # Unannotierte CVs laden\n",
        "    inference_df = load_inference_dataset(DATA_DIR)\n",
        "\n",
        "    # Embedding Classifier\n",
        "    emb_dept = create_domain_classifier(dept_df, use_examples=True)\n",
        "    emb_sen = create_seniority_classifier(sen_df, use_examples=True)\n",
        "\n",
        "    dept_preds = emb_dept.predict_with_confidence(inference_df['text'].tolist())\n",
        "    sen_preds = emb_sen.predict_with_confidence(inference_df['text'].tolist())\n",
        "\n",
        "    inference_df['dept_pseudo'] = [p[0] for p in dept_preds]\n",
        "    inference_df['dept_conf'] = [p[1] for p in dept_preds]\n",
        "    inference_df['sen_pseudo'] = [p[0] for p in sen_preds]\n",
        "    inference_df['sen_conf'] = [p[1] for p in sen_preds]\n",
        "\n",
        "    dept_silver = inference_df[inference_df['dept_conf'] >= silver_threshold][['text', 'dept_pseudo']].copy()\n",
        "    dept_silver = dept_silver.rename(columns={'dept_pseudo': 'label'})\n",
        "\n",
        "    sen_silver = inference_df[inference_df['sen_conf'] >= silver_threshold][['text', 'sen_pseudo']].copy()\n",
        "    sen_silver = sen_silver.rename(columns={'sen_pseudo': 'label'})\n",
        "\n",
        "    dept_aug = pd.concat([dept_df[['text', 'label']], dept_silver], ignore_index=True)\n",
        "    sen_aug = pd.concat([sen_df[['text', 'label']], sen_silver], ignore_index=True)\n",
        "\n",
        "    silver_results = {}\n",
        "\n",
        "    silver_results['department'] = train_lookup_only(\n",
        "        task_name='department',\n",
        "        lookup_df=dept_aug,\n",
        "        cv_df=cv_df_full,\n",
        "        label_col='department',\n",
        "        model_name=BASE_MODEL,\n",
        "        config=BACKBONE_CONFIG,\n",
        "        output_dir='./results/10_distilbert/silver/department',\n",
        "        max_samples=None\n",
        "    )\n",
        "    print_summary('Silver Department', silver_results['department'])\n",
        "\n",
        "    silver_results['seniority'] = train_lookup_only(\n",
        "        task_name='seniority',\n",
        "        lookup_df=sen_aug,\n",
        "        cv_df=cv_df_full,\n",
        "        label_col='seniority',\n",
        "        model_name=BASE_MODEL,\n",
        "        config=BACKBONE_CONFIG,\n",
        "        output_dir='./results/10_distilbert/silver/seniority',\n",
        "        max_samples=None\n",
        "    )\n",
        "    print_summary('Silver Seniority', silver_results['seniority'])\n",
        "else:\n",
        "    silver_results = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Ergebnisse speichern\n",
        "\n",
        "Wir speichern alle Resultate in einer JSON-Datei fuer spaetere Vergleiche.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_results = {\n",
        "    'approach': 'DistilBERT Improvements',\n",
        "    'two_stage': two_stage_results,\n",
        "    'backbone': backbone_results,\n",
        "    'label_smoothing': ls_results,\n",
        "    'input_variants': input_variant_results,\n",
        "    'silver': silver_results,\n",
        "    'metadata': {\n",
        "        'base_model': BASE_MODEL,\n",
        "        'random_state': RANDOM_STATE\n",
        "    },\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "output_path = RESULTS_DIR / 'distilbert_improvements.json'\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(f'Results saved to: {output_path}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}