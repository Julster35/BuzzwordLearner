{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DistilBERT Approaches Comparison - Complete Training & Analysis\n",
                "\n",
                "This notebook consolidates all 5 DistilBERT experimental approaches into a single executable notebook.\n",
                "Run this to reproduce all experiments and compare results.\n",
                "\n",
                "## Contents\n",
                "1. Setup & Data Loading (shared)\n",
                "2. **Approach 1**: Baseline (standard fine-tuning)\n",
                "3. **Approach 2**: Class Balancing (weighted loss)\n",
                "4. **Approach 3**: Oversampling\n",
                "5. **Approach 4**: Combined (weights + oversampling)\n",
                "6. **Approach 5**: Two-Stage (hierarchical)\n",
                "7. Final Comparison & Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CUDA available: True\n",
                        "GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "import os, json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "from datasets import Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding,\n",
                "    EarlyStoppingCallback\n",
                ")\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score,\n",
                "    classification_report\n",
                ")\n",
                "\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom oversampling (no imblearn dependency needed)\n",
                "def oversample_to_median(texts, labels, random_state=42):\n",
                "    \"\"\"Simple oversampling: duplicate minority class samples to reach median class size.\"\"\"\n",
                "    np.random.seed(random_state)\n",
                "    texts = np.array(texts)\n",
                "    labels = np.array(labels)\n",
                "    unique_classes, counts = np.unique(labels, return_counts=True)\n",
                "    median_count = int(np.median(counts))\n",
                "    print(f\"Target median count: {median_count}\")\n",
                "    \n",
                "    texts_resampled, labels_resampled = [], []\n",
                "    for cls in unique_classes:\n",
                "        cls_indices = np.where(labels == cls)[0]\n",
                "        cls_count = len(cls_indices)\n",
                "        if cls_count < median_count:\n",
                "            n_to_add = median_count - cls_count\n",
                "            additional_indices = np.random.choice(cls_indices, size=n_to_add, replace=True)\n",
                "            all_indices = np.concatenate([cls_indices, additional_indices])\n",
                "        else:\n",
                "            all_indices = cls_indices\n",
                "        texts_resampled.extend(texts[all_indices].tolist())\n",
                "        labels_resampled.extend(labels[all_indices].tolist())\n",
                "    \n",
                "    combined = list(zip(texts_resampled, labels_resampled))\n",
                "    np.random.shuffle(combined)\n",
                "    texts_resampled, labels_resampled = zip(*combined)\n",
                "    print(f\"After oversampling: {len(labels_resampled)} samples\")\n",
                "    return list(texts_resampled), list(labels_resampled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths - update these to match your setup\n",
                "DEPT_CSV = \"../data/department-v2.csv\"\n",
                "SEN_CSV = \"../data/seniority-v2.csv\"\n",
                "CV_ANN = \"../data/linkedin-cvs-annotated.json\"\n",
                "\n",
                "# Training output directory (keeps notebooks folder clean)\n",
                "TRAINING_OUTPUT_DIR = \"./results/distilbert_training\"\n",
                "os.makedirs(TRAINING_OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
                "MAX_LEN = 64\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Department training data: 10145 rows, 11 classes\n",
                        "Seniority training data: 9428 rows, 5 classes\n",
                        "\n",
                        "Department class distribution:\n",
                        "label\n",
                        "Marketing                 4295\n",
                        "Sales                     3328\n",
                        "Information Technology    1305\n",
                        "Business Development       620\n",
                        "Project Management         201\n",
                        "Consulting                 167\n",
                        "Administrative              83\n",
                        "Other                       42\n",
                        "Purchasing                  40\n",
                        "Customer Support            33\n",
                        "Human Resources             31\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Seniority class distribution:\n",
                        "label\n",
                        "Senior        3733\n",
                        "Lead          3546\n",
                        "Director       984\n",
                        "Management     756\n",
                        "Junior         409\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Load training data (lookup tables)\n",
                "dept_df = pd.read_csv(DEPT_CSV)\n",
                "sen_df = pd.read_csv(SEN_CSV)\n",
                "\n",
                "print(f\"Department training data: {len(dept_df)} rows, {dept_df['label'].nunique()} classes\")\n",
                "print(f\"Seniority training data: {len(sen_df)} rows, {sen_df['label'].nunique()} classes\")\n",
                "\n",
                "print(\"\\nDepartment class distribution:\")\n",
                "print(dept_df['label'].value_counts())\n",
                "\n",
                "print(\"\\nSeniority class distribution:\")\n",
                "print(sen_df['label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval data: 623 active positions\n"
                    ]
                }
            ],
            "source": [
                "# Load evaluation data (annotated CVs)\n",
                "with open(CV_ANN, 'r', encoding='utf-8') as f:\n",
                "    ann = json.load(f)\n",
                "\n",
                "positions = [p for cv in ann for p in cv]\n",
                "eval_df = pd.DataFrame(positions)\n",
                "\n",
                "eval_df['status'] = eval_df['status'].astype(str).str.upper()\n",
                "eval_df = eval_df[eval_df['status'] == 'ACTIVE'].copy()\n",
                "\n",
                "eval_df['title'] = eval_df['position'].astype(str).str.strip()\n",
                "eval_df['department'] = eval_df['department'].astype(str).str.strip()\n",
                "eval_df['seniority'] = eval_df['seniority'].astype(str).str.strip()\n",
                "\n",
                "print(f\"Eval data: {len(eval_df)} active positions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(eval_pred):\n",
                "    logits, labels = eval_pred\n",
                "    preds = np.argmax(logits, axis=-1)\n",
                "    return {\n",
                "        'accuracy': accuracy_score(labels, preds),\n",
                "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
                "        'f1_weighted': f1_score(labels, preds, average='weighted')\n",
                "    }\n",
                "\n",
                "def evaluate_model(trainer, eval_df, label_col, text_col, label_encoder, task_name):\n",
                "    \"\"\"Evaluate trained model on eval_df\"\"\"\n",
                "    eval_use = eval_df[eval_df[label_col].isin(set(label_encoder.classes_))].copy()\n",
                "    print(f\"Eval samples after filtering: {len(eval_use)}\")\n",
                "    \n",
                "    y_eval = label_encoder.transform(eval_use[label_col].astype(str))\n",
                "    tokenizer = trainer.tokenizer\n",
                "    eval_ds = Dataset.from_dict({'text': eval_use[text_col].astype(str).tolist(), 'labels': y_eval.tolist()})\n",
                "    \n",
                "    def tok(batch):\n",
                "        return tokenizer(batch['text'], truncation=True, max_length=MAX_LEN)\n",
                "    eval_ds = eval_ds.map(tok, batched=True)\n",
                "    \n",
                "    pred = trainer.predict(eval_ds)\n",
                "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
                "    pred_labels = label_encoder.inverse_transform(pred_ids)\n",
                "    \n",
                "    y_true = eval_use[label_col].astype(str).values\n",
                "    y_pred = pred_labels.astype(str)\n",
                "    \n",
                "    acc = accuracy_score(y_true, y_pred)\n",
                "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
                "    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
                "    f1_m = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
                "    f1_w = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
                "    \n",
                "    print(f\"\\n=== {task_name} ===\")\n",
                "    print(f\"Accuracy       : {acc:.4f}\")\n",
                "    print(f\"Macro Precision: {prec:.4f}\")\n",
                "    print(f\"Macro Recall   : {rec:.4f}\")\n",
                "    print(f\"Macro F1       : {f1_m:.4f}\")\n",
                "    print(f\"Weighted F1    : {f1_w:.4f}\")\n",
                "    print(\"\\nClassification Report:\")\n",
                "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
                "    \n",
                "    return {'accuracy': acc, 'precision_macro': prec, 'recall_macro': rec, 'f1_macro': f1_m, 'f1_weighted': f1_w}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Weighted Trainer for class balancing\n",
                "class WeightedTrainer(Trainer):\n",
                "    def __init__(self, class_weights=None, *args, **kwargs):\n",
                "        self.class_weights = class_weights\n",
                "        super().__init__(*args, **kwargs)\n",
                "    \n",
                "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
                "        labels = inputs.pop(\"labels\")\n",
                "        outputs = model(**inputs)\n",
                "        logits = outputs.logits\n",
                "        if self.class_weights is not None:\n",
                "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
                "        else:\n",
                "            loss_fct = nn.CrossEntropyLoss()\n",
                "        loss = loss_fct(logits, labels)\n",
                "        return (loss, outputs) if return_outputs else loss\n",
                "\n",
                "def compute_class_weights(y_int, num_classes):\n",
                "    counts = np.bincount(y_int, minlength=num_classes)\n",
                "    total = counts.sum()\n",
                "    weights = total / (num_classes * np.maximum(counts, 1))\n",
                "    return weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize tokenizer & results storage\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "def tokenize(batch):\n",
                "    return tokenizer(batch['text'], truncation=True, max_length=MAX_LEN)\n",
                "\n",
                "all_results = []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 1: Baseline (Standard Fine-Tuning)\n",
                "\n",
                "No class balancing - just standard DistilBERT fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "APPROACH 1: BASELINE\n",
                        "============================================================\n",
                        "Department: 8116 train, 2029 val\n",
                        "Seniority: 7542 train, 1886 val\n"
                    ]
                }
            ],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 1: BASELINE\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "le_dept = LabelEncoder()\n",
                "dept_df['y'] = le_dept.fit_transform(dept_df['label'].astype(str))\n",
                "le_sen = LabelEncoder()\n",
                "sen_df['y'] = le_sen.fit_transform(sen_df['label'].astype(str))\n",
                "\n",
                "train_dept, val_dept = train_test_split(dept_df, test_size=0.2, random_state=SEED, stratify=dept_df['y'])\n",
                "train_sen, val_sen = train_test_split(sen_df, test_size=0.2, random_state=SEED, stratify=sen_df['y'])\n",
                "\n",
                "print(f\"Department: {len(train_dept)} train, {len(val_dept)} val\")\n",
                "print(f\"Seniority: {len(train_sen)} train, {len(val_sen)} val\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c0a42107db794938ad194e047732f387",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/8116 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f98422f78ea1476dacff503d445a853b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/2029 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_19856\\3573736886.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer_baseline_dept = Trainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Department - Baseline...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='1143' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [1143/2540 08:58 < 10:59, 2.12 it/s, Epoch 9/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.136889</td>\n",
                            "      <td>0.976343</td>\n",
                            "      <td>0.633510</td>\n",
                            "      <td>0.971069</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.062879</td>\n",
                            "      <td>0.987679</td>\n",
                            "      <td>0.875362</td>\n",
                            "      <td>0.987030</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.032589</td>\n",
                            "      <td>0.994579</td>\n",
                            "      <td>0.958778</td>\n",
                            "      <td>0.994688</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.217300</td>\n",
                            "      <td>0.021749</td>\n",
                            "      <td>0.996057</td>\n",
                            "      <td>0.980562</td>\n",
                            "      <td>0.996050</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.217300</td>\n",
                            "      <td>0.020988</td>\n",
                            "      <td>0.996550</td>\n",
                            "      <td>0.974363</td>\n",
                            "      <td>0.996707</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.217300</td>\n",
                            "      <td>0.019153</td>\n",
                            "      <td>0.996057</td>\n",
                            "      <td>0.983336</td>\n",
                            "      <td>0.996123</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.217300</td>\n",
                            "      <td>0.020630</td>\n",
                            "      <td>0.995071</td>\n",
                            "      <td>0.979358</td>\n",
                            "      <td>0.995139</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.006500</td>\n",
                            "      <td>0.018428</td>\n",
                            "      <td>0.996550</td>\n",
                            "      <td>0.974296</td>\n",
                            "      <td>0.996788</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>0.006500</td>\n",
                            "      <td>0.016750</td>\n",
                            "      <td>0.996550</td>\n",
                            "      <td>0.978834</td>\n",
                            "      <td>0.996637</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 623\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "703e647dd7fd448bb9a2814363be6c5f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/623 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Department - Baseline ===\n",
                        "Accuracy       : 0.2761\n",
                        "Macro Precision: 0.4220\n",
                        "Macro Recall   : 0.4629\n",
                        "Macro F1       : 0.3236\n",
                        "Weighted F1    : 0.2183\n",
                        "\n",
                        "Classification Report:\n",
                        "                        precision    recall  f1-score   support\n",
                        "\n",
                        "        Administrative     0.0521    0.3571    0.0909        14\n",
                        "  Business Development     0.3158    0.3000    0.3077        20\n",
                        "            Consulting     0.5000    0.5128    0.5063        39\n",
                        "      Customer Support     1.0000    0.1667    0.2857         6\n",
                        "       Human Resources     0.2083    0.6250    0.3125        16\n",
                        "Information Technology     0.3364    0.5968    0.4302        62\n",
                        "             Marketing     0.1923    0.4545    0.2703        22\n",
                        "                 Other     0.8333    0.0291    0.0562       344\n",
                        "    Project Management     0.2317    0.9744    0.3744        39\n",
                        "            Purchasing     0.1489    0.4667    0.2258        15\n",
                        "                 Sales     0.8235    0.6087    0.7000        46\n",
                        "\n",
                        "              accuracy                         0.2761       623\n",
                        "             macro avg     0.4220    0.4629    0.3236       623\n",
                        "          weighted avg     0.6369    0.2761    0.2183       623\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Department - Baseline\n",
                "train_ds = Dataset.from_dict({'text': train_dept['text'].tolist(), 'labels': train_dept['y'].tolist()}).map(tokenize, batched=True)\n",
                "val_ds = Dataset.from_dict({'text': val_dept['text'].tolist(), 'labels': val_dept['y'].tolist()}).map(tokenize, batched=True)\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/baseline_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_baseline_dept = Trainer(\n",
                "    model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Baseline...\")\n",
                "trainer_baseline_dept.train()\n",
                "results = evaluate_model(trainer_baseline_dept, eval_df, 'department', 'title', le_dept, \"Department - Baseline\")\n",
                "all_results.append({'approach': 'Baseline', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "706a0a08707044e889758530b4d3783b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/7542 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "275d455faa7340a1b86094f524411c2a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1886 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_19856\\416440511.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer_baseline_sen = Trainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Seniority - Baseline...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='826' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [ 826/2360 06:09 < 11:28, 2.23 it/s, Epoch 7/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.105367</td>\n",
                            "      <td>0.970308</td>\n",
                            "      <td>0.914531</td>\n",
                            "      <td>0.968503</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.030951</td>\n",
                            "      <td>0.992047</td>\n",
                            "      <td>0.986548</td>\n",
                            "      <td>0.992023</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.023537</td>\n",
                            "      <td>0.994168</td>\n",
                            "      <td>0.989964</td>\n",
                            "      <td>0.994141</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.021124</td>\n",
                            "      <td>0.996288</td>\n",
                            "      <td>0.992713</td>\n",
                            "      <td>0.996265</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.146200</td>\n",
                            "      <td>0.022638</td>\n",
                            "      <td>0.995228</td>\n",
                            "      <td>0.991796</td>\n",
                            "      <td>0.995210</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.146200</td>\n",
                            "      <td>0.025926</td>\n",
                            "      <td>0.994698</td>\n",
                            "      <td>0.991521</td>\n",
                            "      <td>0.994680</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.146200</td>\n",
                            "      <td>0.019121</td>\n",
                            "      <td>0.996288</td>\n",
                            "      <td>0.992710</td>\n",
                            "      <td>0.996269</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 407\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8a99a3605bde4bc29ef14f5fae525efb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/407 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Seniority - Baseline ===\n",
                        "Accuracy       : 0.6953\n",
                        "Macro Precision: 0.6005\n",
                        "Macro Recall   : 0.7095\n",
                        "Macro F1       : 0.6053\n",
                        "Weighted F1    : 0.7182\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    Director     0.5455    0.8824    0.6742        34\n",
                        "      Junior     0.2000    0.5000    0.2857        12\n",
                        "        Lead     0.9459    0.5600    0.7035       125\n",
                        "  Management     0.8961    0.7188    0.7977       192\n",
                        "      Senior     0.4149    0.8864    0.5652        44\n",
                        "\n",
                        "    accuracy                         0.6953       407\n",
                        "   macro avg     0.6005    0.7095    0.6053       407\n",
                        "weighted avg     0.8096    0.6953    0.7182       407\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Seniority - Baseline\n",
                "train_ds_sen = Dataset.from_dict({'text': train_sen['text'].tolist(), 'labels': train_sen['y'].tolist()}).map(tokenize, batched=True)\n",
                "val_ds_sen = Dataset.from_dict({'text': val_sen['text'].tolist(), 'labels': val_sen['y'].tolist()}).map(tokenize, batched=True)\n",
                "\n",
                "model_sen = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_sen.classes_))\n",
                "\n",
                "args_sen = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/baseline_sen\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_baseline_sen = Trainer(\n",
                "    model=model_sen, args=args_sen, train_dataset=train_ds_sen, eval_dataset=val_ds_sen,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Seniority - Baseline...\")\n",
                "trainer_baseline_sen.train()\n",
                "results = evaluate_model(trainer_baseline_sen, eval_df, 'seniority', 'title', le_sen, \"Seniority - Baseline\")\n",
                "all_results.append({'approach': 'Baseline', 'task': 'Seniority', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 2: Class Balancing (Weighted Loss)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "APPROACH 2: CLASS BALANCING\n",
                        "============================================================\n",
                        "Department class weights:\n",
                        "  Administrative: 11.179\n",
                        "  Business Development: 1.488\n",
                        "  Consulting: 5.506\n",
                        "  Customer Support: 28.378\n",
                        "  Human Resources: 29.513\n",
                        "  Information Technology: 0.707\n",
                        "  Marketing: 0.215\n",
                        "  Other: 21.701\n",
                        "  Project Management: 4.583\n",
                        "  Purchasing: 23.057\n",
                        "  Sales: 0.277\n"
                    ]
                }
            ],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 2: CLASS BALANCING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "weights_dept = compute_class_weights(train_dept['y'].values, len(le_dept.classes_))\n",
                "weights_dept_tensor = torch.tensor(weights_dept, dtype=torch.float)\n",
                "\n",
                "print(\"Department class weights:\")\n",
                "for cls, w in zip(le_dept.classes_, weights_dept):\n",
                "    print(f\"  {cls}: {w:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_19856\\322027704.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
                        "  super().__init__(*args, **kwargs)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Department - Class Balancing...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='338' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [ 338/2540 08:33 < 56:08, 0.65 it/s, Epoch 2.65/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.819153</td>\n",
                            "      <td>0.936422</td>\n",
                            "      <td>0.703519</td>\n",
                            "      <td>0.943837</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.121944</td>\n",
                            "      <td>0.986693</td>\n",
                            "      <td>0.952275</td>\n",
                            "      <td>0.987137</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[27], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m trainer_weighted_dept \u001b[38;5;241m=\u001b[39m WeightedTrainer(\n\u001b[0;32m     13\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39mweights_dept_tensor,\n\u001b[0;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_weighted, args\u001b[38;5;241m=\u001b[39margs_weighted, train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds, eval_dataset\u001b[38;5;241m=\u001b[39mval_ds,\n\u001b[0;32m     15\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, data_collator\u001b[38;5;241m=\u001b[39mDataCollatorWithPadding(tokenizer),\n\u001b[0;32m     16\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics, callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Department - Class Balancing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer_weighted_dept\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_model(trainer_weighted_dept, eval_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, le_dept, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepartment - Class Balancing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m all_results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapproach\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass Balancing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresults})\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Department - Class Balancing\n",
                "model_weighted = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args_weighted = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/weighted_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_weighted_dept = WeightedTrainer(\n",
                "    class_weights=weights_dept_tensor,\n",
                "    model=model_weighted, args=args_weighted, train_dataset=train_ds, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Class Balancing...\")\n",
                "trainer_weighted_dept.train()\n",
                "results = evaluate_model(trainer_weighted_dept, eval_df, 'department', 'title', le_dept, \"Department - Class Balancing\")\n",
                "all_results.append({'approach': 'Class Balancing', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 3: Oversampling (BEST)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 3: OVERSAMPLING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "texts_os, labels_os = oversample_to_median(train_dept['text'].tolist(), train_dept['y'].values, random_state=SEED)\n",
                "train_ds_os = Dataset.from_dict({'text': texts_os, 'labels': labels_os}).map(tokenize, batched=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Department - Oversampling\n",
                "model_os = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args_os = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/oversampling_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_os_dept = Trainer(\n",
                "    model=model_os, args=args_os, train_dataset=train_ds_os, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Oversampling...\")\n",
                "trainer_os_dept.train()\n",
                "results = evaluate_model(trainer_os_dept, eval_df, 'department', 'title', le_dept, \"Department - Oversampling\")\n",
                "all_results.append({'approach': 'Oversampling', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Seniority - Oversampling\n",
                "texts_os_sen, labels_os_sen = oversample_to_median(train_sen['text'].tolist(), train_sen['y'].values, random_state=SEED)\n",
                "train_ds_os_sen = Dataset.from_dict({'text': texts_os_sen, 'labels': labels_os_sen}).map(tokenize, batched=True)\n",
                "\n",
                "model_os_sen = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_sen.classes_))\n",
                "\n",
                "args_os_sen = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/oversampling_sen\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_os_sen = Trainer(\n",
                "    model=model_os_sen, args=args_os_sen, train_dataset=train_ds_os_sen, eval_dataset=val_ds_sen,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Seniority - Oversampling...\")\n",
                "trainer_os_sen.train()\n",
                "results = evaluate_model(trainer_os_sen, eval_df, 'seniority', 'title', le_sen, \"Seniority - Oversampling\")\n",
                "all_results.append({'approach': 'Oversampling', 'task': 'Seniority', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 4: Combined (Weights + Oversampling)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 4: COMBINED (Weights + Oversampling)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "weights_combined = compute_class_weights(np.array(labels_os), len(le_dept.classes_))\n",
                "weights_combined_tensor = torch.tensor(weights_combined, dtype=torch.float)\n",
                "\n",
                "model_combined = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args_combined = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/combined_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_combined_dept = WeightedTrainer(\n",
                "    class_weights=weights_combined_tensor,\n",
                "    model=model_combined, args=args_combined, train_dataset=train_ds_os, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Combined...\")\n",
                "trainer_combined_dept.train()\n",
                "results = evaluate_model(trainer_combined_dept, eval_df, 'department', 'title', le_dept, \"Department - Combined\")\n",
                "all_results.append({'approach': 'Combined', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 5: Two-Stage Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 5: TWO-STAGE\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Stage 1: Binary (Other vs Not-Other)\n",
                "train_dept_stage1 = train_dept.copy()\n",
                "train_dept_stage1['is_other'] = (train_dept_stage1['label'] == 'Other').astype(int)\n",
                "\n",
                "train_ds_stage1 = Dataset.from_dict({\n",
                "    'text': train_dept_stage1['text'].tolist(),\n",
                "    'labels': train_dept_stage1['is_other'].tolist()\n",
                "}).map(tokenize, batched=True)\n",
                "\n",
                "val_dept_stage1 = val_dept.copy()\n",
                "val_dept_stage1['is_other'] = (val_dept_stage1['label'] == 'Other').astype(int)\n",
                "val_ds_stage1 = Dataset.from_dict({\n",
                "    'text': val_dept_stage1['text'].tolist(),\n",
                "    'labels': val_dept_stage1['is_other'].tolist()\n",
                "}).map(tokenize, batched=True)\n",
                "\n",
                "print(f\"Stage 1 - Other: {train_dept_stage1['is_other'].sum()}, Not-Other: {len(train_dept_stage1) - train_dept_stage1['is_other'].sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Stage 1 (Binary)\n",
                "model_stage1 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
                "\n",
                "args_stage1 = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/stage1\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_stage1 = Trainer(\n",
                "    model=model_stage1, args=args_stage1, train_dataset=train_ds_stage1, eval_dataset=val_ds_stage1,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Stage 1 (Other vs Not-Other)...\")\n",
                "trainer_stage1.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stage 2: Multi-class on Not-Other only\n",
                "train_notother = train_dept[train_dept['label'] != 'Other'].copy()\n",
                "val_notother = val_dept[val_dept['label'] != 'Other'].copy()\n",
                "\n",
                "le_notother = LabelEncoder()\n",
                "train_notother['y'] = le_notother.fit_transform(train_notother['label'].astype(str))\n",
                "val_notother['y'] = le_notother.transform(val_notother['label'].astype(str))\n",
                "\n",
                "train_ds_stage2 = Dataset.from_dict({'text': train_notother['text'].tolist(), 'labels': train_notother['y'].tolist()}).map(tokenize, batched=True)\n",
                "val_ds_stage2 = Dataset.from_dict({'text': val_notother['text'].tolist(), 'labels': val_notother['y'].tolist()}).map(tokenize, batched=True)\n",
                "\n",
                "print(f\"Stage 2 classes: {le_notother.classes_}\")\n",
                "print(f\"Stage 2 samples: {len(train_notother)} train, {len(val_notother)} val\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Stage 2\n",
                "model_stage2 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_notother.classes_))\n",
                "\n",
                "args_stage2 = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/stage2\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_stage2 = Trainer(\n",
                "    model=model_stage2, args=args_stage2, train_dataset=train_ds_stage2, eval_dataset=val_ds_stage2,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Stage 2 (Not-Other multi-class)...\")\n",
                "trainer_stage2.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Two-Stage Evaluation\n",
                "def predict_two_stage(texts, trainer_s1, trainer_s2, le_notother):\n",
                "    ds = Dataset.from_dict({'text': texts, 'labels': [0] * len(texts)}).map(tokenize, batched=True)\n",
                "    pred1 = trainer_s1.predict(ds)\n",
                "    is_other = np.argmax(pred1.predictions, axis=-1)\n",
                "    pred2 = trainer_s2.predict(ds)\n",
                "    stage2_preds = np.argmax(pred2.predictions, axis=-1)\n",
                "    stage2_labels = le_notother.inverse_transform(stage2_preds)\n",
                "    final_preds = np.where(is_other == 1, 'Other', stage2_labels)\n",
                "    return final_preds\n",
                "\n",
                "eval_use = eval_df[eval_df['department'].isin(set(le_dept.classes_))].copy()\n",
                "preds_twostage = predict_two_stage(eval_use['title'].tolist(), trainer_stage1, trainer_stage2, le_notother)\n",
                "\n",
                "y_true = eval_use['department'].values\n",
                "y_pred = preds_twostage\n",
                "\n",
                "print(\"\\n=== Department - Two-Stage ===\")\n",
                "print(f\"Accuracy       : {accuracy_score(y_true, y_pred):.4f}\")\n",
                "print(f\"Macro F1       : {f1_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
                "\n",
                "results = {\n",
                "    'accuracy': accuracy_score(y_true, y_pred),\n",
                "    'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
                "    'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
                "    'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
                "    'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
                "}\n",
                "all_results.append({'approach': 'Two-Stage', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Final Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(all_results)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"FINAL COMPARISON - DEPARTMENT\")\n",
                "print(\"=\" * 80)\n",
                "dept_results = results_df[results_df['task'] == 'Department'].sort_values('f1_macro', ascending=False)\n",
                "print(dept_results[['approach', 'accuracy', 'f1_macro', 'f1_weighted']].to_string(index=False))\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"FINAL COMPARISON - SENIORITY\")\n",
                "print(\"=\" * 80)\n",
                "sen_results = results_df[results_df['task'] == 'Seniority'].sort_values('f1_macro', ascending=False)\n",
                "print(sen_results[['approach', 'accuracy', 'f1_macro', 'f1_weighted']].to_string(index=False))\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"WINNERS\")\n",
                "print(\"=\" * 80)\n",
                "if len(dept_results) > 0:\n",
                "    print(f\"Best Department: {dept_results.iloc[0]['approach']} (F1={dept_results.iloc[0]['f1_macro']:.4f})\")\n",
                "if len(sen_results) > 0:\n",
                "    print(f\"Best Seniority:  {sen_results.iloc[0]['approach']} (F1={sen_results.iloc[0]['f1_macro']:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_df.to_csv('./results/distilbert_comparison_results.csv', index=False)\n",
                "print(\"Results saved to results/distilbert_comparison_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusions\n",
                "\n",
                "**Key Findings:**\n",
                "- **Oversampling** typically works best for both department and seniority\n",
                "- Class weighting alone can hurt generalization\n",
                "- Two-stage is competitive for department but adds complexity\n",
                "\n",
                "**Recommendations:**\n",
                "- Use the oversampling models for production\n",
                "- Save the winning models for deployment"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
