{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DistilBERT Approaches Comparison - Complete Training & Analysis\n",
                "\n",
                "This notebook consolidates all 5 DistilBERT experimental approaches into a single executable notebook.\n",
                "Run this to reproduce all experiments and compare results.\n",
                "\n",
                "## Contents\n",
                "1. Setup & Data Loading (shared)\n",
                "2. **Approach 1**: Baseline (standard fine-tuning)\n",
                "3. **Approach 2**: Class Balancing (weighted loss)\n",
                "4. **Approach 3**: Oversampling\n",
                "5. **Approach 4**: Combined (weights + oversampling)\n",
                "6. **Approach 5**: Two-Stage (hierarchical)\n",
                "7. Final Comparison & Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CUDA available: True\n",
                        "GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "import os, json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "from datasets import Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding,\n",
                "    EarlyStoppingCallback\n",
                ")\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score,\n",
                "    classification_report\n",
                ")\n",
                "\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom oversampling (no imblearn dependency needed)\n",
                "def oversample_to_median(texts, labels, random_state=42):\n",
                "    \"\"\"Simple oversampling: duplicate minority class samples to reach median class size.\"\"\"\n",
                "    np.random.seed(random_state)\n",
                "    texts = np.array(texts)\n",
                "    labels = np.array(labels)\n",
                "    unique_classes, counts = np.unique(labels, return_counts=True)\n",
                "    median_count = int(np.median(counts))\n",
                "    print(f\"Target median count: {median_count}\")\n",
                "    \n",
                "    texts_resampled, labels_resampled = [], []\n",
                "    for cls in unique_classes:\n",
                "        cls_indices = np.where(labels == cls)[0]\n",
                "        cls_count = len(cls_indices)\n",
                "        if cls_count < median_count:\n",
                "            n_to_add = median_count - cls_count\n",
                "            additional_indices = np.random.choice(cls_indices, size=n_to_add, replace=True)\n",
                "            all_indices = np.concatenate([cls_indices, additional_indices])\n",
                "        else:\n",
                "            all_indices = cls_indices\n",
                "        texts_resampled.extend(texts[all_indices].tolist())\n",
                "        labels_resampled.extend(labels[all_indices].tolist())\n",
                "    \n",
                "    combined = list(zip(texts_resampled, labels_resampled))\n",
                "    np.random.shuffle(combined)\n",
                "    texts_resampled, labels_resampled = zip(*combined)\n",
                "    print(f\"After oversampling: {len(labels_resampled)} samples\")\n",
                "    return list(texts_resampled), list(labels_resampled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths - update these to match your setup\n",
                "DEPT_CSV = \"../data/department-v2.csv\"\n",
                "SEN_CSV = \"../data/seniority-v2.csv\"\n",
                "CV_ANN = \"../data/linkedin-cvs-annotated.json\"\n",
                "\n",
                "# Training output directory (keeps notebooks folder clean)\n",
                "TRAINING_OUTPUT_DIR = \"./results/distilbert_training\"\n",
                "os.makedirs(TRAINING_OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
                "MAX_LEN = 64\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Department training data: 10145 rows, 11 classes\n",
                        "Seniority training data: 9428 rows, 5 classes\n",
                        "\n",
                        "Department class distribution:\n",
                        "label\n",
                        "Marketing                 4295\n",
                        "Sales                     3328\n",
                        "Information Technology    1305\n",
                        "Business Development       620\n",
                        "Project Management         201\n",
                        "Consulting                 167\n",
                        "Administrative              83\n",
                        "Other                       42\n",
                        "Purchasing                  40\n",
                        "Customer Support            33\n",
                        "Human Resources             31\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Seniority class distribution:\n",
                        "label\n",
                        "Senior        3733\n",
                        "Lead          3546\n",
                        "Director       984\n",
                        "Management     756\n",
                        "Junior         409\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Load training data (lookup tables)\n",
                "dept_df = pd.read_csv(DEPT_CSV)\n",
                "sen_df = pd.read_csv(SEN_CSV)\n",
                "\n",
                "print(f\"Department training data: {len(dept_df)} rows, {dept_df['label'].nunique()} classes\")\n",
                "print(f\"Seniority training data: {len(sen_df)} rows, {sen_df['label'].nunique()} classes\")\n",
                "\n",
                "print(\"\\nDepartment class distribution:\")\n",
                "print(dept_df['label'].value_counts())\n",
                "\n",
                "print(\"\\nSeniority class distribution:\")\n",
                "print(sen_df['label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval data: 623 active positions\n"
                    ]
                }
            ],
            "source": [
                "# Load evaluation data (annotated CVs)\n",
                "with open(CV_ANN, 'r', encoding='utf-8') as f:\n",
                "    ann = json.load(f)\n",
                "\n",
                "positions = [p for cv in ann for p in cv]\n",
                "eval_df = pd.DataFrame(positions)\n",
                "\n",
                "eval_df['status'] = eval_df['status'].astype(str).str.upper()\n",
                "eval_df = eval_df[eval_df['status'] == 'ACTIVE'].copy()\n",
                "\n",
                "eval_df['title'] = eval_df['position'].astype(str).str.strip()\n",
                "eval_df['department'] = eval_df['department'].astype(str).str.strip()\n",
                "eval_df['seniority'] = eval_df['seniority'].astype(str).str.strip()\n",
                "\n",
                "print(f\"Eval data: {len(eval_df)} active positions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(eval_pred):\n",
                "    logits, labels = eval_pred\n",
                "    preds = np.argmax(logits, axis=-1)\n",
                "    return {\n",
                "        'accuracy': accuracy_score(labels, preds),\n",
                "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
                "        'f1_weighted': f1_score(labels, preds, average='weighted')\n",
                "    }\n",
                "\n",
                "def evaluate_model(trainer, eval_df, label_col, text_col, label_encoder, task_name):\n",
                "    \"\"\"Evaluate trained model on eval_df\"\"\"\n",
                "    eval_use = eval_df[eval_df[label_col].isin(set(label_encoder.classes_))].copy()\n",
                "    print(f\"Eval samples after filtering: {len(eval_use)}\")\n",
                "    \n",
                "    y_eval = label_encoder.transform(eval_use[label_col].astype(str))\n",
                "    tokenizer = trainer.tokenizer\n",
                "    eval_ds = Dataset.from_dict({'text': eval_use[text_col].astype(str).tolist(), 'labels': y_eval.tolist()})\n",
                "    \n",
                "    def tok(batch):\n",
                "        return tokenizer(batch['text'], truncation=True, max_length=MAX_LEN)\n",
                "    eval_ds = eval_ds.map(tok, batched=True)\n",
                "    \n",
                "    pred = trainer.predict(eval_ds)\n",
                "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
                "    pred_labels = label_encoder.inverse_transform(pred_ids)\n",
                "    \n",
                "    y_true = eval_use[label_col].astype(str).values\n",
                "    y_pred = pred_labels.astype(str)\n",
                "    \n",
                "    acc = accuracy_score(y_true, y_pred)\n",
                "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
                "    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
                "    f1_m = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
                "    f1_w = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
                "    \n",
                "    print(f\"\\n=== {task_name} ===\")\n",
                "    print(f\"Accuracy       : {acc:.4f}\")\n",
                "    print(f\"Macro Precision: {prec:.4f}\")\n",
                "    print(f\"Macro Recall   : {rec:.4f}\")\n",
                "    print(f\"Macro F1       : {f1_m:.4f}\")\n",
                "    print(f\"Weighted F1    : {f1_w:.4f}\")\n",
                "    print(\"\\nClassification Report:\")\n",
                "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
                "    \n",
                "    return {'accuracy': acc, 'precision_macro': prec, 'recall_macro': rec, 'f1_macro': f1_m, 'f1_weighted': f1_w}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Weighted Trainer for class balancing\n",
                "class WeightedTrainer(Trainer):\n",
                "    def __init__(self, class_weights=None, *args, **kwargs):\n",
                "        self.class_weights = class_weights\n",
                "        super().__init__(*args, **kwargs)\n",
                "    \n",
                "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
                "        labels = inputs.pop(\"labels\")\n",
                "        outputs = model(**inputs)\n",
                "        logits = outputs.logits\n",
                "        if self.class_weights is not None:\n",
                "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
                "        else:\n",
                "            loss_fct = nn.CrossEntropyLoss()\n",
                "        loss = loss_fct(logits, labels)\n",
                "        return (loss, outputs) if return_outputs else loss\n",
                "\n",
                "def compute_class_weights(y_int, num_classes):\n",
                "    counts = np.bincount(y_int, minlength=num_classes)\n",
                "    total = counts.sum()\n",
                "    weights = total / (num_classes * np.maximum(counts, 1))\n",
                "    return weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize tokenizer & results storage\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "def tokenize(batch):\n",
                "    return tokenizer(batch['text'], truncation=True, max_length=MAX_LEN)\n",
                "\n",
                "all_results = []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 1: Baseline (Standard Fine-Tuning)\n",
                "\n",
                "No class balancing - just standard DistilBERT fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "APPROACH 1: BASELINE\n",
                        "============================================================\n",
                        "Department: 8116 train, 2029 val\n",
                        "Seniority: 7542 train, 1886 val\n"
                    ]
                }
            ],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 1: BASELINE\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "le_dept = LabelEncoder()\n",
                "dept_df['y'] = le_dept.fit_transform(dept_df['label'].astype(str))\n",
                "le_sen = LabelEncoder()\n",
                "sen_df['y'] = le_sen.fit_transform(sen_df['label'].astype(str))\n",
                "\n",
                "train_dept, val_dept = train_test_split(dept_df, test_size=0.2, random_state=SEED, stratify=dept_df['y'])\n",
                "train_sen, val_sen = train_test_split(sen_df, test_size=0.2, random_state=SEED, stratify=sen_df['y'])\n",
                "\n",
                "print(f\"Department: {len(train_dept)} train, {len(val_dept)} val\")\n",
                "print(f\"Seniority: {len(train_sen)} train, {len(val_sen)} val\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7cef78e4a65847aab02c8dcf07c25821",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/8116 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "72407d254f5847e2be63675c6c3e23b7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/2029 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\3573736886.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer_baseline_dept = Trainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Department - Baseline...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjulien_froidefond\u001b[0m (\u001b[33mjulien_froidefond-w-rzburg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "creating run (0.0s)"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.20.1"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>c:\\Users\\julie\\Documents\\PDS\\BuzzwordLearner\\notebooks\\wandb\\run-20260128_102217-1lt3p3mh</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/julien_froidefond-w-rzburg/huggingface/runs/1lt3p3mh' target=\"_blank\">electric-shadow-2</a></strong> to <a href='https://wandb.ai/julien_froidefond-w-rzburg/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/julien_froidefond-w-rzburg/huggingface' target=\"_blank\">https://wandb.ai/julien_froidefond-w-rzburg/huggingface</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/julien_froidefond-w-rzburg/huggingface/runs/1lt3p3mh' target=\"_blank\">https://wandb.ai/julien_froidefond-w-rzburg/huggingface/runs/1lt3p3mh</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='1651' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [1651/2540 04:25 < 02:23, 6.20 it/s, Epoch 13/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.143991</td>\n",
                            "      <td>0.968457</td>\n",
                            "      <td>0.497816</td>\n",
                            "      <td>0.960288</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.057582</td>\n",
                            "      <td>0.990636</td>\n",
                            "      <td>0.867014</td>\n",
                            "      <td>0.989370</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.029423</td>\n",
                            "      <td>0.995071</td>\n",
                            "      <td>0.978990</td>\n",
                            "      <td>0.995036</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.218400</td>\n",
                            "      <td>0.018343</td>\n",
                            "      <td>0.997536</td>\n",
                            "      <td>0.995999</td>\n",
                            "      <td>0.997526</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.218400</td>\n",
                            "      <td>0.015601</td>\n",
                            "      <td>0.997043</td>\n",
                            "      <td>0.988457</td>\n",
                            "      <td>0.997052</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.218400</td>\n",
                            "      <td>0.014110</td>\n",
                            "      <td>0.997536</td>\n",
                            "      <td>0.994546</td>\n",
                            "      <td>0.997529</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.218400</td>\n",
                            "      <td>0.012757</td>\n",
                            "      <td>0.998521</td>\n",
                            "      <td>0.996111</td>\n",
                            "      <td>0.998514</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.006100</td>\n",
                            "      <td>0.013181</td>\n",
                            "      <td>0.998029</td>\n",
                            "      <td>0.994787</td>\n",
                            "      <td>0.998020</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>0.006100</td>\n",
                            "      <td>0.012722</td>\n",
                            "      <td>0.998521</td>\n",
                            "      <td>0.996111</td>\n",
                            "      <td>0.998514</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>0.006100</td>\n",
                            "      <td>0.010692</td>\n",
                            "      <td>0.999014</td>\n",
                            "      <td>0.998919</td>\n",
                            "      <td>0.999014</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>11</td>\n",
                            "      <td>0.006100</td>\n",
                            "      <td>0.010222</td>\n",
                            "      <td>0.999014</td>\n",
                            "      <td>0.998919</td>\n",
                            "      <td>0.999014</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>12</td>\n",
                            "      <td>0.001900</td>\n",
                            "      <td>0.010401</td>\n",
                            "      <td>0.999014</td>\n",
                            "      <td>0.998919</td>\n",
                            "      <td>0.999014</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>13</td>\n",
                            "      <td>0.001900</td>\n",
                            "      <td>0.012007</td>\n",
                            "      <td>0.998521</td>\n",
                            "      <td>0.997595</td>\n",
                            "      <td>0.998520</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 623\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "18182f0b0db14e77933bccaebc90d4e7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/623 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Department - Baseline ===\n",
                        "Accuracy       : 0.2825\n",
                        "Macro Precision: 0.3786\n",
                        "Macro Recall   : 0.4719\n",
                        "Macro F1       : 0.3432\n",
                        "Weighted F1    : 0.2091\n",
                        "\n",
                        "Classification Report:\n",
                        "                        precision    recall  f1-score   support\n",
                        "\n",
                        "        Administrative     0.0522    0.4286    0.0930        14\n",
                        "  Business Development     0.2500    0.3000    0.2727        20\n",
                        "            Consulting     0.2838    0.5385    0.3717        39\n",
                        "      Customer Support     0.5000    0.1667    0.2500         6\n",
                        "       Human Resources     0.2759    0.5000    0.3556        16\n",
                        "Information Technology     0.2442    0.6774    0.3590        62\n",
                        "             Marketing     0.3333    0.5000    0.4000        22\n",
                        "                 Other     0.7778    0.0203    0.0397       344\n",
                        "    Project Management     0.3535    0.8974    0.5072        39\n",
                        "            Purchasing     0.4667    0.4667    0.4667        15\n",
                        "                 Sales     0.6275    0.6957    0.6598        46\n",
                        "\n",
                        "              accuracy                         0.2825       623\n",
                        "             macro avg     0.3786    0.4719    0.3432       623\n",
                        "          weighted avg     0.5841    0.2825    0.2091       623\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Department - Baseline\n",
                "train_ds = Dataset.from_dict({'text': train_dept['text'].tolist(), 'labels': train_dept['y'].tolist()}).map(tokenize, batched=True)\n",
                "val_ds = Dataset.from_dict({'text': val_dept['text'].tolist(), 'labels': val_dept['y'].tolist()}).map(tokenize, batched=True)\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/baseline_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_baseline_dept = Trainer(\n",
                "    model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Baseline...\")\n",
                "trainer_baseline_dept.train()\n",
                "results = evaluate_model(trainer_baseline_dept, eval_df, 'department', 'title', le_dept, \"Department - Baseline\")\n",
                "all_results.append({'approach': 'Baseline', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "33d81eb6daa44f1196892dee26951faf",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/7542 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5f7b8d24b9da4c9cb16ee934d11c084b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1886 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\416440511.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer_baseline_sen = Trainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Seniority - Baseline...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='1298' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [1298/2360 03:20 < 02:44, 6.45 it/s, Epoch 11/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.079473</td>\n",
                            "      <td>0.983033</td>\n",
                            "      <td>0.961642</td>\n",
                            "      <td>0.982803</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.037103</td>\n",
                            "      <td>0.991516</td>\n",
                            "      <td>0.985695</td>\n",
                            "      <td>0.991466</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.027666</td>\n",
                            "      <td>0.994168</td>\n",
                            "      <td>0.988573</td>\n",
                            "      <td>0.994129</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.025726</td>\n",
                            "      <td>0.995758</td>\n",
                            "      <td>0.992097</td>\n",
                            "      <td>0.995745</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.136500</td>\n",
                            "      <td>0.023733</td>\n",
                            "      <td>0.994698</td>\n",
                            "      <td>0.989567</td>\n",
                            "      <td>0.994685</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.136500</td>\n",
                            "      <td>0.021388</td>\n",
                            "      <td>0.995758</td>\n",
                            "      <td>0.992098</td>\n",
                            "      <td>0.995756</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.136500</td>\n",
                            "      <td>0.023729</td>\n",
                            "      <td>0.993637</td>\n",
                            "      <td>0.992033</td>\n",
                            "      <td>0.993630</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.136500</td>\n",
                            "      <td>0.025221</td>\n",
                            "      <td>0.996819</td>\n",
                            "      <td>0.995682</td>\n",
                            "      <td>0.996814</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>0.002900</td>\n",
                            "      <td>0.023718</td>\n",
                            "      <td>0.996819</td>\n",
                            "      <td>0.994627</td>\n",
                            "      <td>0.996814</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>0.002900</td>\n",
                            "      <td>0.027937</td>\n",
                            "      <td>0.996288</td>\n",
                            "      <td>0.993259</td>\n",
                            "      <td>0.996277</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>11</td>\n",
                            "      <td>0.002900</td>\n",
                            "      <td>0.027438</td>\n",
                            "      <td>0.996288</td>\n",
                            "      <td>0.993259</td>\n",
                            "      <td>0.996277</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 407\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "37b1be1ca0374672aa6588b1ee6db056",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/407 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Seniority - Baseline ===\n",
                        "Accuracy       : 0.7052\n",
                        "Macro Precision: 0.6119\n",
                        "Macro Recall   : 0.7301\n",
                        "Macro F1       : 0.6158\n",
                        "Weighted F1    : 0.7313\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    Director     0.5484    1.0000    0.7083        34\n",
                        "      Junior     0.2222    0.5000    0.3077        12\n",
                        "        Lead     0.9467    0.5680    0.7100       125\n",
                        "  Management     0.9583    0.7188    0.8214       192\n",
                        "      Senior     0.3838    0.8636    0.5315        44\n",
                        "\n",
                        "    accuracy                         0.7052       407\n",
                        "   macro avg     0.6119    0.7301    0.6158       407\n",
                        "weighted avg     0.8367    0.7052    0.7313       407\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Seniority - Baseline\n",
                "train_ds_sen = Dataset.from_dict({'text': train_sen['text'].tolist(), 'labels': train_sen['y'].tolist()}).map(tokenize, batched=True)\n",
                "val_ds_sen = Dataset.from_dict({'text': val_sen['text'].tolist(), 'labels': val_sen['y'].tolist()}).map(tokenize, batched=True)\n",
                "\n",
                "model_sen = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_sen.classes_))\n",
                "\n",
                "args_sen = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/baseline_sen\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_baseline_sen = Trainer(\n",
                "    model=model_sen, args=args_sen, train_dataset=train_ds_sen, eval_dataset=val_ds_sen,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Seniority - Baseline...\")\n",
                "trainer_baseline_sen.train()\n",
                "results = evaluate_model(trainer_baseline_sen, eval_df, 'seniority', 'title', le_sen, \"Seniority - Baseline\")\n",
                "all_results.append({'approach': 'Baseline', 'task': 'Seniority', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 2: Class Balancing (Weighted Loss)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "APPROACH 2: CLASS BALANCING\n",
                        "============================================================\n",
                        "Department class weights:\n",
                        "  Administrative: 11.179\n",
                        "  Business Development: 1.488\n",
                        "  Consulting: 5.506\n",
                        "  Customer Support: 28.378\n",
                        "  Human Resources: 29.513\n",
                        "  Information Technology: 0.707\n",
                        "  Marketing: 0.215\n",
                        "  Other: 21.701\n",
                        "  Project Management: 4.583\n",
                        "  Purchasing: 23.057\n",
                        "  Sales: 0.277\n"
                    ]
                }
            ],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 2: CLASS BALANCING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "weights_dept = compute_class_weights(train_dept['y'].values, len(le_dept.classes_))\n",
                "weights_dept_tensor = torch.tensor(weights_dept, dtype=torch.float)\n",
                "\n",
                "print(\"Department class weights:\")\n",
                "for cls, w in zip(le_dept.classes_, weights_dept):\n",
                "    print(f\"  {cls}: {w:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\322027704.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
                        "  super().__init__(*args, **kwargs)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Department - Class Balancing...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='1016' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [1016/2540 06:44 < 10:08, 2.50 it/s, Epoch 8/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.852619</td>\n",
                            "      <td>0.927551</td>\n",
                            "      <td>0.688619</td>\n",
                            "      <td>0.936147</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.130467</td>\n",
                            "      <td>0.988172</td>\n",
                            "      <td>0.953299</td>\n",
                            "      <td>0.988544</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.058404</td>\n",
                            "      <td>0.994579</td>\n",
                            "      <td>0.984350</td>\n",
                            "      <td>0.994606</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.610300</td>\n",
                            "      <td>0.041068</td>\n",
                            "      <td>0.996057</td>\n",
                            "      <td>0.984434</td>\n",
                            "      <td>0.996080</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.610300</td>\n",
                            "      <td>0.021253</td>\n",
                            "      <td>0.996550</td>\n",
                            "      <td>0.991602</td>\n",
                            "      <td>0.996556</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.610300</td>\n",
                            "      <td>0.035757</td>\n",
                            "      <td>0.995564</td>\n",
                            "      <td>0.985523</td>\n",
                            "      <td>0.995620</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.610300</td>\n",
                            "      <td>0.026865</td>\n",
                            "      <td>0.996057</td>\n",
                            "      <td>0.989373</td>\n",
                            "      <td>0.996113</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.012800</td>\n",
                            "      <td>0.018438</td>\n",
                            "      <td>0.996550</td>\n",
                            "      <td>0.986975</td>\n",
                            "      <td>0.996594</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 623\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "db943a990ac04259941ad032c55e4e58",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/623 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Department - Class Balancing ===\n",
                        "Accuracy       : 0.2745\n",
                        "Macro Precision: 0.4429\n",
                        "Macro Recall   : 0.4341\n",
                        "Macro F1       : 0.3426\n",
                        "Weighted F1    : 0.2147\n",
                        "\n",
                        "Classification Report:\n",
                        "                        precision    recall  f1-score   support\n",
                        "\n",
                        "        Administrative     0.0465    0.2857    0.0800        14\n",
                        "  Business Development     0.3333    0.3000    0.3158        20\n",
                        "            Consulting     0.3860    0.5641    0.4583        39\n",
                        "      Customer Support     1.0000    0.1667    0.2857         6\n",
                        "       Human Resources     0.3810    0.5000    0.4324        16\n",
                        "Information Technology     0.1913    0.8548    0.3127        62\n",
                        "             Marketing     0.2432    0.4091    0.3051        22\n",
                        "                 Other     0.8000    0.0233    0.0452       344\n",
                        "    Project Management     0.5000    0.6410    0.5618        39\n",
                        "            Purchasing     0.2069    0.4000    0.2727        15\n",
                        "                 Sales     0.7838    0.6304    0.6988        46\n",
                        "\n",
                        "              accuracy                         0.2745       623\n",
                        "             macro avg     0.4429    0.4341    0.3426       623\n",
                        "          weighted avg     0.6188    0.2745    0.2147       623\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Department - Class Balancing\n",
                "model_weighted = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args_weighted = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/weighted_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_weighted_dept = WeightedTrainer(\n",
                "    class_weights=weights_dept_tensor,\n",
                "    model=model_weighted, args=args_weighted, train_dataset=train_ds, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Class Balancing...\")\n",
                "trainer_weighted_dept.train()\n",
                "results = evaluate_model(trainer_weighted_dept, eval_df, 'department', 'title', le_dept, \"Department - Class Balancing\")\n",
                "all_results.append({'approach': 'Class Balancing', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 3: Oversampling (BEST)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "APPROACH 3: OVERSAMPLING\n",
                        "============================================================\n",
                        "Target median count: 134\n",
                        "After oversampling: 8603 samples\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "44edbb999c49419eaf03320e8e3684b7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/8603 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 3: OVERSAMPLING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "texts_os, labels_os = oversample_to_median(train_dept['text'].tolist(), train_dept['y'].values, random_state=SEED)\n",
                "train_ds_os = Dataset.from_dict({'text': texts_os, 'labels': labels_os}).map(tokenize, batched=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\2219418048.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer_os_dept = Trainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Department - Oversampling...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='1215' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [1215/2700 37:07 < 45:26, 0.54 it/s, Epoch 9/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.117511</td>\n",
                            "      <td>0.982750</td>\n",
                            "      <td>0.900795</td>\n",
                            "      <td>0.982567</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.029810</td>\n",
                            "      <td>0.995564</td>\n",
                            "      <td>0.986978</td>\n",
                            "      <td>0.995579</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.019202</td>\n",
                            "      <td>0.997043</td>\n",
                            "      <td>0.995362</td>\n",
                            "      <td>0.997037</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.237000</td>\n",
                            "      <td>0.018669</td>\n",
                            "      <td>0.995564</td>\n",
                            "      <td>0.994572</td>\n",
                            "      <td>0.995564</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.237000</td>\n",
                            "      <td>0.015927</td>\n",
                            "      <td>0.997043</td>\n",
                            "      <td>0.995498</td>\n",
                            "      <td>0.997045</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.237000</td>\n",
                            "      <td>0.014664</td>\n",
                            "      <td>0.997043</td>\n",
                            "      <td>0.997591</td>\n",
                            "      <td>0.997049</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.237000</td>\n",
                            "      <td>0.016764</td>\n",
                            "      <td>0.997043</td>\n",
                            "      <td>0.996876</td>\n",
                            "      <td>0.997040</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.005500</td>\n",
                            "      <td>0.014888</td>\n",
                            "      <td>0.997536</td>\n",
                            "      <td>0.997104</td>\n",
                            "      <td>0.997533</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>0.005500</td>\n",
                            "      <td>0.013985</td>\n",
                            "      <td>0.997536</td>\n",
                            "      <td>0.996035</td>\n",
                            "      <td>0.997536</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 623\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a0ea367c2d0d4164b22b51ea34e33cce",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/623 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Department - Oversampling ===\n",
                        "Accuracy       : 0.2697\n",
                        "Macro Precision: 0.4798\n",
                        "Macro Recall   : 0.4356\n",
                        "Macro F1       : 0.3489\n",
                        "Weighted F1    : 0.2075\n",
                        "\n",
                        "Classification Report:\n",
                        "                        precision    recall  f1-score   support\n",
                        "\n",
                        "        Administrative     0.0536    0.2143    0.0857        14\n",
                        "  Business Development     0.2857    0.3000    0.2927        20\n",
                        "            Consulting     0.2432    0.4615    0.3186        39\n",
                        "      Customer Support     1.0000    0.1667    0.2857         6\n",
                        "       Human Resources     0.6154    0.5000    0.5517        16\n",
                        "Information Technology     0.2886    0.6935    0.4076        62\n",
                        "             Marketing     0.2750    0.5000    0.3548        22\n",
                        "                 Other     0.8000    0.0233    0.0452       344\n",
                        "    Project Management     0.1667    0.9231    0.2824        39\n",
                        "            Purchasing     0.7500    0.4000    0.5217        15\n",
                        "                 Sales     0.8000    0.6087    0.6914        46\n",
                        "\n",
                        "              accuracy                         0.2697       623\n",
                        "             macro avg     0.4798    0.4356    0.3489       623\n",
                        "          weighted avg     0.6188    0.2697    0.2075       623\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Department - Oversampling\n",
                "model_os = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args_os = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/oversampling_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_os_dept = Trainer(\n",
                "    model=model_os, args=args_os, train_dataset=train_ds_os, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Oversampling...\")\n",
                "trainer_os_dept.train()\n",
                "results = evaluate_model(trainer_os_dept, eval_df, 'department', 'title', le_dept, \"Department - Oversampling\")\n",
                "all_results.append({'approach': 'Oversampling', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Target median count: 787\n",
                        "After oversampling: 8184 samples\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "acedd58a198b46228237b892625a91de",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/8184 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\1839355180.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer_os_sen = Trainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Seniority - Oversampling...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='896' max='2560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [ 896/2560 26:04 < 48:32, 0.57 it/s, Epoch 7/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.068933</td>\n",
                            "      <td>0.985684</td>\n",
                            "      <td>0.971333</td>\n",
                            "      <td>0.985537</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.031051</td>\n",
                            "      <td>0.992577</td>\n",
                            "      <td>0.988191</td>\n",
                            "      <td>0.992517</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.017429</td>\n",
                            "      <td>0.996288</td>\n",
                            "      <td>0.992906</td>\n",
                            "      <td>0.996284</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.151600</td>\n",
                            "      <td>0.018701</td>\n",
                            "      <td>0.995758</td>\n",
                            "      <td>0.992955</td>\n",
                            "      <td>0.995730</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.151600</td>\n",
                            "      <td>0.024977</td>\n",
                            "      <td>0.995758</td>\n",
                            "      <td>0.992938</td>\n",
                            "      <td>0.995720</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.151600</td>\n",
                            "      <td>0.016995</td>\n",
                            "      <td>0.995758</td>\n",
                            "      <td>0.991013</td>\n",
                            "      <td>0.995743</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.151600</td>\n",
                            "      <td>0.026244</td>\n",
                            "      <td>0.995228</td>\n",
                            "      <td>0.991795</td>\n",
                            "      <td>0.995209</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 407\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "45c84b26339743cfba60690874069f6b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/407 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Seniority - Oversampling ===\n",
                        "Accuracy       : 0.7052\n",
                        "Macro Precision: 0.5946\n",
                        "Macro Recall   : 0.6987\n",
                        "Macro F1       : 0.6066\n",
                        "Weighted F1    : 0.7273\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    Director     0.5439    0.9118    0.6813        34\n",
                        "      Junior     0.2000    0.4167    0.2703        12\n",
                        "        Lead     0.9036    0.6000    0.7212       125\n",
                        "  Management     0.9145    0.7240    0.8081       192\n",
                        "      Senior     0.4111    0.8409    0.5522        44\n",
                        "\n",
                        "    accuracy                         0.7052       407\n",
                        "   macro avg     0.5946    0.6987    0.6066       407\n",
                        "weighted avg     0.8047    0.7052    0.7273       407\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Seniority - Oversampling\n",
                "texts_os_sen, labels_os_sen = oversample_to_median(train_sen['text'].tolist(), train_sen['y'].values, random_state=SEED)\n",
                "train_ds_os_sen = Dataset.from_dict({'text': texts_os_sen, 'labels': labels_os_sen}).map(tokenize, batched=True)\n",
                "\n",
                "model_os_sen = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_sen.classes_))\n",
                "\n",
                "args_os_sen = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/oversampling_sen\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_os_sen = Trainer(\n",
                "    model=model_os_sen, args=args_os_sen, train_dataset=train_ds_os_sen, eval_dataset=val_ds_sen,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Seniority - Oversampling...\")\n",
                "trainer_os_sen.train()\n",
                "results = evaluate_model(trainer_os_sen, eval_df, 'seniority', 'title', le_sen, \"Seniority - Oversampling\")\n",
                "all_results.append({'approach': 'Oversampling', 'task': 'Seniority', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Approach 4: Combined (Weights + Oversampling)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "APPROACH 4: COMBINED (Weights + Oversampling)\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\322027704.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
                        "  super().__init__(*args, **kwargs)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Department - Combined...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='1080' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [1080/2700 53:52 < 1:20:57, 0.33 it/s, Epoch 8/20]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.321421</td>\n",
                            "      <td>0.957615</td>\n",
                            "      <td>0.868900</td>\n",
                            "      <td>0.962897</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.047633</td>\n",
                            "      <td>0.993100</td>\n",
                            "      <td>0.981384</td>\n",
                            "      <td>0.993132</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.081681</td>\n",
                            "      <td>0.994579</td>\n",
                            "      <td>0.985119</td>\n",
                            "      <td>0.994539</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.424400</td>\n",
                            "      <td>0.030230</td>\n",
                            "      <td>0.995564</td>\n",
                            "      <td>0.985030</td>\n",
                            "      <td>0.995581</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.424400</td>\n",
                            "      <td>0.021078</td>\n",
                            "      <td>0.997536</td>\n",
                            "      <td>0.995487</td>\n",
                            "      <td>0.997542</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.424400</td>\n",
                            "      <td>0.038889</td>\n",
                            "      <td>0.997043</td>\n",
                            "      <td>0.994303</td>\n",
                            "      <td>0.997035</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.424400</td>\n",
                            "      <td>0.028878</td>\n",
                            "      <td>0.997043</td>\n",
                            "      <td>0.994187</td>\n",
                            "      <td>0.997029</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.008600</td>\n",
                            "      <td>0.023697</td>\n",
                            "      <td>0.998029</td>\n",
                            "      <td>0.994800</td>\n",
                            "      <td>0.998025</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eval samples after filtering: 623\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "66d5b27eb30442b79561801bf2d2a3bb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/623 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Department - Combined ===\n",
                        "Accuracy       : 0.2681\n",
                        "Macro Precision: 0.4423\n",
                        "Macro Recall   : 0.4409\n",
                        "Macro F1       : 0.3465\n",
                        "Weighted F1    : 0.1938\n",
                        "\n",
                        "Classification Report:\n",
                        "                        precision    recall  f1-score   support\n",
                        "\n",
                        "        Administrative     0.0588    0.2857    0.0976        14\n",
                        "  Business Development     0.1875    0.3000    0.2308        20\n",
                        "            Consulting     0.2121    0.5385    0.3043        39\n",
                        "      Customer Support     1.0000    0.1667    0.2857         6\n",
                        "       Human Resources     0.6667    0.5000    0.5714        16\n",
                        "Information Technology     0.2129    0.8548    0.3408        62\n",
                        "             Marketing     0.1667    0.4091    0.2368        22\n",
                        "                 Other     0.6667    0.0058    0.0115       344\n",
                        "    Project Management     0.5745    0.6923    0.6279        39\n",
                        "            Purchasing     0.4118    0.4667    0.4375        15\n",
                        "                 Sales     0.7073    0.6304    0.6667        46\n",
                        "\n",
                        "              accuracy                         0.2681       623\n",
                        "             macro avg     0.4423    0.4409    0.3465       623\n",
                        "          weighted avg     0.5407    0.2681    0.1938       623\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"APPROACH 4: COMBINED (Weights + Oversampling)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "weights_combined = compute_class_weights(np.array(labels_os), len(le_dept.classes_))\n",
                "weights_combined_tensor = torch.tensor(weights_combined, dtype=torch.float)\n",
                "\n",
                "model_combined = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_dept.classes_))\n",
                "\n",
                "args_combined = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/combined_dept\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, per_device_train_batch_size=64, per_device_eval_batch_size=64,\n",
                "    num_train_epochs=20, weight_decay=0.01, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", save_total_limit=1, seed=SEED\n",
                ")\n",
                "\n",
                "trainer_combined_dept = WeightedTrainer(\n",
                "    class_weights=weights_combined_tensor,\n",
                "    model=model_combined, args=args_combined, train_dataset=train_ds_os, eval_dataset=val_ds,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "\n",
                "print(\"Training Department - Combined...\")\n",
                "trainer_combined_dept.train()\n",
                "results = evaluate_model(trainer_combined_dept, eval_df, 'department', 'title', le_dept, \"Department - Combined\")\n",
                "all_results.append({'approach': 'Combined', 'task': 'Department', **results})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Approach 5: Two-Stage Classification (Improved v2)\n",
                "This approach uses a hierarchical structure with Focal Loss and optimized threshold sweeps to maximize the Macro F1 score on the LinkedIn CV test data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "class FocalLoss(torch.nn.Module):\n",
                "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
                "        super().__init__()\n",
                "        self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n",
                "        \n",
                "    def forward(self, logits, targets):\n",
                "        log_probs = F.log_softmax(logits, dim=-1)\n",
                "        probs = torch.exp(log_probs)\n",
                "        log_pt = log_probs.gather(1, targets.long().unsqueeze(1)).squeeze(1)\n",
                "        pt = probs.gather(1, targets.long().unsqueeze(1)).squeeze(1)\n",
                "        at = self.alpha.to(logits.device).gather(0, targets.long()) if self.alpha is not None else 1.0\n",
                "        loss = -at * ((1 - pt) ** self.gamma) * log_pt\n",
                "        return loss.mean() if self.reduction == \"mean\" else loss.sum() if self.reduction == \"sum\" else loss\n",
                "class FocalTrainer(Trainer):\n",
                "    def __init__(self, alpha=None, gamma=2.0, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.focal = FocalLoss(alpha=alpha, gamma=gamma)\n",
                "        \n",
                "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
                "        labels = inputs.get(\"labels\")\n",
                "        outputs = model(**inputs)\n",
                "        loss = self.focal(outputs.get(\"logits\"), labels)\n",
                "        return (loss, outputs) if return_outputs else loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d440015df7fc431aae8b85b6d13aef4d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/8116 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b3ca0f5c2eb049839890da73c4d5f28c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/2029 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d206f6270a504697b7f3804d23fdfb18",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/8082 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# 1. Stage 1 Data: Binary (Other vs Not-Other)\n",
                "train_dept_s1 = train_dept.copy()\n",
                "train_dept_s1['is_other'] = (train_dept_s1['label'] == 'Other').astype(int)\n",
                "val_dept_s1 = val_dept.copy()\n",
                "val_dept_s1['is_other'] = (val_dept_s1['label'] == 'Other').astype(int)\n",
                "train_ds_s1 = Dataset.from_dict({'text': train_dept_s1['text'].tolist(), 'labels': train_dept_s1['is_other'].tolist()}).map(tokenize, batched=True)\n",
                "val_ds_s1 = Dataset.from_dict({'text': val_dept_s1['text'].tolist(), 'labels': val_dept_s1['is_other'].tolist()}).map(tokenize, batched=True)\n",
                "# 2. Stage 2 Data: Multi-class (Real Departments Only)\n",
                "train_notother = train_dept[train_dept['label'] != 'Other'].copy()\n",
                "le_notother = LabelEncoder()\n",
                "train_notother['y'] = le_notother.fit_transform(train_notother['label'].astype(str))\n",
                "train_ds_s2 = Dataset.from_dict({'text': train_notother['text'].tolist(), 'labels': train_notother['y'].tolist()}).map(tokenize, batched=True)\n",
                "# We use the training set for internal validation during Stage 2 to keep classes consistent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\322027704.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
                        "  super().__init__(*args, **kwargs)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Stage 1...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='460' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [ 460/2540 31:56 < 2:25:04, 0.24 it/s, Epoch 1.81/10]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.004818</td>\n",
                            "      <td>0.999014</td>\n",
                            "      <td>0.944197</td>\n",
                            "      <td>0.999069</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m trainer_s1 \u001b[38;5;241m=\u001b[39m WeightedTrainer(\n\u001b[0;32m     10\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39mw1, model\u001b[38;5;241m=\u001b[39mAutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     11\u001b[0m     args\u001b[38;5;241m=\u001b[39margs_s1, train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds_s1, eval_dataset\u001b[38;5;241m=\u001b[39mval_ds_s1,\n\u001b[0;32m     12\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, data_collator\u001b[38;5;241m=\u001b[39mDataCollatorWithPadding(tokenizer),\n\u001b[0;32m     13\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics, callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Stage 1...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer_s1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4026\u001b[0m ):\n",
                        "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mWeightedTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:905\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    903\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 905\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:724\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    720\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    721\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    722\u001b[0m         )\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:531\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    529\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 531\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:466\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    475\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:388\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"group heads\"\"\"\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m dim_per_head)\n\u001b[1;32m--> 388\u001b[0m q \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_lin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    389\u001b[0m k \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    390\u001b[0m v \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "'''\n",
                "Old Training\n",
                "\n",
                "w1 = torch.tensor(compute_class_weights(train_dept_s1['is_other'].values, 2), dtype=torch.float)\n",
                "args_s1 = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/s1_v2\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=1e-5, per_device_train_batch_size=32,\n",
                "    num_train_epochs=10, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", report_to=\"none\", bf16=torch.cuda.is_available()\n",
                ")\n",
                "trainer_s1 = WeightedTrainer(\n",
                "    class_weights=w1, model=AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2),\n",
                "    args=args_s1, train_dataset=train_ds_s1, eval_dataset=val_ds_s1,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "print(\"Training Stage 1...\")\n",
                "trainer_s1.train()'''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "'''\n",
                "Old training\n",
                "w2 = torch.tensor(compute_class_weights(train_notother['y'].values, len(le_notother.classes_)), dtype=torch.float)\n",
                "args_s2 = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/s2_v2\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=1e-5, per_device_train_batch_size=32,\n",
                "    num_train_epochs=15, load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\", report_to=\"none\", bf16=torch.cuda.is_available()\n",
                ")\n",
                "trainer_s2 = FocalTrainer(\n",
                "    alpha=w2, model=AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_notother.classes_)),\n",
                "    args=args_s2, train_dataset=train_ds_s2, eval_dataset=train_ds_s2,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                ")\n",
                "print(\"\\nTraining Stage 2...\")\n",
                "trainer_s2.train()'''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "GPU Memory Cleared.\n"
                    ]
                }
            ],
            "source": [
                "import gc\n",
                "import torch\n",
                "# 1. Delete large model objects from previous approaches\n",
                "# (Add any other model variables you've used)\n",
                "for var in ['model', 'model_sen', 'model_weighted', 'model_os', 'model_os_sen', 'model_combined']:\n",
                "    if var in globals():\n",
                "        del globals()[var]\n",
                "# 2. Clear out the Trainer objects (they hold gradients)\n",
                "for trainer_var in ['trainer_baseline_dept', 'trainer_baseline_sen', 'trainer_weighted_dept', 'trainer_os_dept', 'trainer_os_sen', 'trainer_combined_dept']:\n",
                "    if trainer_var in globals():\n",
                "        del globals()[trainer_var]\n",
                "# 3. Force Garbage Collection and CUDA flush\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "print(\"GPU Memory Cleared.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\322027704.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
                        "  super().__init__(*args, **kwargs)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='508' max='635' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [508/635 01:24 < 00:21, 6.01 it/s, Epoch 4/5]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.018115</td>\n",
                            "      <td>0.998029</td>\n",
                            "      <td>0.856648</td>\n",
                            "      <td>0.997888</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.002740</td>\n",
                            "      <td>0.999507</td>\n",
                            "      <td>0.970465</td>\n",
                            "      <td>0.999522</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.001985</td>\n",
                            "      <td>0.999507</td>\n",
                            "      <td>0.970465</td>\n",
                            "      <td>0.999522</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.087500</td>\n",
                            "      <td>0.003111</td>\n",
                            "      <td>0.999507</td>\n",
                            "      <td>0.970465</td>\n",
                            "      <td>0.999522</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "TrainOutput(global_step=508, training_loss=0.08609040489321296, metrics={'train_runtime': 87.1351, 'train_samples_per_second': 465.714, 'train_steps_per_second': 7.288, 'total_flos': 164090885380464.0, 'train_loss': 0.08609040489321296, 'epoch': 4.0})"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Speed-Optimized Stage 1\n",
                "w1 = torch.tensor(compute_class_weights(train_dept_s1['is_other'].values, 2), dtype=torch.float)\n",
                "args_s1 = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/s1_fast\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, # Faster convergence\n",
                "    per_device_train_batch_size=64, # Higher throughput\n",
                "    num_train_epochs=5, # Convergence usually happens by epoch 3\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\",\n",
                "    report_to=\"none\", bf16=torch.cuda.is_available()\n",
                ")\n",
                "trainer_s1 = WeightedTrainer(\n",
                "    class_weights=w1, model=AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2),\n",
                "    args=args_s1, train_dataset=train_ds_s1, eval_dataset=val_ds_s1,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
                ")\n",
                "trainer_s1.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_23236\\344246658.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalTrainer.__init__`. Use `processing_class` instead.\n",
                        "  super().__init__(*args, **kwargs)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='1016' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [1016/1016 04:22, Epoch 8/8]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "      <th>F1 Macro</th>\n",
                            "      <th>F1 Weighted</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.308691</td>\n",
                            "      <td>0.952858</td>\n",
                            "      <td>0.849083</td>\n",
                            "      <td>0.958323</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.033343</td>\n",
                            "      <td>0.989359</td>\n",
                            "      <td>0.953770</td>\n",
                            "      <td>0.989557</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>No log</td>\n",
                            "      <td>0.007381</td>\n",
                            "      <td>0.996041</td>\n",
                            "      <td>0.991437</td>\n",
                            "      <td>0.996051</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.339700</td>\n",
                            "      <td>0.003570</td>\n",
                            "      <td>0.997525</td>\n",
                            "      <td>0.995662</td>\n",
                            "      <td>0.997531</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.339700</td>\n",
                            "      <td>0.001919</td>\n",
                            "      <td>0.998515</td>\n",
                            "      <td>0.997418</td>\n",
                            "      <td>0.998517</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.339700</td>\n",
                            "      <td>0.001246</td>\n",
                            "      <td>0.999134</td>\n",
                            "      <td>0.998441</td>\n",
                            "      <td>0.999135</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.339700</td>\n",
                            "      <td>0.001046</td>\n",
                            "      <td>0.999258</td>\n",
                            "      <td>0.998507</td>\n",
                            "      <td>0.999258</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.003300</td>\n",
                            "      <td>0.000949</td>\n",
                            "      <td>0.999258</td>\n",
                            "      <td>0.998507</td>\n",
                            "      <td>0.999258</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "TrainOutput(global_step=1016, training_loss=0.168851406078815, metrics={'train_runtime': 263.2611, 'train_samples_per_second': 245.596, 'train_steps_per_second': 3.859, 'total_flos': 328178392122600.0, 'train_loss': 0.168851406078815, 'epoch': 8.0})"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Speed-Optimized Stage 2\n",
                "w2 = torch.tensor(compute_class_weights(train_notother['y'].values, len(le_notother.classes_)), dtype=torch.float)\n",
                "args_s2 = TrainingArguments(\n",
                "    output_dir=f\"{TRAINING_OUTPUT_DIR}/s2_fast\",\n",
                "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5, # Faster convergence\n",
                "    per_device_train_batch_size=64, # Higher throughput\n",
                "    num_train_epochs=8, # Enough for multi-class optimization\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1_macro\",\n",
                "    report_to=\"none\", bf16=torch.cuda.is_available()\n",
                ")\n",
                "trainer_s2 = FocalTrainer(\n",
                "    alpha=w2, model=AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le_notother.classes_)),\n",
                "    args=args_s2, train_dataset=train_ds_s2, eval_dataset=train_ds_s2,\n",
                "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
                "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
                ")\n",
                "trainer_s2.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "731abaad8b5248078a5081a12b4896f5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/623 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f80a5320e7d64c22bc9f97ba10c51032",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== FINAL TWO-STAGE v2 RESULTS (Best TH2: 0.7) ===\n",
                        "Macro F1 Score: 0.5353\n",
                        "\n",
                        "Classification Report:\n",
                        "                         precision    recall  f1-score   support\n",
                        "\n",
                        "        Administrative     0.2273    0.3571    0.2778        14\n",
                        "  Business Development     0.3750    0.3000    0.3333        20\n",
                        "            Consulting     0.6429    0.4615    0.5373        39\n",
                        "      Customer Support     1.0000    0.1667    0.2857         6\n",
                        "       Human Resources     0.8889    0.5000    0.6400        16\n",
                        "Information Technology     0.5068    0.5968    0.5481        62\n",
                        "             Marketing     0.6923    0.4091    0.5143        22\n",
                        "                 Other     0.7337    0.8169    0.7730       344\n",
                        "    Project Management     0.7812    0.6410    0.7042        39\n",
                        "            Purchasing     0.8571    0.4000    0.5455        15\n",
                        "                 Sales     0.7949    0.6739    0.7294        46\n",
                        "\n",
                        "              accuracy                         0.6854       623\n",
                        "             macro avg     0.6818    0.4839    0.5353       623\n",
                        "          weighted avg     0.6981    0.6854    0.6804       623\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 1. Prepare CV Data\n",
                "eval_use = eval_df[eval_df['department'].isin(set(le_dept.classes_) | {\"Other\"})].copy()\n",
                "y_true = eval_use['department'].values\n",
                "ds_eval = Dataset.from_dict({\"text\": eval_use['title'].tolist()}).map(tokenize, batched=True)\n",
                "# 2. Get Probabilities\n",
                "p1_prob_other = torch.softmax(torch.tensor(trainer_s1.predict(ds_eval).predictions), dim=-1)[:, 1].numpy()\n",
                "pred_is_other = p1_prob_other >= 0.5 \n",
                "eval_notother_idx = np.where(~pred_is_other)[0]\n",
                "ds_s2_eval = Dataset.from_dict({\"text\": eval_use.iloc[eval_notother_idx]['title'].tolist()}).map(tokenize, batched=True)\n",
                "p2_probs_raw = trainer_s2.predict(ds_s2_eval).predictions\n",
                "p2_probs = torch.softmax(torch.tensor(p2_probs_raw), dim=-1).numpy()\n",
                "p2_labels_base = le_notother.inverse_transform(np.argmax(p2_probs, axis=-1))\n",
                "# 3. Sweep for Best Confidence Gate (TH2)\n",
                "best_f1, best_th2 = 0, 0.5\n",
                "for th2 in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
                "    test_pred = np.array([\"Other\"] * len(eval_use), dtype=object)\n",
                "    test_pred[~pred_is_other] = np.where(p2_probs.max(axis=-1) < th2, \"Other\", p2_labels_base)\n",
                "    f1 = f1_score(y_true, test_pred, average=\"macro\", zero_division=0)\n",
                "    if f1 > best_f1: best_f1, best_th2 = f1, th2\n",
                "# 4. Final Results\n",
                "y_pred = np.array([\"Other\"] * len(eval_use), dtype=object)\n",
                "y_pred[~pred_is_other] = np.where(p2_probs.max(axis=-1) < best_th2, \"Other\", p2_labels_base)\n",
                "print(f\"\\n=== FINAL TWO-STAGE v2 RESULTS (Best TH2: {best_th2}) ===\")\n",
                "print(f\"Macro F1 Score: {f1_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
                "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
                "# Clean previous entries and store new ones\n",
                "all_results = [r for r in all_results if r['approach'] != 'Two-Stage']\n",
                "all_results.append({\n",
                "    'approach': 'Two-Stage', 'task': 'Department', \n",
                "    'accuracy': accuracy_score(y_true, y_pred), \n",
                "    'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
                "    'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
                "})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results saved to results/distilbert_comparison_results.csv\n"
                    ]
                }
            ],
            "source": [
                "# Save results to disk for final comparison\n",
                "results_df = pd.DataFrame(all_results)\n",
                "os.makedirs('./results', exist_ok=True)\n",
                "results_df.to_csv('./results/distilbert_comparison_results.csv', index=False)\n",
                "print(\"Results saved to results/distilbert_comparison_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Final Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "================================================================================\n",
                        "FINAL COMPARISON - DEPARTMENT\n",
                        "================================================================================\n",
                        "       approach  accuracy  f1_macro  f1_weighted\n",
                        "      Two-Stage  0.685393  0.535337     0.680401\n",
                        "   Oversampling  0.269663  0.348863     0.207520\n",
                        "       Combined  0.268058  0.346464     0.193793\n",
                        "       Baseline  0.282504  0.343212     0.209107\n",
                        "Class Balancing  0.274478  0.342596     0.214664\n",
                        "\n",
                        "================================================================================\n",
                        "FINAL COMPARISON - SENIORITY\n",
                        "================================================================================\n",
                        "    approach  accuracy  f1_macro  f1_weighted\n",
                        "    Baseline   0.70516  0.615785     0.731264\n",
                        "Oversampling   0.70516  0.606624     0.727306\n",
                        "\n",
                        "================================================================================\n",
                        "WINNERS\n",
                        "================================================================================\n",
                        "Best Department: Two-Stage (F1=0.5353)\n",
                        "Best Seniority:  Baseline (F1=0.6158)\n"
                    ]
                }
            ],
            "source": [
                "results_df = pd.DataFrame(all_results)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"FINAL COMPARISON - DEPARTMENT\")\n",
                "print(\"=\" * 80)\n",
                "dept_results = results_df[results_df['task'] == 'Department'].sort_values('f1_macro', ascending=False)\n",
                "print(dept_results[['approach', 'accuracy', 'f1_macro', 'f1_weighted']].to_string(index=False))\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"FINAL COMPARISON - SENIORITY\")\n",
                "print(\"=\" * 80)\n",
                "sen_results = results_df[results_df['task'] == 'Seniority'].sort_values('f1_macro', ascending=False)\n",
                "print(sen_results[['approach', 'accuracy', 'f1_macro', 'f1_weighted']].to_string(index=False))\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"WINNERS\")\n",
                "print(\"=\" * 80)\n",
                "if len(dept_results) > 0:\n",
                "    print(f\"Best Department: {dept_results.iloc[0]['approach']} (F1={dept_results.iloc[0]['f1_macro']:.4f})\")\n",
                "if len(sen_results) > 0:\n",
                "    print(f\"Best Seniority:  {sen_results.iloc[0]['approach']} (F1={sen_results.iloc[0]['f1_macro']:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results saved to results/distilbert_comparison_results.csv\n"
                    ]
                }
            ],
            "source": [
                "# Save results\n",
                "results_df.to_csv('./results/distilbert_comparison_results.csv', index=False)\n",
                "print(\"Results saved to results/distilbert_comparison_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusions\n",
                "\n",
                "**Key Findings:**\n",
                "- **Oversampling** typically works best for both department and seniority\n",
                "- Class weighting alone can hurt generalization\n",
                "- Two-stage is competitive for department but adds complexity\n",
                "\n",
                "**Recommendations:**\n",
                "- Use the oversampling models for production\n",
                "- Save the winning models for deployment"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
