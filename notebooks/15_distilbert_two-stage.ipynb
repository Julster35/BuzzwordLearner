{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be228e23",
   "metadata": {},
   "source": [
    "Zelle 1: Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers datasets accelerate scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ae037",
   "metadata": {},
   "source": [
    "Zelle 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639bbd1",
   "metadata": {},
   "source": [
    "Zelle 3: Pfade + Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPT_CSV = \"data/department-v2.csv\"\n",
    "CV_ANN   = \"data/linkedin-cvs-annotated.json\"\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "MAX_LEN = 32\n",
    "SEED = 42\n",
    "\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c445ca4",
   "metadata": {},
   "source": [
    "Zelle 4: Train Lookup laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_df = pd.read_csv(DEPT_CSV).dropna(subset=[\"text\", \"label\"]).copy()\n",
    "dept_df[\"text\"]  = dept_df[\"text\"].astype(str).str.strip()\n",
    "dept_df[\"label\"] = dept_df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Train rows:\", len(dept_df))\n",
    "display(dept_df.head())\n",
    "print(dept_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6ebef",
   "metadata": {},
   "source": [
    "Zelle 5: Eval laden (nur messen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ead28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CV_ANN, \"r\", encoding=\"utf-8\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "positions = [p for cv in ann for p in cv]\n",
    "eval_df = pd.DataFrame(positions)\n",
    "eval_df[\"status\"] = eval_df[\"status\"].astype(str).str.upper()\n",
    "eval_df = eval_df[eval_df[\"status\"] == \"ACTIVE\"].copy()\n",
    "\n",
    "eval_df[\"title\"] = eval_df[\"position\"].astype(str).str.strip()\n",
    "eval_df[\"department\"] = eval_df[\"department\"].astype(str).str.strip()\n",
    "eval_df = eval_df[[\"title\", \"department\"]].dropna().copy()\n",
    "\n",
    "print(\"Eval rows:\", len(eval_df))\n",
    "print(eval_df[\"department\"].value_counts())\n",
    "display(eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299503a3",
   "metadata": {},
   "source": [
    "Zelle 6: Tokenizer + Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29705b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def make_ds(df, text_col, label_col=None):\n",
    "    data = {\"text\": df[text_col].astype(str).tolist()}\n",
    "    if label_col is not None:\n",
    "        data[\"labels\"] = df[label_col].astype(int).tolist()\n",
    "    ds = Dataset.from_dict(data)\n",
    "    return ds.map(tokenize, batched=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "def make_trainer(task_name, num_labels, train_ds, val_ds, lr=2e-5, batch_size=512, epochs=20, patience=3):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./out_{task_name}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        learning_rate=lr,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.06,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        logging_steps=50,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_int, num_classes):\n",
    "    counts = np.bincount(y_int, minlength=num_classes)\n",
    "    total = counts.sum()\n",
    "    weights = total / (num_classes * counts)\n",
    "    return weights\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd200d38",
   "metadata": {},
   "source": [
    "STAGE 1: Other vs NotOther\n",
    "\n",
    "Zelle 7: Stage1 Dataset bauen (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_df = dept_df.copy()\n",
    "stage1_df[\"bin_label\"] = (stage1_df[\"label\"] == \"Other\").astype(int)  # 1=Other, 0=NotOther\n",
    "\n",
    "train1, val1 = train_test_split(\n",
    "    stage1_df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=stage1_df[\"bin_label\"]\n",
    ")\n",
    "\n",
    "train1_ds = make_ds(train1, \"text\", \"bin_label\")\n",
    "val1_ds   = make_ds(val1, \"text\", \"bin_label\")\n",
    "\n",
    "print(\"Stage1 train:\", len(train1), \"val:\", len(val1))\n",
    "print(train1[\"bin_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491b7ee",
   "metadata": {},
   "source": [
    "Zelle 8: Stage1 train (optional LR Sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ea401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 train (Other vs NotOther) mit Class Weights\n",
    "\n",
    "w1 = torch.tensor(compute_class_weights(train1[\"bin_label\"].values, 2), dtype=torch.float)\n",
    "print(\"Stage1 class weights:\", w1.tolist())\n",
    "print(\"Stage1 train label counts:\\n\", train1[\"bin_label\"].value_counts())\n",
    "\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=\"./out_dept_stage1_other_vs_not\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=512,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "stage1_trainer = WeightedTrainer(\n",
    "    class_weights=w1,\n",
    "    model=model1,\n",
    "    args=args1,\n",
    "    train_dataset=train1_ds,\n",
    "    eval_dataset=val1_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "stage1_trainer.train()\n",
    "print(\"Stage1 val:\", stage1_trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef44427",
   "metadata": {},
   "source": [
    "STAGE 2: Multi-class f체r NotOther\n",
    "\n",
    "Zelle 9: Stage2 Dataset bauen (nur NotOther)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867359bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_df = dept_df[dept_df[\"label\"] != \"Other\"].copy()\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "stage2_df[\"y\"] = le2.fit_transform(stage2_df[\"label\"])\n",
    "\n",
    "train2, val2 = train_test_split(\n",
    "    stage2_df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=stage2_df[\"y\"]\n",
    ")\n",
    "\n",
    "train2_ds = make_ds(train2, \"text\", \"y\")\n",
    "val2_ds   = make_ds(val2, \"text\", \"y\")\n",
    "\n",
    "print(\"Stage2 train:\", len(train2), \"val:\", len(val2))\n",
    "print(\"Stage2 classes:\", list(le2.classes_))\n",
    "print(stage2_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f0a27",
   "metadata": {},
   "source": [
    "Zelle 10: Stage2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 train (NotOther Multi-class) mit Class Weights\n",
    "\n",
    "w2 = torch.tensor(compute_class_weights(train2[\"y\"].values, len(le2.classes_)), dtype=torch.float)\n",
    "print(\"Stage2 class weights (first 10):\", w2[:10].tolist())\n",
    "print(\"Stage2 train label counts:\\n\", train2[\"label\"].value_counts().head(20))\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le2.classes_))\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=\"./out_dept_stage2_notother_multiclass\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=512,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "stage2_trainer = WeightedTrainer(\n",
    "    class_weights=w2,\n",
    "    model=model2,\n",
    "    args=args2,\n",
    "    train_dataset=train2_ds,\n",
    "    eval_dataset=val2_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "stage2_trainer.train()\n",
    "print(\"Stage2 val:\", stage2_trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f7d1e",
   "metadata": {},
   "source": [
    "INFERENCE PIPELINE (Stage1 -> Stage2)\n",
    "\n",
    "Zelle 11: Zwei-Stufen Vorhersage auf eval_df + Metriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d69e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval labels filter: nur labels, die Stage2 kennt + Other\n",
    "valid_labels = set(le2.classes_) | {\"Other\"}\n",
    "eval_use = eval_df[eval_df[\"department\"].isin(valid_labels)].copy()\n",
    "print(\"Eval used:\", len(eval_use))\n",
    "\n",
    "# Stage1 prediction: Other prob\n",
    "eval_stage1_ds = Dataset.from_dict({\"text\": eval_use[\"title\"].astype(str).tolist()}).map(tokenize, batched=True)\n",
    "p1 = stage1_trainer.predict(eval_stage1_ds).predictions\n",
    "p1_prob_other = torch.softmax(torch.tensor(p1), dim=-1)[:, 1].numpy()\n",
    "\n",
    "# Threshold f체r Other (0.5 default, kannst du sp채ter tunen)\n",
    "TH = 0.5\n",
    "pred_is_other = p1_prob_other >= TH\n",
    "\n",
    "# Stage2 prediction nur f체r NotOther\n",
    "eval_notother = eval_use.loc[~pred_is_other].copy()\n",
    "eval_stage2_ds = Dataset.from_dict({\"text\": eval_notother[\"title\"].astype(str).tolist()}).map(tokenize, batched=True)\n",
    "p2 = stage2_trainer.predict(eval_stage2_ds).predictions\n",
    "p2_ids = np.argmax(p2, axis=-1)\n",
    "p2_labels = le2.inverse_transform(p2_ids)\n",
    "\n",
    "# Combine predictions\n",
    "y_pred = np.array([\"Other\"] * len(eval_use), dtype=object)\n",
    "y_pred[~pred_is_other] = p2_labels\n",
    "\n",
    "y_true = eval_use[\"department\"].astype(str).values\n",
    "\n",
    "print(\"\\n=== Two-stage FINAL EVAL on eval_df (Department) ===\")\n",
    "print(\"Accuracy       :\", accuracy_score(y_true, y_pred))\n",
    "print(\"Macro Precision:\", precision_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "print(\"Macro Recall   :\", recall_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "print(\"Macro F1       :\", f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "print(\"Weighted F1    :\", f1_score(y_true, y_pred, average=\"weighted\", zero_division=0))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "preview = eval_use[[\"title\", \"department\"]].copy()\n",
    "preview[\"pred\"] = y_pred\n",
    "preview[\"p_other\"] = p1_prob_other\n",
    "preview[\"correct\"] = preview[\"department\"] == preview[\"pred\"]\n",
    "display(preview.head(50))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
