{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-Based Baseline with Text Normalization\n",
    "\n",
    "**Approach**: Exact and fuzzy string matching against lookup tables with text normalization\n",
    "\n",
    "## Baseline Performance Target\n",
    "\n",
    "From the EDA (Exploratory Data Analysis) we observed the class distribution in our evaluation dataset:\n",
    "- **55% of departments are labeled as \"Other\"**\n",
    "- **35% of seniority levels are \"Professional\"**\n",
    "\n",
    "**Naive Baseline**: A model that always predicts \"Other\" for department would achieve 55% accuracy. A model that always predicts \"Professional\" for seniority would achieve 35% accuracy.\n",
    "\n",
    "**Goal**: Every model in this project must beat these naive baseline accuracies.\n",
    "\n",
    "## Model Configuration:\n",
    "This notebook implements the optimized rule-based approach with:\n",
    "\n",
    "1. **Text Normalization**: Lowercase conversion + whitespace cleaning\n",
    "2. **Department Classification**: Can fall back to \"Other\" if no match found\n",
    "3. **Seniority Classification**: Must always assign one of 6 levels, can fall back to \"Professional\"\n",
    "\n",
    "## Module Optimizations (src/models/rule_based.py):\n",
    "- âœ… Fuzzy matching implementation using `difflib.SequenceMatcher`\n",
    "- âœ… Strategy reordering: Fast methods (Keyword) before slow methods (Fuzzy)\n",
    "- âœ… Length-based pre-filtering to skip impossible matches\n",
    "\n",
    "## Matching Strategies (in order):\n",
    "1. Exact match (O(1) - instant)\n",
    "2. Substring match (O(n) - fast)\n",
    "3. Keyword match (O(n) - fast)\n",
    "4. Fuzzy match (O(n*m) - last resort, optimized)\n",
    "5. Default fallback (Department: \"Other\", Seniority: \"Professional\")\n",
    "\n",
    "**Training Data**: Lookup tables (~19k examples)  \n",
    "**Validation Data**: Annotated LinkedIn CVs (623 positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import data loaders and models\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.data.loader import load_label_lists, load_inference_dataset, load_evaluation_dataset\n",
    "from src.models.rule_based import RuleConfig, create_department_classifier, create_seniority_classifier\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "RESULTS_DIR = Path('./results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data (Lookup Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lookup tables\n",
    "# removed capping\n",
    "dept_df, sen_df = load_label_lists(DATA_DIR, max_per_class=None)\n",
    "\n",
    "print(f\"Department lookup: {len(dept_df):,} examples\")\n",
    "print(f\"Seniority lookup:  {len(sen_df):,} examples\")\n",
    "print(f\"\\nUnique departments: {dept_df['label'].nunique()}\")\n",
    "print(f\"Unique seniority levels: {sen_df['label'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Note: Rule-based models do not \"train\" on the lookup tables.\n",
    "# The lookup tables are treated as fixed rule/label dictionaries.\n",
    "# We only use train_test_split later to create a fair dev/test split\n",
    "# on the annotated SnapAddy evaluation dataset (to avoid tuning on the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Rule-Based Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.rule_based import RuleConfig, create_department_classifier, create_seniority_classifier\n",
    "\n",
    "# Configure WITH text normalization (best performance)\n",
    "# IMPORTANT: Department can have \"Other\", but Seniority must choose from 6 levels\n",
    "config_dept = RuleConfig(\n",
    "    fuzzy_threshold=0.8, \n",
    "    use_text_normalization=True,  # Enable normalization for best results\n",
    "    default_label=\"Other\"  # Department can fall back to \"Other\"\n",
    ")\n",
    "\n",
    "config_sen = RuleConfig(\n",
    "    fuzzy_threshold=0.8, \n",
    "    use_text_normalization=True,  # Enable normalization for best results\n",
    "    default_label=\"Professional\"  # Seniority must choose one of the 6 levels (most common fallback)\n",
    ")\n",
    "\n",
    "# Create classifiers WITH text normalization AND keyword matching\n",
    "# Using factory functions to automatically include predefined keywords\n",
    "dept_clf = create_department_classifier(dept_df, config=config_dept)\n",
    "sen_clf = create_seniority_classifier(sen_df, config=config_sen)\n",
    "\n",
    "print(\"âœ… Department classifier created (WITH text normalization + keywords)\")\n",
    "print(f\"   Text normalization: âœ… ENABLED (lowercase + whitespace)\")\n",
    "print(f\"   Keyword matching: âœ… ENABLED (predefined department keywords)\")\n",
    "print(f\"   Default fallback: 'Other' (department can be unspecified)\")\n",
    "print(\"âœ… Seniority classifier created (WITH text normalization + keywords)\")\n",
    "print(f\"   Text normalization: âœ… ENABLED (lowercase + whitespace)\")\n",
    "print(f\"   Keyword matching: âœ… ENABLED (predefined seniority keywords)\")\n",
    "print(f\"   Default fallback: 'Professional' (must choose from 6 levels)\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Fuzzy match threshold: {config_dept.fuzzy_threshold} ({config_dept.fuzzy_threshold * 100:.0f}% similarity required)\")\n",
    "print(f\"\\nAvailable Seniority Levels: Director, Junior, Lead, Management, Professional, Senior\")\n",
    "print(f\"Note: Seniority has NO 'Other' category - must always assign one of the 6 levels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Text Normalization Effect\n",
    "\n",
    "Let's see how text normalization works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How text normalization works\n",
    "from src.models.rule_based import HybridRuleClassifier\n",
    "\n",
    "# Create a dummy classifier to use the normalization function\n",
    "dummy_df = pd.DataFrame({'text': ['dummy'], 'label': ['dummy']})\n",
    "dummy_clf = HybridRuleClassifier(dummy_df)\n",
    "\n",
    "# Test cases showing normalization effects\n",
    "test_cases = [\n",
    "    \"Senior  Software   Engineer\",      # Multiple spaces\n",
    "    \"Senior Software Engineer\",         # Normal\n",
    "    \" Senior Software Engineer \",       # Leading/trailing spaces\n",
    "    \"SENIOR SOFTWARE ENGINEER\",         # Uppercase\n",
    "    \"Senior\\tSoftware\\nEngineer\",       # Tabs & newlines\n",
    "    \"  TEAM  LEADER   IT  \",            # Mixed issues\n",
    "]\n",
    "\n",
    "print(\"Text Normalization Examples:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Original':<40} â†’ {'Normalized':<40}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for text in test_cases:\n",
    "    normalized = dummy_clf._clean_text(text)\n",
    "    # Show whitespace issues visually\n",
    "    original_display = repr(text)[:38]\n",
    "    print(f\"{original_display:<40} â†’ {normalized:<40}\")\n",
    "\n",
    "print(\"\\nâœ… All variations are now normalized to the same format!\")\n",
    "print(\"This helps matching: 'Senior  Engineer' will now match 'Senior Engineer' in lookup table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference Demo on Unannotated CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unannotated CVs for demonstration\n",
    "inference_df = load_inference_dataset(DATA_DIR)\n",
    "\n",
    "print(f\"Loaded {len(inference_df):,} unannotated CV positions for inference demo\")\n",
    "print(f\"\\nFirst few job titles:\")\n",
    "for i, row in inference_df.head(10).iterrows():\n",
    "    print(f\"  {i+1}. {row['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on sample\n",
    "sample_titles = inference_df['title'].head(20).tolist()\n",
    "\n",
    "# Predict with details (includes match method and confidence)\n",
    "dept_preds = dept_clf.predict_with_details(sample_titles)\n",
    "sen_preds = sen_clf.predict_with_details(sample_titles)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"INFERENCE DEMO: Rule-Based Predictions\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Title':<40} | {'Department':<20} | {'Seniority':<15} | {'Methods':<20}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for title, (dept, dept_conf, dept_method), (sen, sen_conf, sen_method) in zip(sample_titles, dept_preds, sen_preds):\n",
    "    title_short = title[:37] + \"...\" if len(title) > 40 else title\n",
    "    methods = f\"{dept_method[:4]}/{sen_method[:4]}\"\n",
    "    print(f\"{title_short:<40} | {dept:<20} | {sen:<15} | {methods:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Match Method Statistics\n",
    "\n",
    "Let's analyze which matching strategies are being used most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze matching methods on inference data\n",
    "all_titles = inference_df['title'].tolist()\n",
    "dept_stats = dept_clf.get_stats(all_titles)\n",
    "sen_stats = sen_clf.get_stats(all_titles)\n",
    "\n",
    "print(\"Department Classification Methods:\")\n",
    "print(\"-\" * 50)\n",
    "total = len(all_titles)\n",
    "for method, count in dept_stats.items():\n",
    "    pct = 100 * count / total\n",
    "    print(f\"  {method.capitalize():<15}: {count:>5} ({pct:>5.1f}%)\")\n",
    "print(f\"  {'TOTAL':<15}: {total:>5} (100.0%)\")\n",
    "\n",
    "print(\"\\nSeniority Classification Methods:\")\n",
    "print(\"-\" * 50)\n",
    "for method, count in sen_stats.items():\n",
    "    pct = 100 * count / total\n",
    "    print(f\"  {method.capitalize():<15}: {count:>5} ({pct:>5.1f}%)\")\n",
    "print(f\"  {'TOTAL':<15}: {total:>5} (100.0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Annotated Dataset\n",
    "\n",
    "**FIRST TIME LOADING ANNOTATED DATA**\n",
    "\n",
    "Now we evaluate on the held-out annotated dataset to get true performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotated dataset for evaluation\n",
    "eval_df = load_evaluation_dataset(DATA_DIR)\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(eval_df)} annotated CV positions for evaluation\")\n",
    "print(f\"\\nColumns: {list(eval_df.columns)}\")\n",
    "print(f\"\\nChecking for missing labels:\")\n",
    "print(f\"  Missing department labels: {eval_df['department'].isna().sum()}\")\n",
    "print(f\"  Missing seniority labels: {eval_df['seniority'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dev/test split on the annotated evaluation dataset\n",
    "# (dev can be used for threshold/keyword tuning; test is held-out for reporting)\n",
    "try:\n",
    "    eval_dev_df, eval_test_df = train_test_split(\n",
    "        eval_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=eval_df[\"department\"]\n",
    "    )\n",
    "except ValueError:\n",
    "    # Fallback if stratification is not possible due to very small classes\n",
    "    eval_dev_df, eval_test_df = train_test_split(\n",
    "        eval_df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(f\"Evaluation split: Dev={len(eval_dev_df):,}, Test={len(eval_test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Department Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on evaluation set\n",
    "eval_titles = eval_test_df['title'].tolist()\n",
    "dept_predictions = dept_clf.predict(eval_titles)\n",
    "\n",
    "# Ground truth\n",
    "dept_true = eval_test_df['department'].tolist()\n",
    "\n",
    "# Calculate metrics\n",
    "dept_accuracy = accuracy_score(dept_true, dept_predictions)\n",
    "dept_precision, dept_recall, dept_f1, _ = precision_recall_fscore_support(\n",
    "    dept_true, dept_predictions, average='macro', zero_division=0\n",
    ")\n",
    "dept_weighted_f1 = precision_recall_fscore_support(\n",
    "    dept_true, dept_predictions, average='weighted', zero_division=0\n",
    ")[2]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEPARTMENT CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:          {dept_accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {dept_precision:.4f}\")\n",
    "print(f\"Recall (macro):    {dept_recall:.4f}\")\n",
    "print(f\"F1-score (macro):  {dept_f1:.4f}\")\n",
    "print(f\"F1-score (wtd):    {dept_weighted_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores\n",
    "dept_report = classification_report(dept_true, dept_predictions, output_dict=True, zero_division=0)\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores (Department):\")\n",
    "dept_f1_scores = {label: metrics['f1-score'] for label, metrics in dept_report.items() \n",
    "                  if label not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "for label, f1 in sorted(dept_f1_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {label:<30}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Seniority Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on evaluation set\n",
    "sen_predictions = sen_clf.predict(eval_titles)\n",
    "\n",
    "# Ground truth\n",
    "sen_true = eval_test_df['seniority'].tolist()\n",
    "\n",
    "# Calculate metrics\n",
    "sen_accuracy = accuracy_score(sen_true, sen_predictions)\n",
    "sen_precision, sen_recall, sen_f1, _ = precision_recall_fscore_support(\n",
    "    sen_true, sen_predictions, average='macro', zero_division=0\n",
    ")\n",
    "sen_weighted_f1 = precision_recall_fscore_support(\n",
    "    sen_true, sen_predictions, average='weighted', zero_division=0\n",
    ")[2]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENIORITY CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:          {sen_accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {sen_precision:.4f}\")\n",
    "print(f\"Recall (macro):    {sen_recall:.4f}\")\n",
    "print(f\"F1-score (macro):  {sen_f1:.4f}\")\n",
    "print(f\"F1-score (wtd):    {sen_weighted_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores\n",
    "sen_report = classification_report(sen_true, sen_predictions, output_dict=True, zero_division=0)\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores (Seniority):\")\n",
    "sen_f1_scores = {label: metrics['f1-score'] for label, metrics in sen_report.items() \n",
    "                 if label not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "for label, f1 in sorted(sen_f1_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {label:<30}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Department confusion matrix\n",
    "dept_cm = confusion_matrix(dept_true, dept_predictions)\n",
    "dept_labels = sorted(set(dept_true + dept_predictions))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(dept_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=dept_labels, yticklabels=dept_labels)\n",
    "plt.title('Department Confusion Matrix (Rule-Based)', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seniority confusion matrix\n",
    "sen_cm = confusion_matrix(sen_true, sen_predictions)\n",
    "sen_labels = sorted(set(sen_true + sen_predictions))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sen_cm, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=sen_labels, yticklabels=sen_labels)\n",
    "plt.title('Seniority Confusion Matrix (Rule-Based)', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples (on the held-out TEST split)\n",
    "eval_results_df = eval_test_df.copy()\n",
    "\n",
    "eval_results_df['dept_pred'] = dept_predictions\n",
    "eval_results_df['sen_pred'] = sen_predictions\n",
    "eval_results_df['dept_correct'] = eval_results_df['department'] == eval_results_df['dept_pred']\n",
    "eval_results_df['sen_correct'] = eval_results_df['seniority'] == eval_results_df['sen_pred']\n",
    "\n",
    "# Department errors\n",
    "dept_errors = eval_results_df[~eval_results_df['dept_correct']]\n",
    "print(f\"\\nDepartment errors (TEST): {len(dept_errors)} / {len(eval_results_df)} ({100*len(dept_errors)/len(eval_results_df):.1f}%)\")\n",
    "print(\"\\nExample misclassifications (Department):\")\n",
    "for i, row in dept_errors.head(10).iterrows():\n",
    "    print(f\"  Title: {row['title']}\")\n",
    "    print(f\"  True:  {row['department']}  | Pred: {row['dept_pred']}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Seniority errors\n",
    "sen_errors = eval_results_df[~eval_results_df['sen_correct']]\n",
    "print(f\"\\nSeniority errors (TEST): {len(sen_errors)} / {len(eval_results_df)} ({100*len(sen_errors)/len(eval_results_df):.1f}%)\")\n",
    "print(\"\\nExample misclassifications (Seniority):\")\n",
    "for i, row in sen_errors.head(10).iterrows():\n",
    "    print(f\"  Title: {row['title']}\")\n",
    "    print(f\"  True:  {row['seniority']}  | Pred: {row['sen_pred']}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Performance Comparison\n",
    "\n",
    "Compare performance on two different test sets:\n",
    "- **In-Distribution (CSV)**: Test split from lookup tables (dept_df, sen_df)\n",
    "- **Real-World (JSON)**: Annotated LinkedIn CVs (eval_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test splits from lookup tables (in-distribution evaluation)\n",
    "dept_train, dept_test = train_test_split(dept_df, test_size=0.2, random_state=42)\n",
    "sen_train, sen_test = train_test_split(sen_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"In-Distribution Test Sets Created:\")\n",
    "print(f\"  Department: {len(dept_test):,} samples\")\n",
    "print(f\"  Seniority:  {len(sen_test):,} samples\")\n",
    "\n",
    "# A. In-Distribution Evaluation (CSV Test Split)\n",
    "dept_id_preds = dept_clf.predict(dept_test['text'])\n",
    "sen_id_preds = sen_clf.predict(sen_test['text'])\n",
    "acc_dept_id = accuracy_score(dept_test['label'], dept_id_preds)\n",
    "acc_sen_id = accuracy_score(sen_test['label'], sen_id_preds)\n",
    "\n",
    "# B. Real-World Evaluation (JSON) - already computed\n",
    "# Using existing variables: dept_accuracy, sen_accuracy from eval_test_df\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"DEPARTMENT - In-Distribution (CSV): {acc_dept_id:.4f}\")\n",
    "print(f\"DEPARTMENT - Real-World (JSON):     {dept_accuracy:.4f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"SENIORITY  - In-Distribution (CSV): {acc_sen_id:.4f}\")\n",
    "print(f\"SENIORITY  - Real-World (JSON):     {sen_accuracy:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extended results dictionary with both evaluations\n",
    "results_comprehensive = {\n",
    "    \"approach\": \"Rule-Based (Exact + Fuzzy Matching) WITH Text Normalization\",\n",
    "    \"text_normalization\": True,\n",
    "    \"improvements\": [\"Lowercase conversion\", \"Whitespace normalization\"],\n",
    "    \"department\": {\n",
    "        \"in_distribution\": {\n",
    "            \"accuracy\": float(acc_dept_id),\n",
    "            \"test_samples\": len(dept_test)\n",
    "        },\n",
    "        \"real_world\": {\n",
    "            \"accuracy\": float(dept_accuracy),\n",
    "            \"precision\": float(dept_precision),\n",
    "            \"recall\": float(dept_recall),\n",
    "            \"f1_macro\": float(dept_f1),\n",
    "            \"f1_weighted\": float(dept_weighted_f1),\n",
    "            \"per_class_f1\": {k: float(v) for k, v in dept_f1_scores.items()},\n",
    "            \"test_samples\": len(eval_test_df)\n",
    "        }\n",
    "    },\n",
    "    \"seniority\": {\n",
    "        \"in_distribution\": {\n",
    "            \"accuracy\": float(acc_sen_id),\n",
    "            \"test_samples\": len(sen_test)\n",
    "        },\n",
    "        \"real_world\": {\n",
    "            \"accuracy\": float(sen_accuracy),\n",
    "            \"precision\": float(sen_precision),\n",
    "            \"recall\": float(sen_recall),\n",
    "            \"f1_macro\": float(sen_f1),\n",
    "            \"f1_weighted\": float(sen_weighted_f1),\n",
    "            \"per_class_f1\": {k: float(v) for k, v in sen_f1_scores.items()},\n",
    "            \"test_samples\": len(eval_test_df)\n",
    "        }\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"evaluation_split\": {\n",
    "            \"csv_dev_size\": len(dept_train) + len(sen_train),\n",
    "            \"csv_test_size\": len(dept_test) + len(sen_test),\n",
    "            \"json_dev_size\": len(eval_dev_df),\n",
    "            \"json_test_size\": len(eval_test_df)\n",
    "        },\n",
    "        \"training_samples\": len(dept_df) + len(sen_df),\n",
    "        \"dept_training_samples\": len(dept_df),\n",
    "        \"sen_training_samples\": len(sen_df),\n",
    "        \"fuzzy_threshold\": 0.8,\n",
    "        \"dept_default\": \"Other\",\n",
    "        \"sen_default\": \"Professional\",\n",
    "        \"match_methods\": {\n",
    "            \"department\": dept_stats,\n",
    "            \"seniority\": sen_stats\n",
    "        }\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "output_path_comp = RESULTS_DIR / 'rule_based_comprehensive_results.json'\n",
    "with open(output_path_comp, 'w') as f:\n",
    "    json.dump(results_comprehensive, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Comprehensive results saved to: {output_path_comp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
