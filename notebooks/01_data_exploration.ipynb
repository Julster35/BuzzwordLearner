{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Lookup Tables & Unannotated CVs\n",
    "\n",
    "This notebook explores the **training data sources** available for the zero-shot learning problem:\n",
    "- `department-v2.csv` - Job title → department mappings (~10k examples)\n",
    "- `seniority-v2.csv` - Job title → seniority mappings (~9k examples)\n",
    "- `linkedin-cvs-not-annotated.json` - Unannotated LinkedIn CVs for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Import our data loaders\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data.loader import load_label_lists, load_inference_dataset\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Lookup Tables (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load department and seniority lookup tables\n",
    "dept_df, sen_df = load_label_lists(DATA_DIR)\n",
    "\n",
    "print(f\"Department lookup: {len(dept_df):,} examples\")\n",
    "print(f\"Seniority lookup:  {len(sen_df):,} examples\")\n",
    "print(f\"\\nTotal training examples: {len(dept_df) + len(sen_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 1 Improvements Verification\n",
    "\n",
    "Verify that encoding fixes and deduplication are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify encoding fix - check for mojibake markers\n",
    "print(\"Checking for encoding issues (Ã character = mojibake)...\")\n",
    "dept_mojibake = dept_df['text'].str.contains('Ã', na=False).sum()\n",
    "sen_mojibake = sen_df['text'].str.contains('Ã', na=False).sum()\n",
    "print(f\"Department texts with mojibake: {dept_mojibake}\")\n",
    "print(f\"Seniority texts with mojibake: {sen_mojibake}\")\n",
    "\n",
    "# Show some sample texts after encoding fix\n",
    "print(\"\\nSample German texts (should show ä, ö, ü correctly):\")\n",
    "german_samples = dept_df[dept_df['text'].str.contains('ä|ö|ü|ß', na=False, regex=True)]['text'].head(5)\n",
    "for t in german_samples:\n",
    "    print(f\"  - {t}\")\n",
    "\n",
    "# Verify deduplication\n",
    "print(f\"\\n--- After Deduplication ---\")\n",
    "print(f\"Department examples: {len(dept_df)}\")\n",
    "print(f\"Seniority examples: {len(sen_df)}\")\n",
    "\n",
    "print(f\"\\nLabel distribution after dedup:\")\n",
    "print(\"\\nDepartment:\")\n",
    "print(dept_df['label'].value_counts())\n",
    "print(\"\\nSeniority:\")\n",
    "print(sen_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Department Labels Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "dept_counts = dept_df['label'].value_counts()\n",
    "print(\"Department Label Distribution:\")\n",
    "print(dept_counts)\n",
    "print(f\"\\nNumber of unique departments: {len(dept_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize department distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "dept_counts.plot(kind='bar')\n",
    "plt.title('Department Label Distribution (Lookup Table)', fontsize=14)\n",
    "plt.xlabel('Department', fontsize=12)\n",
    "plt.ylabel('Number of Examples', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example job titles per department\n",
    "print(\"\\nExample job titles per department:\\n\")\n",
    "for dept in dept_counts.head(5).index:\n",
    "    examples = dept_df[dept_df['label'] == dept]['text'].head(5).tolist()\n",
    "    print(f\"\\n{dept}:\")\n",
    "    for ex in examples:\n",
    "        print(f\"  - {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seniority Labels Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "sen_counts = sen_df['label'].value_counts()\n",
    "print(\"Seniority Label Distribution:\")\n",
    "print(sen_counts)\n",
    "print(f\"\\nNumber of unique seniority levels: {len(sen_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize seniority distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sen_counts.plot(kind='bar', color='coral')\n",
    "plt.title('Seniority Label Distribution (Lookup Table)', fontsize=14)\n",
    "plt.xlabel('Seniority Level', fontsize=12)\n",
    "plt.ylabel('Number of Examples', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example job titles per seniority level\n",
    "print(\"\\nExample job titles per seniority level:\\n\")\n",
    "for sen in sen_counts.index:\n",
    "    examples = sen_df[sen_df['label'] == sen]['text'].head(5).tolist()\n",
    "    print(f\"\\n{sen}:\")\n",
    "    for ex in examples:\n",
    "        print(f\"  - {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute text statistics\n",
    "dept_df['text_length'] = dept_df['text'].str.len()\n",
    "dept_df['word_count'] = dept_df['text'].str.split().str.len()\n",
    "\n",
    "sen_df['text_length'] = sen_df['text'].str.len()\n",
    "sen_df['word_count'] = sen_df['text'].str.split().str.len()\n",
    "\n",
    "print(\"Department job titles:\")\n",
    "print(f\"  Avg length: {dept_df['text_length'].mean():.1f} chars\")\n",
    "print(f\"  Avg words:  {dept_df['word_count'].mean():.1f} words\")\n",
    "\n",
    "print(\"\\nSeniority job titles:\")\n",
    "print(f\"  Avg length: {sen_df['text_length'].mean():.1f} chars\")\n",
    "print(f\"  Avg words:  {sen_df['word_count'].mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word count distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(dept_df['word_count'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Department: Word Count Distribution', fontsize=12)\n",
    "axes[0].set_xlabel('Number of Words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(sen_df['word_count'], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_title('Seniority: Word Count Distribution', fontsize=12)\n",
    "axes[1].set_xlabel('Number of Words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Unannotated LinkedIn CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unannotated CVs (for inference demonstration)\n",
    "inference_df = load_inference_dataset(DATA_DIR)\n",
    "\n",
    "print(f\"Unannotated LinkedIn CVs: {len(inference_df):,} positions\")\n",
    "print(f\"\\nColumns: {list(inference_df.columns)}\")\n",
    "print(f\"\\nFirst few examples:\")\n",
    "inference_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze LinkedIn CV job titles\n",
    "inference_df['text_length'] = inference_df['title'].str.len()\n",
    "inference_df['word_count'] = inference_df['title'].str.split().str.len()\n",
    "\n",
    "print(\"LinkedIn CV job titles:\")\n",
    "print(f\"  Avg length: {inference_df['text_length'].mean():.1f} chars\")\n",
    "print(f\"  Avg words:  {inference_df['word_count'].mean():.1f} words\")\n",
    "print(f\"  Min/Max words: {inference_df['word_count'].min()} / {inference_df['word_count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions: Lookup tables vs LinkedIn CVs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Word count comparison\n",
    "axes[0].hist(dept_df['word_count'], bins=20, alpha=0.5, label='Lookup (Dept)', edgecolor='black')\n",
    "axes[0].hist(inference_df['word_count'], bins=20, alpha=0.5, label='LinkedIn CVs', edgecolor='black')\n",
    "axes[0].set_title('Word Count Comparison', fontsize=12)\n",
    "axes[0].set_xlabel('Number of Words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Character length comparison\n",
    "axes[1].hist(dept_df['text_length'], bins=20, alpha=0.5, label='Lookup (Dept)', edgecolor='black')\n",
    "axes[1].hist(inference_df['text_length'], bins=20, alpha=0.5, label='LinkedIn CVs', edgecolor='black')\n",
    "axes[1].set_title('Character Length Comparison', fontsize=12)\n",
    "axes[1].set_xlabel('Number of Characters')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple language detection based on keywords\n",
    "def detect_language(text):\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # German indicators\n",
    "    german_words = ['geschäftsführer', 'leiter', 'mitarbeiter', 'projektmanager', \n",
    "                    'entwickler', 'berater', 'assistent', 'sachbearbeiter']\n",
    "    \n",
    "    # French indicators\n",
    "    french_words = ['responsable', 'directeur', 'chef', 'chargé', 'gérant', 'adjoint']\n",
    "    \n",
    "    if any(word in text_lower for word in german_words):\n",
    "        return 'German'\n",
    "    elif any(word in text_lower for word in french_words):\n",
    "        return 'French'\n",
    "    else:\n",
    "        return 'English/Other'\n",
    "\n",
    "# Detect languages\n",
    "dept_df['language'] = dept_df['text'].apply(detect_language)\n",
    "sen_df['language'] = sen_df['text'].apply(detect_language)\n",
    "inference_df['language'] = inference_df['title'].apply(detect_language)\n",
    "\n",
    "print(\"Language distribution in lookup tables:\")\n",
    "print(\"\\nDepartment:\")\n",
    "print(dept_df['language'].value_counts())\n",
    "print(\"\\nSeniority:\")\n",
    "print(sen_df['language'].value_counts())\n",
    "print(\"\\nLinkedIn CVs:\")\n",
    "print(inference_df['language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Observations\n",
    "\n",
    "### Distribution Mismatch\n",
    "- Lookup tables have simpler, cleaner job titles\n",
    "- LinkedIn CVs may have longer, more complex titles\n",
    "- Need models that can generalize from lookup patterns to real-world CVs\n",
    "\n",
    "### Label Imbalance\n",
    "- Some departments/seniority levels have many more examples than others\n",
    "- May need class weighting or stratified sampling\n",
    "\n",
    "### Multilingual Challenge\n",
    "- Data contains German, French, and English\n",
    "- Need multilingual models (e.g., multilingual-BERT, sentence-transformers)\n",
    "\n",
    "### Zero-Shot Setting\n",
    "- No labeled LinkedIn CV data for training\n",
    "- Must transfer knowledge from lookup table patterns\n",
    "- Challenge: How well do lookup table patterns generalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary = pd.DataFrame({\n",
    "    'Dataset': ['Department Lookup', 'Seniority Lookup', 'LinkedIn CVs (Unannotated)'],\n",
    "    'Examples': [len(dept_df), len(sen_df), len(inference_df)],\n",
    "    'Unique Labels': [dept_df['label'].nunique(), sen_df['label'].nunique(), 'N/A'],\n",
    "    'Avg Words': [dept_df['word_count'].mean(), sen_df['word_count'].mean(), inference_df['word_count'].mean()],\n",
    "    'Languages': ['Multi', 'Multi', 'Multi']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
