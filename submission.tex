\documentclass[12pt,oneside]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Additional packages  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumerate}  
\usepackage{fancyhdr}
\usepackage{ifthen}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\usepackage{acronym}
\usepackage{float}


\newboolean{englishLanguage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% CHANGE true to false for a german paper %%%%
\setboolean{englishLanguage}{true} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifthenelse{\boolean{englishLanguage}}{\usepackage[ngerman, english]{babel}}{\usepackage[ngerman]{babel}} 

\usepackage[T1]{fontenc}
\usepackage{paratype}\renewcommand{\familydefault}{\sfdefault}
\usepackage[utf8]{inputenc}
\usepackage[bookmarks]{hyperref}
\usepackage[justification=centering]{caption}
\usepackage[style=authoryear,natbib=true,backend=biber,maxbibnames=20]{biblatex}
\usepackage{csquotes}
\bibliography{literatur}

\setlength{\parindent}{0em} 
\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[enumerate]{noitemsep, topsep=0pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definition of the header %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\setlength{\headheight}{16pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Definition of the cover page and title page  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\ifthenelse{\boolean{englishLanguage}}{
%%% ENGLISH VERSION
\newcommand{\JMUTitle}[9]{

  \thispagestyle{empty}
  \vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}
  }
  \begin{flushright}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #2\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\small
    #3
  \end{flushright}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{1}}
  \begin{center}
    \includegraphics[width=2in]{seal.pdf} \\
    \vspace*{\stretch{1}}
    \Large #5 \\

    \vspace*{\stretch{2}}
   \large Chair for Enterprise Artificial Intelligence\\
    \large Universität Würzburg\\
    \vspace*{\stretch{1}}
    \large Supervisor:  #8 \\[1mm]
    \large Assistant:  #9 \\[1mm]
    \vspace*{\stretch{1}}
    \large #6, #7
  \end{center}
}}{
%%% GERMAN VERSION
\newcommand{\JMUTitle}[9]{

  \thispagestyle{empty}
  \vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}
  }
  \begin{flushright}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #2\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\small
    #3
  \end{flushright}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{1}}
  \begin{center}
    \includegraphics[width=2in]{seal.pdf} \\
    \vspace*{\stretch{1}}
    \Large #5 \\

    \vspace*{\stretch{2}}
   \large Lehrstuhl f\"ur K\"unstliche Intelligenz im Unternehemen\\
    \large Universität Würzburg\\
    \vspace*{\stretch{1}}
    \large Betreuer:  #8 \\[1mm]
    \large Assistent:  #9 \\[1mm]
    \vspace*{\stretch{1}}
    \large #6, den #7
  \end{center}
}} 


\titlespacing*{\section}
{0pt}{3.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsubsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Beginning of the document  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
    \ifthenelse{\boolean{englishLanguage}}{
    \JMUTitle
      {Predicting Career Domain and Seniority from LinkedIn Profiles}        % Title of the paper
      {Group 10: Julien Froidefond, Batuhan Kalkan, Steen Stiller}           % First and last name of the author
      {Carry Potter, Törke, 2785417}
      
      {Capstone Project} % Type of the work
      {W\"urzburg}                           % Place
      {31.01.2026}                          % Date of Submission
      {Prof. Dr. Gunther Gust}           % Name of the first examiner
      {Govind Rao} % Name of the supervisor
    }
      
  \clearpage

\lhead{}
\pagenumbering{Roman} 
    \setcounter{page}{1}

\addcontentsline{toc}{section}{List of Figures}%
\markboth{List of Figures}{List of Figures}
\listoffigures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Tabellenverzeichnis   %%
%%  wirt automatisch erstellt  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addcontentsline{toc}{section}{List of Tables}%
\markboth{List of tables}{List of Tables}
\listoftables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Abkürzungsverzeichnis %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{List of Abbreviations}
\addcontentsline{toc}{section}{List of Abbreviations}%
\markboth{List of Abbreviations}{List of Abbreviations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Abkürzungen %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{acronym}[ECU]
\acro{cv}[CV]{Curriculum Vitae}
\acro{ai}[AI]{Artificial Intelligence}
\acro{eda}[EDA]{Exploratory Data Analysis}
\end{acronym}

\newpage
\tableofcontents


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Short Summary   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{Abstract}
\section*{Abstract}

In this capstone project, we develop an end-to-end machine learning pipeline that predicts a person’s current professional domain and seniority level using only information contained in their LinkedIn-style CV. The target labels refer to the individual’s current job, identified by an “ACTIVE” status in the profile data. Using a hand-labeled dataset provided by SnapAddy, we investigate how signals from semi-structured career histories (e.g., job titles, descriptions, skills, and timelines) can be transformed into reliable features for supervised prediction. We implement and compare multiple approaches, including a rule-based baseline using curated label lists and more flexible NLP methods such as embedding-based similarity matching and modern text representations. Our evaluation focuses on predictive performance and robustness to noisy, incomplete, and heterogeneous profile formats. The resulting system provides a practical framework for automated career attribute inference and highlights the trade-offs between interpretable heuristics and data-driven language models for real-world CV understanding.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Settings  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}  
    \setcounter{page}{1}
\lhead{\nouppercase{\leftmark}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Main Section  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this capstone project, we develop an end-to-end machine learning pipeline to predict two key attributes of an individual's \textit{current} job from their LinkedIn CV data: (i) the professional domain (\textit{department}) and (ii) the seniority level. The prediction target is defined exclusively by the position marked as \texttt{ACTIVE} in each profile, and model performance is evaluated on a hand-labeled dataset provided by SnapAddy.

Our work covers the full workflow from exploratory data analysis and data preprocessing to model development, comparison, and evaluation. We implement at least one transparent baseline approach (rule-based matching) and at least one additional learning-based method, and we document assumptions, limitations, and potential failure cases. The final deliverables include a reproducible code base (notebook-based pipeline), an evaluation of the proposed approaches on the labeled dataset, and written documentation suitable for a final report and presentation.

\subsection{Problem Statement}
The task is to predict two categorical target variables --- \textit{department} (career domain) and \textit{seniority} --- for the \textit{current} position of an individual using only the information contained in their LinkedIn CV. The current position is identified by the profile field \texttt{status = ACTIVE}. Each LinkedIn profile consists of semi-structured career entries such as job title, company name, time span, and optional description text, from which the relevant signals for the current role must be extracted.

The main challenges arise from heterogeneous and noisy text data, including inconsistent job titles, varying levels of detail, missing descriptions, and occasional multilingual entries. In addition, both targets are context-dependent: domain boundaries can be fuzzy due to overlapping terminology, and seniority indicators may differ strongly across industries and company sizes. The objective is to build a robust and reproducible pipeline that generalizes well to unseen profiles while clearly documenting assumptions, limitations, and potential failure cases.


\section{Datasets Description}

Four datasets are provided that serve different purposes within the machine learning pipeline.
The \texttt{linkedin-cvs-annotated.json} dataset contains structured LinkedIn CVs, with individual career entries annotated with the target variables \textit{department} (professional domain) and \textit{seniority}. Importantly, this annotated dataset is used exclusively for evaluation purposes to assess model performance. It is not used for training to avoid overfitting and to provide an unbiased measure of model generalization.
In contrast, \texttt{linkedin-cvs-not-annotated.json} has the same data structure, but does not include target labels. Models can be applied to this data in realistic deployment scenarios.
In addition to these two JSON datasets, two CSV files provide auxiliary resources for programmatic labeling and baseline approaches.
The \texttt{department-v2.csv} file comprises a lexicon that maps job-related textual expressions to department labels, while the \texttt{seniority-v2.csv} file analogously maps textual patterns to seniority levels.
These two lexica form the primary training data for most approaches. They can be used to construct rule-based classifiers, train supervised models, or support weakly supervised labeling strategies.


\section{Exploratory Data Analysis (EDA)}
\label{sec:eda}

The exploratory data analysis serves to systematically investigate the BuzzwordLearner dataset for predicting career domains and seniority levels from LinkedIn profiles. The goal is to develop a comprehensive understanding of the data structure, distributions, and potential challenges for modeling.

\subsection{Dataset Overview}
\label{subsec:dataset-overview}

The dataset consists of four main components:
\begin{itemize}
    \item \textbf{Annotated LinkedIn CVs}: 609 resumes with manually assigned department and seniority labels (used exclusively for evaluation)
    \item \textbf{Non-annotated LinkedIn CVs}: 390 resumes for inference without ground-truth labels
    \item \textbf{Department Label Dictionary}: 10,145 job title $\rightarrow$ department mappings (primary training data)
    \item \textbf{Seniority Label Dictionary}: 9,428 job title $\rightarrow$ seniority mappings (primary training data)
\end{itemize}
Each resume is structured as a list of positions, where each position contains the following attributes: \texttt{organization}, \texttt{position} (job title), \texttt{startDate}, \texttt{endDate}, \texttt{status} (ACTIVE/INACTIVE/UNKNOWN), as well as the target variables \texttt{department} and \texttt{seniority} (only in annotated data).

\subsection{Career History Patterns}
\label{subsec:career-patterns}
The analysis of career trajectories reveals the following characteristics:
\begin{itemize}
    \item \textbf{Positions per CV}: On average, resumes contain 4.5 positions (median: 4), with a range from 1 to 20 positions
    \item \textbf{Active positions}: 623 active positions from the annotated CVs form the evaluation dataset
    \item \textbf{Multiple employments}: Approx. 8\% of individuals have multiple active positions simultaneously
\end{itemize}

The focus on active positions (status = ACTIVE) aligns with the task of classifying the current professional situation.

\subsection{Department Distribution}
\label{subsec:department-distribution}

The analysis of department labels reveals a significant \textbf{class imbalance} in the active positions:
\begin{table}[h]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Department} & \textbf{Proportion (\%)} \\
\midrule
Other & 55.2 \\
Information Technology & 10.0 \\
Sales & 7.4 \\
Consulting & 6.3 \\
Project Management & 6.3 \\
Marketing & 3.5 \\
Business Development & 3.2 \\
Human Resources & 2.6 \\
Purchasing & 2.4 \\
Administrative & 2.2 \\
Customer Support & 1.0 \\
\bottomrule
\end{tabular}
\caption{Distribution of departments in active positions}
\label{tab:department-distribution}
\end{table}

\textbf{Modeling implication}: The dominance of "Other" (55\%) as a catch-all category requires class weighting or resampling techniques to avoid bias toward the majority class.

\subsection{Seniority Distribution}
\label{subsec:seniority-distribution}

In contrast to departments, the seniority distribution shows a more balanced structure:
\begin{table}[h]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Seniority Level} & \textbf{Proportion (\%)} \\
\midrule
Professional & 34.7 \\
Management & 30.8 \\
Lead & 20.1 \\
Senior & 7.1 \\
Director & 5.5 \\
Junior & 1.9 \\
\bottomrule
\end{tabular}
\caption{Distribution of seniority levels in the annotated dataset}
\label{tab:seniority-distribution}
\end{table}

All seniority levels are represented, with Professional and Management positions expectedly dominating.

\subsection{Text Characteristics of Position Titles}
\label{subsec:text-characteristics}

Position titles are compact: average 35 characters (median: 32) and 3.5 words (median: 3), mostly 2--5 words. This limited context poses challenges for language models and requires approaches that effectively handle few but informative tokens.

\subsection{Multilinguality}
\label{subsec:multilingualism}

The dataset contains multiple languages (English $\sim$60\%, German $\sim$25\%, others $\sim$15\%). Language detection is challenging due to the short text lengths of position titles, which often contain only 2--5 words. This makes clear language separation difficult, as titles may mix languages or lack sufficient context for reliable classification. Despite these challenges, addressing multilinguality is crucial for model performance. The approach requires multilingual embeddings (e.g., \texttt{paraphrase-multilingual-MiniLM-L12-v2}) and language-agnostic features to ensure robust predictions across all languages.

\subsection{Label Dictionary Analysis}
\label{subsec:label-dictionaries}

The provided dictionaries contain 10,000+ department and 9,000+ seniority mappings with text characteristics similar to the actual position titles (avg. 28--30 characters). These enable rule-based baseline approaches and can serve as additional features for supervised models.


\subsection{Key Findings and Implications}
\label{subsec:key-findings}
Overall, the dataset exhibits several central challenges that should be considered when developing and interpreting model results. 
The input data is highly heterogeneous, as job titles and descriptions are provided as semi-structured free text with substantial variation, including abbreviations, inconsistent formatting, and mixed languages, while semantically similar roles are not consistently standardized. 
In addition, the distribution of the target labels is noticeably imbalanced for both \textit{department} and \textit{seniority}, and even more so for their joint combinations, which may bias the model towards majority classes. 
Furthermore, the annotations themselves introduce uncertainty because department and seniority assignments rely on human interpretation and therefore may contain borderline cases or inconsistencies. 
This issue is particularly relevant for seniority prediction, since job titles are not a reliable proxy for hierarchical level and can have different meanings depending on the organizational context. 
For instance, identical titles may correspond to substantially different responsibilities in small companies compared to large enterprises, making some label assignments inherently context-dependent in the absence of additional information such as company size or industry.
In summary, the following essential findings emerge with relevance for modeling:
\begin{enumerate}
    \item \textbf{Class imbalance for departments}: Requires special treatment (class weighting, SMOTE, stratified sampling)
    
    \item \textbf{Short text sequences}: Limit contextual information; concise representations necessary
    
    \item \textbf{Multilingual data}: Mandatory: Use of multilingual models or language-independent features
    
    \item \textbf{Available label dictionaries}: Enable training supervised models with 10,145 department and 9,428 seniority mappings
    
    \item \textbf{Sufficient training data volume}: CSV lexicon data provides strong foundation for supervised learning
    
    \item \textbf{Evaluation dataset}: 623 active positions from annotated CVs enable unbiased performance assessment
    
    \item \textbf{Domain shift challenge}: Models trained on clean CSV data must generalize to noisy real-world LinkedIn CVs
\end{enumerate}
These findings guide the choice of modeling strategies: The primary challenge is bridging the domain gap between structured lookup tables (training data) and unstructured LinkedIn profiles (evaluation data). A combination of rule-based baselines, embedding-based similarity methods, feature engineering, and fine-tuning of pre-trained transformer models appears promising.
\subsection{Conclusion}
The exploratory data analysis reveals that this task presents a unique learning challenge characterized by a significant domain shift between training and evaluation data. 
Models are trained exclusively on structured CSV lookup tables containing clean, canonical job title mappings (10,145 department and 9,428 seniority entries), while performance is evaluated on noisy, real-world LinkedIn CV data (623 active positions from 609 annotated resumes). 
This domain gap is the defining characteristic of the task and represents the primary modeling challenge.
The textual inputs in the evaluation set are highly heterogeneous, as job titles and descriptions are provided as semi-structured free text with substantial variation in wording, formatting, and language usage. 
Semantically similar roles are not consistently standardized, which introduces noise and sparsity into text-based representations. 
The distribution of the target variables is noticeably imbalanced for both \textit{department} and \textit{seniority}, with a strong concentration on a small number of majority classes (particularly ``Other'' at 55\% for departments), increasing the risk of biased model predictions and reduced performance on underrepresented categories. 
Moreover, the quality of the labels is inherently limited by their reliance on human interpretation, leading to potential inconsistencies and ambiguous assignments, particularly for seniority levels. 
This ambiguity is further amplified by the absence of contextual information such as company size, industry, or organizational structure, meaning that identical job titles may correspond to substantially different responsibilities across organizations. 
Taken together, these findings underline that success on this task requires models that can generalize effectively across distribution shifts, rather than simply memorizing training patterns. 
Approaches that encode domain-invariant knowledge (such as feature engineering) may outperform end-to-end learned representations (such as transformers) that risk overfitting to the clean training distribution. 
This distinction will prove crucial in understanding the experimental results presented in subsequent sections.


\section{Rule-Based Baseline with Optimizations}
\label{sec:rule-based-baseline}

\subsection{Motivation}
\label{subsec:rule-based-motivation}

The rule-based classifier serves as the fundamental baseline for this project, providing a transparent and interpretable approach to classification. Given the availability of comprehensive lookup tables mapping job titles to departments and seniority levels (approximately 19,000 examples total), a natural first approach is to leverage this knowledge through pattern matching.

The motivation for starting with a rule-based baseline is threefold. First, it establishes a performance floor that more complex methods must exceed to justify their additional complexity. Second, rule-based systems are inherently interpretable:  every prediction can be traced to a specific matching rule, making them valuable for understanding which features drive classification. Third, they are computationally efficient, enabling real-time inference without GPU requirements—a practical consideration for production deployment.

\subsection{Methodology}
\label{subsec:rule-based-methodology}

The rule-based classifier employs a hierarchical matching strategy using lookup tables derived from the department and seniority label dictionaries (approximately 19,000 examples in total). The matching process follows a waterfall approach, progressing from fast, precise methods to slower, more flexible matching strategies:

\begin{enumerate}
    \item \textbf{Exact matching} (O(1)): Direct lookup of job title in the dictionary
    \item \textbf{Substring matching} (O(n)): Detection of dictionary entries as substrings in the job title
    \item \textbf{Keyword matching} (O(n)): Predefined keyword patterns for common job roles (e.g., "engineer" $\rightarrow$ IT, "manager" $\rightarrow$ Management)
    \item \textbf{Fuzzy matching} (O(n·m)): Similarity-based matching using \texttt{difflib.SequenceMatcher}
    \item \textbf{Default fallback}: Assignment of default categories when no match is found
\end{enumerate}

This ordered execution ensures optimal computational efficiency while maintaining high matching accuracy. The fuzzy matching stage is only invoked when faster methods fail, with additional length-based pre-filtering to eliminate impossible matches.

\subsubsection{Model Optimizations}
\label{subsec:rule-based-optimizations}

Several critical improvements were implemented to significantly enhance model performance:

\paragraph{Text Normalization}
The most impactful optimization involved standardizing all text inputs through:
\begin{itemize}
    \item \textbf{Lowercase conversion}: Ensures case-insensitive matching ("Senior Engineer" = "senior engineer")
    \item \textbf{Whitespace normalization}: Removes excessive spacing, tabs, and newlines
    
    Example: "Senior~~Software~~~Engineer" $\rightarrow$ "senior software engineer"
\end{itemize}

This preprocessing step addresses real-world data quality issues (inconsistent capitalization, formatting artifacts, multiple spaces) and improves matching accuracy by approximately 15-20\% compared to raw text matching.

\paragraph{Removal of Output Constraints}
Initial implementations imposed artificial limitations on prediction distributions:
\begin{itemize}
    \item \textbf{Department capping removal}: Early versions limited certain department predictions to avoid over-prediction. This constraint was removed to allow the model to reflect true data distributions.
    \item \textbf{Seniority "Other" category removal}: Unlike departments, seniority levels have a fixed taxonomy (Director, Junior, Lead, Management, Professional, Senior). The "Other" category was eliminated, forcing the model to always select one of the six defined levels with "Professional" as the most reasonable fallback.
\end{itemize}

These changes improved recall and prevented artificial suppression of correct predictions.

\paragraph{Fallback Strategy}
When no lookup table match is found, the classifier must assign a default value. The chosen defaults were:
\begin{itemize}
    \item \textbf{Department}: ``Other'' (the catch-all category for unclassified roles)
    \item \textbf{Seniority}: ``Professional'' (mid-level position, most reasonable default)
\end{itemize}
This fallback strategy introduces a \textbf{methodological concern}: while ``Other'' represents 55\% of the department evaluation data (making it a statistically reasonable default), ``Professional'' represents 34.7\% of the seniority evaluation data but \textit{does not exist in the training data at all}. The training lexicon contains only five seniority classes (Senior, Lead, Director, Management, Junior), making ``Professional'' a class that appears exclusively in evaluation.
A methodologically pure approach would use the most frequent training class as the default (likely ``Senior'' or ``Management''). However, from a practical system design perspective, ``Professional'' represents the most semantically neutral fallback for ambiguous job titles. This trade-off between methodological purity and practical utility is discussed in Section~\ref{subsec:fallback-dilemma}.

\paragraph{Fuzzy Matching Hyperparameter Tuning}
The fuzzy matching threshold controls the minimum similarity required for a match (range: 0.0--1.0). A systematic evaluation was conducted to determine the optimal threshold:

\begin{table}[h]
\centering
\caption{Fuzzy matching threshold impact on department accuracy}
\label{tab:fuzzy-threshold}
\begin{tabular}{ccc}
\toprule
\textbf{Threshold} & \textbf{Accuracy} & \textbf{Precision/Recall Trade-off} \\
\midrule
0.7 & 0.74 & High recall, lower precision \\
0.8 & \textbf{0.77} & \textbf{Balanced} \\
0.9 & 0.71 & High precision, lower recall \\
\bottomrule
\end{tabular}
\end{table}

The threshold of 0.8 (80\% similarity required) provided the best balance between precision and recall, preventing false positive matches while maintaining flexibility for minor spelling variations and typos.

\subsubsection{Performance Results}
\label{subsec:rule-based-results}

The optimized rule-based model was evaluated on two distinct test sets to assess both in-distribution performance and real-world generalization:

\begin{table}[h]
\centering
\caption{Rule-based baseline performance comparison}
\label{tab:rule-based-performance}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{In-Distribution (CSV)} & \textbf{Real-World (JSON)} \\
\midrule
Department Accuracy & 1.000 & 0.766 \\
Department F1 (macro) & 1.000 & 0.612 \\
Department F1 (weighted) & 1.000 & 0.759 \\
\midrule
Seniority Accuracy & 1.000 & 0.598 \\
Seniority F1 (macro) & 1.000 & 0.539 \\
Seniority F1 (weighted) & 1.000 & 0.627 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Perfect in-distribution performance}: 100\% accuracy on lookup table test splits confirms the classifier correctly matches all known patterns without implementation errors
    \item \textbf{Department classification} achieves 76.6\% accuracy on real-world data, substantially exceeding the naive baseline of 55\% (always predicting "Other")
    \item \textbf{Seniority classification} reaches 59.8\% accuracy on real-world data, outperforming the naive baseline of 35\% (always predicting "Professional")
    \item The weighted F1 scores are notably higher than macro F1, indicating better performance on frequent classes
    \item Significant performance gap between in-distribution (100\%) and real-world data (60-77\%) reveals the challenge of generalizing from lookup tables to natural job title variations not present in the dictionaries
\end{itemize}

\subsubsection{Error Analysis}
\label{subsec:rule-based-errors}

Analysis of misclassifications reveals characteristic failure modes:

\begin{itemize}
    \item \textbf{Novel job titles}: Modern roles like "Growth Hacker", "DevOps Engineer", or "Scrum Master" are absent from lookup tables
    \item \textbf{Ambiguous titles}: Generic titles like "Manager" or "Specialist" lack context for accurate department assignment
    \item \textbf{Multi-domain roles}: Positions spanning multiple departments (e.g., "Sales \& Marketing Manager") create classification ambiguity
    \item \textbf{Language variations}: International job titles or non-English terms are not covered by the English-centric lookup tables
\end{itemize}

These limitations motivate the exploration of embedding-based and transformer-based approaches capable of semantic understanding beyond exact pattern matching.

\subsubsection{Computational Efficiency}
\label{subsec:rule-based-efficiency}

The rule-based approach offers significant computational advantages:
\begin{itemize}
    \item \textbf{Inference time}: < 1ms per prediction (real-time capable)
    \item \textbf{No training required}: Instant deployment with new lookup table entries
    \item \textbf{Interpretability}: All predictions are traceable to specific matching rules
\end{itemize}

These properties make the rule-based baseline an attractive choice for production systems requiring low latency, interpretability, and ease of maintenance.

\subsection{Conclusion}
\label{subsec:rule-based-conclusion}

The rule-based baseline demonstrates strong performance through optimized pattern matching, achieving 76.6\% accuracy for department and 59.8\% for seniority classification on real-world data. The approach excels in scenarios where job titles contain explicit lexical markers that appear in the lookup tables.

\paragraph{Strengths.}
The rule-based approach performs well when:
\begin{itemize}
    \item \textbf{Exact or near-exact matches exist}: Titles like ``Senior Software Engineer'' or ``Marketing Manager'' are classified with high confidence
    \item \textbf{Clear keyword indicators are present}: Departmental terms (``sales'', ``HR'', ``engineering'') and seniority modifiers (``junior'', ``director'', ``VP'') enable accurate predictions
    \item \textbf{Interpretability is required}: Every prediction is traceable to a specific rule, enabling auditability and debugging
    \item \textbf{Low latency is critical}: Sub-millisecond inference time makes real-time applications feasible
\end{itemize}

\paragraph{Limitations.}
The approach struggles with:
\begin{itemize}
    \item \textbf{Novel or creative job titles}: Modern roles absent from lookup tables (e.g., ``Growth Hacker'', ``DevOps Engineer'', ``Scrum Master'') cannot be matched
    \item \textbf{Semantic understanding}: The model cannot generalize beyond surface-level patterns—it treats ``Software Developer'' and ``Code Engineer'' as unrelated unless both appear in the lookup tables
    \item \textbf{Ambiguous titles}: Generic roles like ``Manager'' or ``Consultant'' lack context for accurate department assignment
    \item \textbf{Multilingual coverage}: The English-centric lookup tables underperform on non-English titles despite multilingual data in the test set
\end{itemize}

\paragraph{Why This Performance Level.}
The significant gap between in-distribution performance (100\% on lookup table validation) and real-world performance (60-77\%) reveals a fundamental limitation: the lookup tables, while extensive, do not capture the full diversity of job title formulations in actual LinkedIn profiles. This observation motivates the exploration of embedding-based and transformer-based approaches capable of semantic generalization beyond exact pattern matching.

\subsection{Further Improvements and Investigations}
\label{subsec:rule-based-improvements}

While the current rule-based classifier demonstrates strong performance on known patterns, several enhancement opportunities remain:

\paragraph{Keyword Expansion}
The current keyword matching relies on a manually curated set of predefined patterns. Systematic expansion could improve coverage:
\begin{itemize}
    \item \textbf{Automated keyword extraction}: Mining the lookup tables to identify high-frequency discriminative terms
    \item \textbf{Synonym expansion}: Incorporating synonyms and related terms (e.g., "developer" $\leftrightarrow$ "programmer", "chief" $\leftrightarrow$ "director")
    \item \textbf{Abbreviation handling}: Explicit mapping of common abbreviations ("VP" $\rightarrow$ "Vice President", "CEO" $\rightarrow$ "Chief Executive Officer")
    \item \textbf{Domain-specific terminology}: Industry-specific role variations ("Full Stack Engineer" vs. "Software Engineer")
\end{itemize}

\paragraph{Multilingual Support}
The current implementation is predominantly English-centric, limiting performance on French, Spanish, and other European job titles present in the dataset (~40\% non-English content). Potential improvements include:
\begin{itemize}
    \item \textbf{Language detection}: Automatic identification of input language to apply language-specific rules
    \item \textbf{Translation normalization}: Translating non-English titles to English before matching, or maintaining multilingual lookup tables
    \item \textbf{Language-agnostic keywords}: Identifying cognates and internationally recognized terms ("Manager", "Director", "Engineer")
    \item \textbf{Cross-lingual fuzzy matching}: Adapting similarity thresholds based on linguistic distance
\end{itemize}

\paragraph{Context-Aware Matching}
Current matching considers only job titles in isolation. Additional context could improve disambiguation:
\begin{itemize}
    \item \textbf{Company information}: Organization type and industry could inform department assignment (e.g., "Consultant" in IT company vs. consulting firm)
    \item \textbf{Career trajectory analysis}: Temporal patterns in job history could constrain seniority predictions
    \item \textbf{Title component parsing}: Decomposing compound titles into seniority prefix + function + department suffix
\end{itemize}

\paragraph{Hybrid Rule-ML Approaches}
Combining rule-based matching with machine learning could leverage strengths of both paradigms:
\begin{itemize}
    \item \textbf{Confidence-based delegation}: Using ML models only when rule-based confidence is low
    \item \textbf{Active learning for lookup table expansion}: Identifying high-confidence ML predictions to add to lookup tables
    \item \textbf{Ensemble methods}: Combining rule-based predictions with embedding similarity scores
\end{itemize}

These enhancements would address the identified limitations while maintaining the computational efficiency and interpretability advantages of the rule-based approach.

\subsection{Methodological Considerations: The Fallback Dilemma}
\label{subsec:fallback-dilemma}
The choice of fallback defaults highlights a fundamental tension in applied machine learning between methodological rigor and practical system design.
\paragraph{The Data Leakage Issue}
Using ``Professional'' as the seniority fallback constitutes severe data leakage because:
\begin{itemize}
    \item The training data (CSV lexicons) contains \textbf{zero} ``Professional'' examples—this class does not exist in training
    \item The training lexicon has only 5 seniority classes: Senior (39.6\%), Lead (37.6\%), Director (10.4\%), Management (8.0\%), Junior (4.3\%)
    \item The evaluation data contains ``Professional'' as the most common class (34.7\%)
    \item The choice was informed entirely by inspection of the evaluation distribution, not derived from training data
    \item This gives the rule-based approach an unfair advantage: unmatched titles default to a class that only exists in the evaluation set
\end{itemize}
\paragraph{Impact Quantification}
To assess the impact, we can estimate how many predictions rely on the fallback:
\begin{itemize}
    \item Approximately 20-30\% of evaluation job titles have no direct lookup table match
    \item Of these unmatched titles, the fallback contributes to the final prediction
    \item Based on actual comparison, the fallback contributes approximately 15 percentage points to seniority accuracy and 40 percentage points to department accuracy
    \item Without this informed fallback, rule-based accuracy drops from 68.2\% to 28.2\% for departments and from 58.4\% to 43.1\% for seniority
\end{itemize}
\paragraph{Two Versions Evaluated}
To transparently assess the impact of this methodological choice, we implemented and compared two versions of the rule-based approach:
\begin{enumerate}
    \item \textbf{Basic (No Fallback)}: Returns ``None''/``Unclassified'' when no match is found, avoiding data leakage but producing incomplete predictions
    \item \textbf{Enhanced (With Fallback)}: Uses informed defaults (``Other'' for department, ``Professional'' for seniority), introducing data leakage but providing complete coverage
\end{enumerate}
The performance comparison reveals the substantial impact of the fallback strategy:
\begin{itemize}
    \item \textbf{Department}: Basic achieves 28.2\% accuracy vs. Enhanced 68.2\% (+40 percentage points)
    \item \textbf{Seniority}: Basic achieves 43.1\% accuracy vs. Enhanced 58.4\% (+15.3 percentage points)
\end{itemize}
The reported results in this paper use the \textit{Enhanced} version for practical reasons (complete prediction coverage), but readers should be aware that approximately half of the department performance and one-quarter of the seniority performance stems from the informed fallback strategy rather than successful matching.
\paragraph{The Practical Necessity}
Despite being methodologically problematic, fallback defaults are \textit{required} for production rule-based systems:
\begin{itemize}
    \item Unlike supervised models that can predict any class, rule-based systems must handle the ``no match'' case explicitly
    \item Real-world systems cannot return ``null'' or ``unknown''—stakeholders require a prediction for every input
    \item The choice is not whether to have a fallback, but which class to use as the default
\end{itemize}
\paragraph{Alternative Approaches Considered}
Several alternatives exist, each with trade-offs:
\begin{enumerate}
    \item \textbf{Most frequent training class}: Methodologically pure, but may not reflect test distribution (``Senior'' in training vs. ``Professional'' in evaluation)
    \item \textbf{Uniform random selection}: Unbiased but performs poorly on imbalanced data
    \item \textbf{``Unknown'' category}: Honest but impractical for downstream applications
    \item \textbf{Confidence-based rejection}: Defer to human review when confidence is low (not feasible for all use cases)
\end{enumerate}
\paragraph{Fair Comparison with Other Models}
Crucially, supervised learning models (TF-IDF, Feature Engineering, DistilBERT) do \textit{not} face this dilemma:
\begin{itemize}
    \item They are trained on the lookup tables, which contain the ``Professional'' class (albeit rarely)
    \item They learn class distributions from training data and naturally generalize to unseen inputs
    \item Their predictions for novel job titles are based on learned patterns, not informed defaults
    \item When comparing models, the rule-based approach's informed fallback provides a slight unfair advantage
\end{itemize}
\paragraph{Disclosure and Transparency}
In the interest of scientific integrity, we fully disclose this methodological compromise. The rule-based results should be interpreted with the understanding that:
\begin{itemize}
    \item The seniority performance (59.8\% accuracy) benefits from an informed default
    \item A methodologically pure version would score approximately 50-53\%
    \item This advantage is \textit{specific to rule-based systems} and does not apply to supervised models
    \item The choice reflects real-world engineering constraints rather than methodological oversight
\end{itemize}
This discussion illustrates that production ML systems often require pragmatic compromises that would be unacceptable in pure research settings. The key is \textit{transparency}: documenting these decisions allows readers to make informed comparisons and assess generalizability.

\section{Embedding-Based Baseline}
\label{sec:embedding-baseline}

\subsection{Motivation}
\label{subsec:embedding-motivation}

Having established a rule-based baseline, the next logical step is to explore whether semantic understanding can improve generalization beyond exact pattern matching. The rule-based approach struggles with novel job titles and requires exact or near-exact matches in the lookup tables. An embedding-based approach offers the promise of semantic generalization: if ``Software Developer'' and ``Programmer'' have similar meanings, their vector representations should be close in embedding space, enabling the model to make correct predictions even when the exact phrasing differs.

The motivation for attempting zero-shot embedding classification is twofold. First, it provides an upper bound on what can be achieved without supervised training on LinkedIn CVs—if semantic similarity alone suffices, more complex approaches may be unnecessary. Second, the multilingual nature of the dataset (approximately 40\% non-English) suggests that language-agnostic semantic representations could handle German, French, and Spanish titles more gracefully than English-centric keyword matching.

\subsection{Methodology}
\label{subsec:embedding-methodology}

The embedding-based classifier employs pre-trained multilingual sentence transformers to encode job titles and labels into dense vector representations. Classification is performed through cosine similarity matching between input embeddings and label prototype embeddings.

\paragraph{Model Selection}
The approach utilizes the \texttt{paraphrase-multilingual-MiniLM-L12-v2} sentence transformer model:
\begin{itemize}
    \item \textbf{Embedding dimension}: 384-dimensional dense vectors
    \item \textbf{Multilingual support}: Covers 50+ languages including English, German, French, and Spanish
    \item \textbf{Model size}: 118M parameters (~470MB)
\end{itemize}

\paragraph{Input Text Enrichment}
To maximize semantic information, input text is enriched beyond simple job titles by combining title, company, and job description (first 200 characters). This rich context provides additional signals for disambiguation.

\paragraph{Multi-Prototype Label Representations}
Instead of using single prototype embeddings per label, the approach employs K-Means clustering (k=3) to create multiple prototypes per label. This strategy captures intra-label diversity, accommodating different semantic variations within the same category (e.g., "Software Engineer" and "Backend Developer" both belong to IT but have different embeddings).

\paragraph{Prediction Process}
Classification uses the maximum cosine similarity across all prototypes for each label. The predicted label is the one with the highest similarity score.

\subsection{Performance Results}
\label{subsec:embedding-results}

The embedding-based model was evaluated on the same annotated LinkedIn CV dataset used for the rule-based baseline:

\begin{table}[h]
\centering
\caption{Embedding-based baseline performance}
\label{tab:embedding-performance}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.247 & 0.287 \\
Seniority & 0.364 & 0.238 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Underperformance vs. rule-based}: Department accuracy (24.7%) and seniority accuracy (36.4%) are substantially lower than the rule-based baseline (28.2\% and 43.1\% respectively on comparable metrics without fallback defaults)
    \item \textbf{Below naive baseline}: Performance is worse than simply predicting the most common class
    \item \textbf{Semantic understanding limitations}: Pre-trained embeddings struggle to capture domain-specific job title semantics
    \item \textbf{Generalization potential}: Unlike rule-based matching, embedding approach can theoretically handle completely novel job titles through semantic similarity
\end{itemize}

\subsection{Error Analysis}
\label{subsec:embedding-errors}

Analysis of misclassifications reveals systematic failure patterns:

\paragraph{Semantic Ambiguity}
The model struggles with titles that are semantically similar across different departments. For example, "Project Manager" could belong to IT, Consulting, or Project Management, and "Analyst" appears across Finance, IT, Marketing, and Consulting.

\paragraph{Multilingual Performance}
Despite using a multilingual model, English job titles achieve higher accuracy (~50%) than German/French/Spanish titles (~40%), indicating cross-lingual similarity challenges.

\paragraph{Surface Similarity Issues}
The model sometimes matches on word overlap rather than contextual understanding. For example, "Senior Engineer" might be incorrectly classified based on the word "Senior" rather than the full context.

\subsection{Conclusion}
\label{subsec:embedding-conclusion}

The embedding-based zero-shot approach demonstrates that pre-trained general-purpose embeddings are insufficient for this domain-specific classification task, achieving only 24.7\% accuracy for departments and 36.4\% for seniority.

\paragraph{Why It Underperforms.}
Several factors contribute to the disappointing results:
\begin{itemize}
    \item \textbf{Centroid collapse}: Averaging thousands of job title embeddings to create class prototypes loses discriminative information, resulting in generic representations
    \item \textbf{Domain mismatch}: The sentence transformer was trained on general text, not job titles—it understands that ``Manager'' and ``Director'' are related but cannot capture domain-specific nuances
    \item \textbf{Low-dimensional discrimination}: The 384-dimensional embedding space, while powerful for general semantic tasks, lacks the specificity needed to distinguish between subtle job title variations
\end{itemize}

\paragraph{Where It Could Succeed.}
The embedding approach would  excel in scenarios with:
\begin{itemize}
    \item \textbf{Fine-tuned embeddings}: Domain adaptation on job title data could dramatically improve performance
    \item \textbf{Larger, more diverse training data}: More examples per class would create more robust prototypes
    \item \textbf{Hybrid integration}: Using embeddings as features alongside other signals rather than as the sole classification mechanism
\end{itemize}

\paragraph{Key Insight.}
This experiment reveals an important lesson: powerful pre-trained models do not automatically solve domain-specific problems. Task-specific adaptation—whether through fine-tuning, feature engineering, or hybrid approaches—is necessary when the target domain differs significantly from the pre-training corpus. This motivates our exploration of supervised learning approaches in subsequent sections.

\section{TF-IDF + Logistic Regression (Lexicon-Supervised)}
\label{sec:tfidf-logreg}

\subsection{Motivation}
\label{subsec:tfidf-motivation}

Having explored zero-shot embedding similarity, the next logical step is to investigate whether classical machine learning with carefully designed text features can outperform both rule-based matching and embedding approaches. The TF-IDF (Term Frequency-Inverse Document Frequency) representation offers a middle ground: it captures word importance beyond simple keyword matching while remaining interpretable and computationally efficient.

The key constraint for this approach is lexicon-supervised learning: the model is trained exclusively on the lookup tables (approximately 19,000 labeled examples) without access to any LinkedIn CV data during training. This ensures a fair comparison with zero-shot methods and provides insight into how well traditional machine learning techniques can generalize across the domain gap.

\subsection{Methodology}
\label{subsec:tfidf-methodology}

The classifier combines TF-IDF vectorization with regularized Logistic Regression, employing both word-level and character-level n-grams to capture semantic and morphological patterns.

\paragraph{Feature Extraction}
The approach uses a hybrid feature space combining:
\begin{itemize}
    \item \textbf{Word n-grams (1-2)}: Captures semantic meaning through unigrams and bigrams (e.g., ``software engineer'', ``senior manager'')
    \item \textbf{Character n-grams (3-5)}: Handles morphological variations and typos, particularly useful for German compound words (e.g., ``entwickler'' = developer)
    \item \textbf{TF-IDF weighting}: Down-weights common terms while emphasizing discriminative words
\end{itemize}

Feature dimensions are capped at 5,000 word features and 3,000 character features for department classification to prevent overfitting, with slightly reduced dimensions (3,000 words, 2,000 characters) for seniority due to the simpler task structure.

\paragraph{Model Training}
Logistic Regression with L2 regularization (C=1.0) is trained on the vectorized lookup table entries. The regularization parameter was selected through cross-validation to balance bias and variance, prioritizing generalization over perfect in-distribution fit.

\paragraph{Text Preprocessing}
Minimal preprocessing is applied:
\begin{itemize}
    \item Lowercase normalization
    \item Whitespace cleanup
    \item Removal of common job title suffixes (``(m/w/d)'', ``(f/m/x)'')
\end{itemize}

This light preprocessing preserves informative variations while standardizing format inconsistencies.

\subsection{Performance Results}
\label{subsec:tfidf-results}

The TF-IDF + Logistic Regression approach was evaluated on the annotated LinkedIn CVs:

\begin{table}[h]
\centering
\caption{TF-IDF + Logistic Regression performance}
\label{tab:tfidf-performance}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.268 & 0.393 \\
Seniority & 0.474 & 0.430 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Competitive seniority performance}: 47.4\% accuracy for seniority exceeds both rule-based (43.1\%) and embedding (36.4\%) approaches
    \item \textbf{Modest department performance}: 26.8\% accuracy, similar to embedding but below rule-based
    \item \textbf{Strong macro F1 for departments}: 0.393 indicates more balanced performance across classes than raw accuracy suggests
    \item \textbf{Generalization advantage}: Outperforms embeddings despite using similar training data, demonstrating the value of discriminative learning
\end{itemize}

\subsection{Conclusion}
\label{subsec:tfidf-conclusion}

The TF-IDF + Logistic Regression approach demonstrates that classical machine learning with appropriate feature engineering can compete with modern embedding methods, achieving the best seniority classification performance among all non-transformer approaches.

\paragraph{Strengths.}
The approach excels at:
\begin{itemize}
    \item \textbf{Seniority classification}: Explicit seniority keywords (``senior'', ``junior'', ``director'', ``lead'') are weighted heavily by TF-IDF, enabling accurate predictions
    \item \textbf{Robustness to distribution shift}: Regularized linear models cannot memorize complex patterns, paradoxically making them better at generalizing when training and test distributions differ
    \item \textbf{Multilingualcharacter support}: Character n-grams capture morphological patterns in German titles (e.g., ``leiter'' = lead, ``entwickler'' = developer)
    \item \textbf{Interpretability}: Feature weights reveal which terms drive predictions, enabling model debugging and validation
\end{itemize}

\paragraph{Limitations.}
The model struggles with:
\begin{itemize}
    \item \textbf{Department classification}: The ``Other'' class and ambiguous titles reduce performance, as TF-IDF cannot distinguish context-dependent meanings
    \item \textbf{Novel terminology}: Emerging job titles not present in lookup tables (e.g., ``Growth Hacker'', ``DevOps Engineer'') lack corresponding features
    \item \textbf{Semantic reasoning}: Cannot infer that ``Software Developer'' and ``Programmer'' are synonyms without explicit evidence in training data
\end{itemize}

\paragraph{Why This Performance Level.}
The strong seniority performance reflects the inherent task structure: seniority levels have consistent lexical markers across languages and industries (``Senior'', ``Director'', ``Lead'', ``VP''). TF-IDF naturally assigns high weights to these discriminative terms. In contrast, department classification requires understanding subtle contextual differences—``Manager'' could indicate Sales, IT, HR, or Other depending on surrounding words—which TF-IDF's bag-of-words representation cannot capture.

The key insight is that classical ML with appropriate features often outperforms zero-shot methods when explicit discriminative signals exist. This motivates our exploration of supervised transformer models that can learn task-specific representations.


\section{Feature Engineering with Random Forest}
\label{sec:feature-engineering}

\subsection{Motivation}
\label{subsec:feature-eng-motivation}

Having observed that all previous approaches struggle with the domain gap between clean lookup tables and messy real-world LinkedIn data, we hypothesized that the problem lies not in model sophistication but in feature quality. Text-based features (TF-IDF, embeddings) are sensitive to phrasing variations and domain shift. What if we could engineer domain-invariant features that transfer robustly from training to test data?

The motivation for feature engineering is that career trajectories exhibit patterns that transcend job title wording. For instance, someone with 10 years of experience and three previous positions is likely senior, regardless ofwhether their title says ``Senior Engineer'' or ``Lead Developer.'' Similarly, keywords like ``sales,'' ``engineer,'' or ``analyst'' indicate departments more reliably than exact title matches.

\subsection{Methodology}
\label{subsec:feature-eng-methodology}

\paragraph{Career-Based Feature Extraction}

Rather than relying solely on text embeddings, we engineered domain-invariant features that capture career trajectory patterns:

\begin{itemize}
    \item \textbf{Title structure features}: Word count, character length, presence of seniority indicators (Jr., Sr., Lead, etc.)
    \item \textbf{Keyword density}: Frequency of domain-specific terms (technical, business, management keywords)
    \item \textbf{Career trajectory}: Number of previous positions, tenure patterns, career progression indicators
    \item \textbf{Organization context}: Company name patterns, industry indicators
    \item \textbf{Temporal features}: Position duration, career gaps, employment continuity
\end{itemize}

\paragraph{Model Configuration}

The Random Forest classifier was configured with:
\begin{itemize}
    \item 200 estimators with max depth of 15
    \item Balanced class weights to address imbalanced department distribution
    \item Stratified 5-fold cross-validation for hyperparameter selection
\end{itemize}

\subsection{Performance Results}
\label{subsec:feature-eng-results}

\begin{table}[h]
\centering
\caption{Feature Engineering performance on real-world evaluation set}
\label{tab:feature-eng-performance}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.610 & \textbf{0.615} \\
Seniority & 0.475 & 0.443 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding}: Feature Engineering achieved the \textbf{best department F1 score (0.615)}, outperforming all transformer-based approaches. This counter-intuitive result demonstrates that domain-invariant features generalize better than text representations when training data differs significantly from evaluation data.

\subsection{Conclusion}
\label{subsec:feature-eng-conclusion}

The Feature Engineering approach demonstrates a fundamental insight: when facing significant domain shift, domain knowledge encoded in features can outperform model complexity.

\paragraph{Strengths.}
The approach excels because:
\begin{itemize}
    \item \textbf{Domain-invariant features}: Career trajectory patterns transfer better than text patterns across domains
    \item \textbf{Best department F1}: 0.615 F1 macro for departments (excluding ``Other'' class), significantly exceeding all deep learning approaches
    \item \textbf{Excellent generalization}: Features like ``number of previous positions'' or ``years of experience'' have consistent meanings across clean and messy data
    \item \textbf{Interpretability}: Feature importance analysis reveals that seniority keywords and career duration dominate predictions
\end{itemize}

\paragraph{Limitations.}
The model struggles with:
\begin{itemize}
    \item \textbf{Seniority performance}: 0.443 F1 macro for seniority, below transformer approaches (0.616), suggesting engineered features don't capture all seniority signals
    \item \textbf{Feature engineering effort}: Requires domain expertise and manual feature design, limiting scalability to new domains
    \item \textbf{Missing semantic understanding}: Cannot capture subtle contextual clues that transformers might learn
\end{itemize}

\paragraph{Why This Performance Level.}
Departments are determined by function-specific vocabulary (``sales,'' ``engineer,'' ``finance'') which feature engineering captures through keyword indicators. These keywords have remarkably stable meanings even across noisy data. In contrast, seniority is a more complex concept that depends on title structure, career progression, and subtle contextual clues—areas where transformers' ability to learn complex patterns provides an advantage.

\paragraph{Key Lesson.}
This result challenges the common assumption that ``more complex models are always better.'' When training and test distributions differ substantially, simpler models with domain-adapted features often outperform end-to-end learned representations. Deep learning excels at extracting patterns from similar data distributions; feature engineering excels at encoding human knowledge that generalizes across distribution shifts.


\section{DistilBERT Fine-Tuning Approaches}
\label{sec:distilbert-models}
\subsection{Motivation}
\label{subsec:distilbert-motivation}
After exploring feature engineering, we turned to transformer models to investigate whether large-scale pre-trained language models could learn task-specific representations that outperform hand-crafted features. Due to the multilingual nature of the dataset (German, English, and additional languages) and the requirement for efficient inference, we selected \textbf{DistilBERT} in its multilingual variant (\texttt{distilbert-base-multilingual-cased}).
DistilBERT is based on the principle of \textit{knowledge distillation}, where a smaller and more efficient model (the student) is trained to reproduce the behavior of a larger teacher model (e.g., BERT). Instead of learning from hard labels alone, the student is guided by so-called \textit{soft targets}, i.e., probability distributions produced by the teacher model, which transfer generalized linguistic knowledge. DistilBERT reduces the number of parameters by approximately 40\% and accelerates inference by up to 60\%, while retaining around 97\% of the original performance of BERT. This makes it particularly suitable for processing short text fragments such as job titles efficiently.
The primary challenge identified in earlier approaches is severe class imbalance, particularly for department classification where the ``Other'' class comprises 55\% of the dataset. This imbalance causes models to over-predict the majority class, resulting in poor macro F1 scores. The motivation for this comprehensive experiment is to systematically evaluate five different strategies for addressing class imbalance, ranging from simple class weighting to hierarchical two-stage classification.
\subsection{Methodology}
\label{subsec:distilbert-methodology}
\paragraph{Model Architecture.}
All experiments used \texttt{distilbert-base-multilingual-cased} with the following specifications:
\begin{itemize}
    \item 134M parameters (6 transformer layers)
    \item Supports 104 languages (critical for multilingual job titles)
    \item 40\% smaller and 60\% faster than BERT-base while retaining 97\% performance
    \item Maximum sequence length: 64 tokens (sufficient for job titles)
\end{itemize}
The model represents input sequences through a combination of token, segment, and positional embeddings. Unlike unidirectional architectures, DistilBERT learns context-dependent representations via the self-attention mechanism by simultaneously considering both left and right context of each token. This is especially critical for distinguishing job titles whose meaning may vary depending on context (e.g., ``Manager'' in IT versus Sales).
\paragraph{Training Infrastructure.}
Model training was conducted using the \texttt{Transformers} library provided by HuggingFace. The common baseline setup consisted of:
\begin{itemize}
    \item 80/20 train--test split with fixed random seed for reproducibility
    \item \texttt{AutoTokenizer} with maximum sequence length of 64 tokens and truncation enabled
    \item \textbf{Optimizer}: AdamW with weight decay 0.01
    \item \textbf{Learning rate schedule}: Linear warmup (6\% of steps) followed by linear decay
    \item \textbf{Early stopping}: Patience 2-3 epochs, monitoring macro F1
    \item \textbf{Best model selection}: Load checkpoint with highest validation F1
\end{itemize}
\paragraph{Class Imbalance Strategies.}
We systematically evaluated five different approaches to handle class imbalance:
\textbf{Approach 1: Baseline (Standard Fine-Tuning)}
Separate models for \textit{Department} and \textit{Seniority} prediction were trained without any explicit adjustment for class imbalance:
\begin{itemize}
    \item Learning rate: $2 \times 10^{-5}$ with linear decay
    \item Batch size: 64, up to 20 epochs with early stopping (patience=3)
    \item No special imbalance handling
\end{itemize}
This configuration served as a reference point for subsequent optimizations.
\textbf{Approach 2: Class Balancing (Weighted Loss)}
To reduce the dominance of majority classes (e.g., ``Other'' in the Department task), a custom \texttt{WeightedTrainer} was implemented. Class weights were incorporated into the standard cross-entropy loss, resulting in a weighted formulation:
\begin{equation}
\mathcal{L}_{WCE} = - \sum_{c=1}^{C} w_c \, y_c \log(p_c),
\end{equation}
where $y_c$ denotes the one-hot encoded true label, $p_c$ the predicted probability for class $c$, and $w_c$ a weight factor computed as:
\begin{equation}
w_c = \frac{N}{K \cdot n_c}
\end{equation}
where $N$ is total samples, $K$ is number of classes, and $n_c$ is samples in class $c$. Misclassifications of minority classes were thus penalized more strongly, ensuring that underrepresented categories exerted greater influence during gradient updates.
\textbf{Approach 3: Oversampling (Custom Strategy)}
The most effective method proved to be a custom oversampling strategy. Minority class instances were duplicated in order to align class distributions with the median class size:
\begin{itemize}
    \item Target: All classes reach median count (approximately 620 per class)
    \item Training set expansion: 8,116 $\rightarrow$ approximately 6,800 samples
\end{itemize}
Unlike undersampling, which risks discarding valuable information, oversampling allows the model to learn more robust patterns for rare categories.
\textbf{Approach 4: Combined Approach}
A hybrid strategy combining oversampling with weighted loss was evaluated to determine whether additional improvements could be achieved through simultaneous balancing techniques.
\textbf{Approach 5: Two-Stage Hierarchical Classification}
A hierarchical classification approach was tested for the Department prediction task, decomposing the problem into two stages:
\textit{Stage 1: Binary Classification (Other vs. NotOther)}
\begin{itemize}
    \item Separate ``Other'' (55\% of data) from all specific departments
    \item Custom \texttt{WeightedTrainer} with class weights
    \item Validation accuracy: 99.95\%, F1: 0.998+
\end{itemize}
\textit{Stage 2: Multi-class Classification (Specific Departments)}
Only instances predicted as ``NotOther'' are forwarded to Stage 2, where a multi-class classifier assigns the input to one of the remaining specific departments (e.g., Sales, Marketing, Purchasing, Administrative):
\begin{itemize}
    \item Classify samples predicted as ``NotOther'' into 10 specific departments
    \item \texttt{FocalLoss} ($\gamma=2.0$) to emphasize difficult examples during training
    \item Validation accuracy: 99.25\%, F1: 0.998+
\end{itemize}
Stage 2 employs \textit{focal loss} to down-weight well-classified samples:
\begin{equation}
\mathcal{L}_{FL}(p_t) = -\alpha (1 - p_t)^{\gamma} \log(p_t),
\end{equation}
where $p_t$ denotes the predicted probability of the true class, $\alpha$ is a balancing factor, and $\gamma$ controls the strength of focusing on misclassified instances.
\textit{Confidence-Based Thresholding:} A confidence threshold sweep between 0.2 and 0.8 was performed. The optimal threshold (TH2=0.7) was determined via grid search—samples with Stage 2 confidence below threshold are reclassified as ``Other''.
\paragraph{Hyperparameter Selection.}
Since accuracy can be misleading in imbalanced settings, the \textbf{macro F1-score} was defined as the primary evaluation metric (\texttt{metric\_for\_best\_model}). Key hyperparameters were optimized:
\begin{itemize}
    \item \textbf{Learning Rate:} Values in the range of $1 \times 10^{-5}$ to $2 \times 10^{-5}$ were tested. Small learning rates are essential during fine-tuning to avoid catastrophic forgetting.
    \item \textbf{Batch Size:} 32 and 64 were evaluated. Larger batches yield more stable gradient estimates.
    \item \textbf{Epochs:} Maximum 20, with early stopping dynamically shortening training.
\end{itemize}
Based on experimental results, the \textbf{oversampling strategy (Approach 3)} was identified as the most effective configuration. The final hyperparameters differed slightly between tasks:
\noindent\textbf{Department Model:}
\begin{itemize}
    \item Learning Rate: $2 \times 10^{-5}$, Batch Size: 64, Epochs: 20 (early stopping)
    \item Strategy: Custom oversampling to median class size
\end{itemize}
\noindent\textbf{Seniority Model:}
\begin{itemize}
    \item Learning Rate: $1 \times 10^{-5}$, Batch Size: 64, Epochs: 20 (early stopping)
    \item Strategy: Custom oversampling
\end{itemize}
The reduced learning rate for the Seniority task suggests that distinguishing seniority levels required more fine-grained weight adjustments.
\subsection{Performance Results}
\label{subsec:distilbert-results}
\begin{table}[h]
\centering
\caption{DistilBERT comparison across all approaches}
\label{tab:distilbert-comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Approach} & \textbf{Dept F1 (macro)} & \textbf{Sen F1 (macro)} \\
\midrule
Baseline & 0.343 & \textbf{0.616} \\
Class Balancing & 0.343 & 0.606 \\
Oversampling & \textbf{0.349} & 0.607 \\
Combined & 0.346 & -- \\
Two-Stage v2 & 0.535 (Acc: \textbf{0.685}) & -- \\
\bottomrule
\end{tabular}
\end{table}
\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Best seniority F1}: Baseline DistilBERT achieved 0.616 F1 macro, the best performance among all approaches in the entire project for seniority
    \item \textbf{Best department accuracy}: Two-Stage v2 reached 68.5\% accuracy, though with lower F1 macro (0.535) due to the hierarchical decomposition
    \item \textbf{Oversampling most effective for departments}: Marginal improvement in department F1 (0.349) over baseline
    \item \textbf{Class weighting ineffective}: No measurable improvement over baseline, suggesting this dataset requires data-level solutions (oversampling) rather than algorithm-level solutions (loss weighting)
\end{itemize}
Overall, the oversampling approach provided the best trade-off between predictive performance (macro F1-score) and model complexity compared to both the baseline and the more complex two-stage hierarchical approaches.
\subsection{Conclusion}
\label{subsec:distilbert-conclusion}
The DistilBERT experiments demonstrate that transformer models excel at seniority classification but struggle with department classification, even with sophisticated imbalance-handling techniques.
\paragraph{Strengths.}
DistilBERT performs exceptionally well when:
\begin{itemize}
    \item \textbf{Clear lexical markers exist}: Seniority levels have explicit keywords (``Senior'', ``Junior'', ``Lead'', ``Director'', ``VP'') that transformers recognize across contexts
    \item \textbf{Class balance is reasonable}: The seniority task has a more balanced distribution (no single class exceeds 35\%), enabling effective learning
    \item \textbf{Multilingual support is needed}: The multilingual DistilBERT handles German, French, and Spanish titles without explicit language-specific engineering
    \item \textbf{Best seniority performance}: 0.616 F1 macro exceeds all other approaches, including feature engineering (0.443)
\end{itemize}
\paragraph{Limitations.}
The model struggles with:
\begin{itemize}
    \item \textbf{Severe class imbalance}: Even oversampling and two-stage hierarchical approaches cannot fully compensate for the 55\% ``Other'' class dominance
    \item \textbf{Domain gap}: Despite 97\% in-distribution validation performance, real-world test performance drops to 28-68\%, revealing significant generalization challenges
    \item \textbf{Department classification}: Best department F1 is only 0.349 (oversampling) or 0.535 (two-stage with higher accuracy focus), below feature engineering's 0.615
\end{itemize}
\paragraph{Why This Performance Pattern.}
Seniority classification benefits from two factors: (1) explicit lexical markers that appear consistently across domains, and (2) reasonable class balance enabling the model to learn all classes. Department classification, by contrast, requires understanding context-dependent terminology (``Manager'' means different things in different departments) while handling extreme class imbalance—a combination that proves challenging even for state-of-the-art transformers.
\paragraph{Key Insight.}
The fundamental problem is not model capacity but the domain gap between training data (clean, structured CSV lookup tables) and test data (noisy, real-world LinkedIn profiles). Transformers trained on clean data overfit to the training distribution's patterns, performing poorly when the test distribution differs. This explains why feature engineering—which explicitly encodes domain-invariant knowledge—outperforms transformers for department classification despite having far fewer parameters.
\paragraph{Practical Recommendation.}
For production systems facing similar domain shift challenges: (1) Use feature engineering for tasks requiring robust generalization across distribution changes, (2) Use transformers for tasks with consistent lexical patterns and balanced classes, (3) Consider hybrid approaches that combine both strengths.


\section{Experiments}
\label{sec:experiments}

This section presents the systematic comparison of all implemented approaches, using consistent evaluation protocols and metrics.

\subsection{Experimental Setup}
\label{subsec:experimental-setup}

\paragraph{Training Data}
Models were trained on the provided lookup tables:
\begin{itemize}
    \item Department: 10,145 text-label pairs across 11 classes
    \item Seniority: 9,428 text-label pairs across 5 classes
\end{itemize}

\paragraph{Evaluation Data}
All models were evaluated on the annotated LinkedIn CVs (623 active positions), representing real-world distribution with significant domain shift from training data.

\paragraph{Metrics}
\begin{itemize}
    \item \textbf{Accuracy}: Overall correct predictions / total predictions
    \item \textbf{Macro F1}: Unweighted average F1 across all classes (treats minority classes equally)
    \item \textbf{Weighted F1}: F1 weighted by class frequency (reflects overall performance)
\end{itemize}

\subsection{Quantitative Results}
\label{subsec:quantitative-results}

\begin{table}[h]
\centering
\caption{Comprehensive comparison of all approaches on evaluation set}
\label{tab:final-comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Dept Acc} & \textbf{Dept F1} & \textbf{Sen Acc} & \textbf{Sen F1} \\
\midrule
\multicolumn{5}{l}{\textit{Baseline Approaches}} \\
Rule-Based & 0.302 & 0.429 & 0.427 & 0.365 \\
Embedding Zero-Shot & 0.247 & 0.287 & 0.364 & 0.238 \\
Hybrid (Rule+Embed) & 0.479 & 0.365 & 0.465 & 0.434 \\
\midrule
\multicolumn{5}{l}{\textit{Classical ML}} \\
TF-IDF + LogReg & 0.268 & 0.393 & 0.474 & 0.430 \\
\textbf{Feature Engineering} & 0.610 & \textbf{0.615} & 0.475 & 0.443 \\
\midrule
\multicolumn{5}{l}{\textit{Transformer-based (DistilBERT)}} \\
Baseline & 0.283 & 0.343 & \textbf{0.705} & \textbf{0.616} \\
Class Balancing & 0.274 & 0.343 & -- & -- \\
Oversampling & 0.270 & 0.349 & 0.705 & 0.607 \\
Combined & 0.268 & 0.346 & -- & -- \\
Two-Stage v2 & \textbf{0.685} & 0.535 & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{enumerate}
    \item \textbf{Department Classification}: Feature Engineering achieves the best F1 (0.615), while Two-Stage achieves the best accuracy (68.5\%)
    \item \textbf{Seniority Classification}: DistilBERT Baseline clearly dominates (F1=0.616, Acc=70.5\%)
    \item \textbf{Accuracy vs. F1 Trade-off}: Two-Stage optimizes for accuracy (important when ``Other'' is prevalent), while Feature Engineering optimizes for balanced performance across all classes
\end{enumerate}

\subsection{Error Analysis}
\label{subsec:error-analysis}

\subsubsection{Department Classification Errors}

The confusion matrix reveals systematic error patterns:

\begin{itemize}
    \item \textbf{``Other'' confusion}: Many specific departments incorrectly classified as ``Other'' and vice versa
    \item \textbf{IT vs. Consulting overlap}: Technical consultants frequently misclassified
    \item \textbf{Business Development vs. Sales}: Similar terminology causes boundary confusion
    \item \textbf{Administrative}: Low recall due to diverse job titles (``Office Manager'', ``Executive Assistant'', etc.)
\end{itemize}

\subsubsection{Seniority Classification Errors}

\begin{itemize}
    \item \textbf{Management vs. Lead confusion}: Organizational hierarchy varies by company size
    \item \textbf{Senior vs. Professional boundary}: Context-dependent distinction not captured by title alone
    \item \textbf{Director underrepresentation}: Only 5.5\% of training data, causing lower recall
\end{itemize}

\subsubsection{Domain Shift Analysis}

The significant performance gap between in-distribution (lookup table validation) and real-world (annotated CVs) evaluation reveals domain shift:

\begin{table}[h]
\centering
\caption{In-distribution vs. Real-world performance gap}
\label{tab:domain-shift}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{In-Dist Acc} & \textbf{Real-World Acc} & \textbf{Gap} \\
\midrule
Rule-Based & 1.000 & 0.302 & -69.8\% \\
DistilBERT Baseline & 0.999 & 0.283 & -71.6\% \\
Feature Engineering & 0.850 & 0.610 & -24.0\% \\
\bottomrule
\end{tabular}
\end{table}

Feature Engineering shows the smallest domain shift gap, explaining its superior real-world performance despite lower in-distribution accuracy.



\section{Discussion and Analysis}
\label{sec:discussion}

\subsection{Model Comparison and Performance Analysis}
\label{subsec:model-comparison}

The experimental results reveal several counter-intuitive findings that merit deeper analysis.

\paragraph{Why Feature Engineering Beats Transformers for Department Classification}

The Feature Engineering approach (F1=0.615) outperformed all DistilBERT variants for department classification. This result contradicts the common assumption that deep learning models should dominate text classification tasks. The explanation lies in \textbf{domain shift}:

\begin{itemize}
    \item \textbf{Training-evaluation mismatch}: Lookup tables contain standardized job titles, while real CVs contain diverse, informal variations
    \item \textbf{Feature invariance}: Hand-crafted career features (tenure patterns, keyword density, title structure) transfer better across domains
    \item \textbf{Overfitting to surface patterns}: Transformers learn lexical patterns that don't generalize to unseen formulations
\end{itemize}

\paragraph{Why DistilBERT Excels at Seniority Classification}

Conversely, DistilBERT Baseline achieves the best seniority F1 (0.616), significantly outperforming other approaches. Key factors:

\begin{itemize}
    \item \textbf{Explicit lexical markers}: Seniority levels have clear textual indicators (Jr., Sr., Lead, Director, VP, Chief)
    \item \textbf{More balanced distribution}: Unlike the 55\% ``Other'' in departments, seniority classes are more evenly distributed
    \item \textbf{Simpler classification boundary}: 5 classes vs. 11 classes reduces complexity
\end{itemize}

\subsection{The DistilBERT Iteration Journey}
\label{subsec:distilbert-journey}

Our systematic exploration of five DistilBERT configurations demonstrates the scientific methodology applied throughout this project:

\begin{enumerate}
    \item \textbf{Baseline} $\rightarrow$ Established performance floor (Dept F1=0.343)
    \item \textbf{Class Balancing} $\rightarrow$ No improvement, suggesting the issue is not class imbalance alone
    \item \textbf{Oversampling} $\rightarrow$ Marginal improvement (+0.006), indicating data augmentation helps slightly
    \item \textbf{Combined} $\rightarrow$ No additive benefit, techniques interfere with each other
    \item \textbf{Two-Stage} $\rightarrow$ Breakthrough for accuracy (+0.40), hierarchical approach addresses ``Other'' dominance
\end{enumerate}

This iterative refinement illustrates that class imbalance in NLP requires problem-specific solutions beyond standard techniques.

\subsection{Impact of Preprocessing and Feature Choices}
\label{subsec:preprocessing-impact}

Several preprocessing decisions significantly impacted performance:

\begin{itemize}
    \item \textbf{Text normalization}: Lowercasing and whitespace normalization improved rule-based matching by 15-20\%
    \item \textbf{Fallback strategies}: The choice of default class (``Other'' for department, ``Professional'' for seniority) has substantial impact on accuracy
    \item \textbf{Sequence length}: 64 tokens sufficient for job titles; longer context (job descriptions) provided minimal benefit
\end{itemize}

\subsection{Limitations and Threats to Validity}
\label{subsec:limitations}

\paragraph{Data Limitations}
\begin{itemize}
    \item \textbf{Annotation quality}: Human labelers may disagree on borderline cases, especially for seniority
    \item \textbf{Class definition ambiguity}: The ``Other'' department is a catch-all with heterogeneous contents
    \item \textbf{Sample size}: 623 evaluation samples limit statistical power for minority classes
\end{itemize}

\paragraph{Methodological Limitations}
\begin{itemize}
    \item \textbf{Single evaluation set}: Results may not generalize to other LinkedIn data distributions
    \item \textbf{No temporal validation}: Job title conventions evolve; models may become outdated
    \item \textbf{Limited hyperparameter search}: Computational constraints prevented exhaustive optimization
    \item \textbf{Data leakage via informed defaults}: The rule-based approach uses ``Professional'' as the seniority fallback, a class that does not exist in the training data (0\%) but represents 34.7\% of evaluation data. Comparison of Basic (no fallback) vs. Enhanced (with fallback) versions shows this provides an unfair advantage of +15pp for seniority and +40pp for department accuracy. While practically necessary for production systems, this constitutes methodological data leakage that should be considered when comparing across approaches. Supervised models do not face this issue as they learn to predict all classes from training data. See Section~\ref{subsec:fallback-dilemma} for detailed discussion.
\end{itemize}

\paragraph{External Validity}
\begin{itemize}
    \item \textbf{Cultural/regional bias}: Dataset reflects specific geographic and industry distributions
    \item \textbf{LinkedIn-specific formatting}: Models may not transfer to other resume formats
\end{itemize}



\section{Conclusion and Summary of Key Findings}
\label{sec:conclusion}

This capstone project developed and systematically compared multiple approaches for predicting career domain and seniority from LinkedIn CVs. Through extensive experimentation across 12 distinct methods, we identified optimal solutions for each classification task.

\subsection{Summary of Results}

\paragraph{Department Classification}
\begin{itemize}
    \item \textbf{Best Accuracy}: Two-Stage DistilBERT (68.5\%)
    \item \textbf{Best F1 Score}: Feature Engineering with Random Forest (0.615)
    \item \textbf{Recommendation}: Use Feature Engineering for balanced performance, Two-Stage for accuracy-focused applications
\end{itemize}

\paragraph{Seniority Classification}
\begin{itemize}
    \item \textbf{Best Accuracy}: DistilBERT Baseline (70.5\%)
    \item \textbf{Best F1 Score}: DistilBERT Baseline (0.616)
    \item \textbf{Recommendation}: Use DistilBERT Baseline (simplest high-performing solution)
\end{itemize}

\subsection{Key Insights}

\begin{enumerate}
    \item \textbf{Domain-invariant features outperform transformers when domain shift is significant}. The Feature Engineering approach achieved the best department F1 by focusing on career patterns rather than lexical features.
    
    \item \textbf{Class imbalance requires problem-specific solutions}. Standard techniques (class weights, oversampling) had minimal impact; the Two-Stage hierarchical approach effectively addressed the 55\% ``Other'' class dominance.
    
    \item \textbf{Simpler models can outperform complex ones}. DistilBERT Baseline was the best seniority classifier, outperforming class-balanced and oversampled variants.
    
    \item \textbf{Accuracy and F1 measure different aspects}. The choice between Two-Stage (high accuracy) and Feature Engineering (high F1) depends on the application requirements.
\end{enumerate}

\subsection{Practical Recommendations}

For production deployment, we recommend:
\begin{itemize}
    \item \textbf{Department}: Feature Engineering model for robust generalization
    \item \textbf{Seniority}: DistilBERT Baseline for optimal performance
    \item \textbf{Fallback strategy}: Rule-based matching as a fast, interpretable backup when transformer inference is impractical
\end{itemize}

\subsection{Future Work}

Several directions could improve performance:
\begin{itemize}
    \item \textbf{Data augmentation}: Synthetic job title generation using LLMs
    \item \textbf{Multi-task learning}: Joint department-seniority prediction with shared representations
    \item \textbf{Active learning}: Iteratively labeling high-uncertainty samples to expand training data
    \item \textbf{Cross-lingual transfer}: Leveraging multilingual models for non-English CVs
\end{itemize}

\section{GenAI Usage Documentation}
\label{sec:genai-usage}

In accordance with project requirements, we document the use of Generative AI tools throughout this project.

\subsection{Tools Used}
\begin{itemize}
    \item \textbf{GitHub Copilot}: Code completion and boilerplate generation
    \item \textbf{Claude (Anthropic)}: Code review, debugging assistance, and documentation
    \item \textbf{ChatGPT}: Research questions and conceptual explanations
\end{itemize}

\subsection{Use Cases}
\begin{enumerate}
    \item \textbf{Code Generation}: Trainer class implementations (WeightedTrainer, FocalTrainer), evaluation utilities
    \item \textbf{Debugging}: Resolving dependency conflicts, fixing tensor shape mismatches
    \item \textbf{Documentation}: LaTeX formatting, report structure suggestions
    \item \textbf{Research}: Understanding Focal Loss, class imbalance strategies
\end{enumerate}

\subsection{Verification Process}
All AI-generated code was:
\begin{itemize}
    \item Reviewed for correctness and alignment with project goals
    \item Tested through unit tests and end-to-end notebook execution
    \item Validated against expected outputs and metrics
\end{itemize}

Complete GenAI usage documentation is available in the repository at \texttt{docs/genai\_usage.md}.

\section{Code Availability}
\label{sec:code-availability}

All code, notebooks, and documentation are available in the project repository:

\begin{center}
\url{https://github.com/Julster35/BuzzwordLearner}
\end{center}

\paragraph{Repository Structure}
\begin{itemize}
    \item \texttt{notebooks/} -- All experimental notebooks (01--99)
    \item \texttt{src/} -- Reusable Python modules
    \item \texttt{data/} -- Dataset files (lookup tables, annotations)
    \item \texttt{docs/genai\_usage.md} -- Complete GenAI documentation
    \item \texttt{results/} -- Saved model outputs and metrics
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The bibliography is inserted automatically
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\lhead{}
\printbibliography
\addcontentsline{toc}{section}{\bibname}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\section{Appendix A}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Affidavit must be adapted 
%% in declaration.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{declaration.tex}

\end{document}
