\documentclass[12pt,oneside]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Additional packages  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumerate}  
\usepackage{fancyhdr}
\usepackage{ifthen}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\usepackage{acronym}
\usepackage{float}


\newboolean{englishLanguage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% CHANGE true to false for a german paper %%%%
\setboolean{englishLanguage}{true} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifthenelse{\boolean{englishLanguage}}{\usepackage[ngerman, english]{babel}}{\usepackage[ngerman]{babel}} 

\usepackage[T1]{fontenc}
\usepackage{paratype}\renewcommand{\familydefault}{\sfdefault}
\usepackage[utf8]{inputenc}
\usepackage[bookmarks]{hyperref}
\usepackage[justification=centering]{caption}
\usepackage[style=authoryear,natbib=true,backend=biber,maxbibnames=20]{biblatex}
\usepackage{csquotes}
\bibliography{literatur}

\setlength{\parindent}{0em} 
\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[enumerate]{noitemsep, topsep=0pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definition of the header %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\setlength{\headheight}{16pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Definition of the cover page and title page  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\ifthenelse{\boolean{englishLanguage}}{
%%% ENGLISH VERSION
\newcommand{\JMUTitle}[9]{

  \thispagestyle{empty}
  \vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}
  }
  \begin{flushright}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #2\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\small
    #3
  \end{flushright}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{1}}
  \begin{center}
    \includegraphics[width=2in]{seal.pdf} \\
    \vspace*{\stretch{1}}
    \Large #5 \\

    \vspace*{\stretch{2}}
   \large Chair for Enterprise Artificial Intelligence\\
    \large Universität Würzburg\\
    \vspace*{\stretch{1}}
    \large Supervisor:  #8 \\[1mm]
    \large Assistant:  #9 \\[1mm]
    \vspace*{\stretch{1}}
    \large #6, #7
  \end{center}
}}{
%%% GERMAN VERSION
\newcommand{\JMUTitle}[9]{

  \thispagestyle{empty}
  \vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}
  }
  \begin{flushright}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #2\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\small
    #3
  \end{flushright}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{1}}
  \begin{center}
    \includegraphics[width=2in]{seal.pdf} \\
    \vspace*{\stretch{1}}
    \Large #5 \\

    \vspace*{\stretch{2}}
   \large Lehrstuhl f\"ur K\"unstliche Intelligenz im Unternehemen\\
    \large Universität Würzburg\\
    \vspace*{\stretch{1}}
    \large Betreuer:  #8 \\[1mm]
    \large Assistent:  #9 \\[1mm]
    \vspace*{\stretch{1}}
    \large #6, den #7
  \end{center}
}} 


\titlespacing*{\section}
{0pt}{3.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}
\titlespacing*{\subsubsection}
{0pt}{1.5ex plus 1ex minus .2ex}{.2ex plus .2ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Beginning of the document  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
    \ifthenelse{\boolean{englishLanguage}}{
    \JMUTitle
      {Predicting Career Domain and Seniority from LinkedIn Profiles}        % Title of the paper
      {Group 10: Julien Froidefond, Batuhan Kalkan, Steen Stiller}           % First and last name of the author
      {2905448, 2768939, 2785417}
      
      {Capstone Project} % Type of the work
      {W\"urzburg}                           % Place
      {31.01.2026}                          % Date of Submission
      {Prof. Dr. Gunther Gust}           % Name of the first examiner
      {Govind Rao} % Name of the supervisor
    }
      
  \clearpage

\lhead{}
\pagenumbering{Roman} 
    \setcounter{page}{1}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Short Summary   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{Abstract}
\section*{Abstract}

This thesis develops and evaluates an end-to-end machine learning pipeline for the automated prediction of both departmental affiliation and seniority level from unstructured LinkedIn resumes, with a particular focus on classifying the current position (``ACTIVE''). The core challenge lies in processing heterogeneous, multilingual free-text data and bridging the domain gap between structured training dictionaries and noisy real-world profiles. This difficulty is further amplified by pronounced label distribution mismatches, including substantial class imbalance between the lookup tables and the evaluation dataset as well as partially missing labels, which together hinder reliable generalization.

Methodologically, the study conducts a systematic comparison across a broad spectrum of approaches, ranging from rule-based systems to deep learning models. The evaluated methods include: (1) an optimized rule-based baseline incorporating fuzzy matching, (2) zero-shot classification using sentence embeddings, (3) classical machine learning models employing TF--IDF representations (logistic regression) and feature engineering based on career trajectories (random forest), and (4) a semi-supervised pseudo-labeling strategy to leverage unannotated data. The primary emphasis is placed on (5) transformer-based architectures (multilingual DistilBERT), which are optimized through multiple strategies for handling class imbalance, including weighted loss functions, custom oversampling, and a hierarchical two-stage classification framework employing focal loss.

Experimental results on the annotated evaluation dataset indicate that the two target variables require distinct modeling strategies. For seniority prediction, feature engineering approaches achieve more robust performance (accuracy: 47.3\%, macro F1: 0.437) by leveraging domain-invariant structural signals. In contrast, for the more semantically complex department classification task, the hierarchical two-stage DistilBERT approach achieves superior performance (accuracy: 68.1\%, macro F1: 0.534) by effectively isolating the majority class and overcoming the limitations of simpler feature-engineering methods. Overall, the findings suggest that hierarchical deep learning strategies are essential for managing strong distributional differences in complex semantic classification problems, whereas robust feature engineering may be preferable for structural tasks characterized by significant distributional shifts in label space.
\newline\newline
\textbf{All code, notebooks, and documentation are available in the project repository:}

\begin{center}
\url{https://github.com/Julster35/BuzzwordLearner}
\end{center}


\clearpage
\lhead{\nouppercase{\leftmark}}

\newpage
\tableofcontents

\clearpage
\pagestyle{plain} % oder empty, wenn auch keine Fußzeile/Seitenzahl

\addcontentsline{toc}{section}{List of Figures}
\listoffigures

\addcontentsline{toc}{section}{List of Tables}
\listoftables

\section*{List of Abbreviations}
\addcontentsline{toc}{section}{List of Abbreviations}
\begin{acronym}[ECU]
\acro{cv}[CV]{Curriculum Vitae}
\acro{eda}[EDA]{Exploratory Data Analysis}
\end{acronym}

\pagestyle{fancy} % zurück zum normalen Stil
\setlength{\parskip}{0.5em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Settings  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}  
    \setcounter{page}{1}
\lhead{\nouppercase{\leftmark}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Main Section  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this capstone project, we develop an machine learning pipeline to predict two key attributes of an individual's \textit{current} job from their LinkedIn CV data: (i) the professional domain (\textit{department}) and (ii) the seniority level. The classification and evaluated is done on a hand-labeled dataset provided by SnapAddy.

Our work covers the full workflow from exploratory data analysis and data preprocessing to model development, comparison, and evaluation. We implement at least one transparent baseline approach (rule-based matching) and at least one additional learning-based method, and we document assumptions, limitations, and potential failure cases. The final deliverables include a reproducible code base (notebook-based pipeline), an evaluation of the proposed approaches on the labeled dataset, and written documentation suitable for a final report and presentation.

\subsection{Problem Statement}
The task is to predict two categorical target variables,  \textit{department} (career domain) and \textit{seniority},  for the \textit{current} position of an individual using only the information contained in their LinkedIn CV. The current position is identified by the profile field \texttt{status = ACTIVE}. Each LinkedIn profile consists of semi-structured career entries such as job title, company name, time span, and optional description text, from which the relevant signals for the current role must be extracted.

The main challenges arise from heterogeneous text data, including inconsistent job titles, varying levels of detail, missing descriptions, and occasional multilingual entries. In addition, both targets are context-dependent: domain boundaries can be fuzzy due to overlapping terminology, and seniority indicators may differ strongly across industries and company sizes. The objective is to build a robust and reproducible pipeline that generalizes well to unseen profiles while clearly documenting assumptions, limitations, and potential failure cases.


\subsection{Datasets Description}

Four datasets are provided that serve different purposes within the machine learning pipeline.
The \texttt{linkedin-cvs-annotated.json} dataset contains structured LinkedIn CVs, with individual career entries annotated with the target variables \textit{department} (professional domain) and \textit{seniority}. Importantly, this annotated dataset is used exclusively for evaluation purposes to assess model performance. It is not used for training to avoid overfitting and to provide an unbiased measure of model generalization.
In contrast, \texttt{linkedin-cvs-not-annotated.json} has the same data structure, but does not include target labels. Models can be applied to this data in realistic deployment scenarios.
In addition to these two JSON datasets, two CSV files provide auxiliary resources for programmatic labeling and baseline approaches.
The \texttt{department-v2.csv} file comprises a lexicon that maps job-related textual expressions to department labels, while the \texttt{seniority-v2.csv} file analogously maps textual patterns to seniority levels.
These two lexica form the primary training data for most approaches. They can be used to construct rule-based classifiers, train supervised models, or support weakly supervised labeling strategies.

\section{Exploratory Data Analysis}
\label{sec:eda}

The exploratory data analysis (EDA) serves to systematically investigate the BuzzwordLearner data for predicting career domains and seniority levels from LinkedIn profiles. The goal is to develop a comprehensive understanding of the data structure, distributions, and potential challenges for modeling.

\subsection{Dataset Overview}
\label{subsec:dataset-overview}

The dataset consists of four main components:
\begin{itemize}
    \item \textbf{Annotated LinkedIn CVs}: 609 resumes with manually assigned department and seniority labels (used exclusively for evaluation)
    \item \textbf{Non-annotated LinkedIn CVs}: 390 resumes for inference without ground-truth labels
    \item \textbf{Department Label Dictionary}: 10,145 job title $\rightarrow$ department mappings (primary training data)
    \item \textbf{Seniority Label Dictionary}: 9,428 job title $\rightarrow$ seniority mappings (primary training data)
\end{itemize}
Each resume is structured as a list of positions, where each position contains the following attributes: \texttt{organization}, \texttt{position} (job title), \texttt{startDate}, \texttt{endDate}, \texttt{status} (ACTIVE/INACTIVE/UNKNOWN), as well as the target variables \texttt{department} and \texttt{seniority} (only in annotated data).

\subsection{Annotated Dataset Analysis}

The annotated dataset comprises 609 LinkedIn resumes with a total of 2,638 position entries. Data quality is high: there are no missing values except for a negligible number in start and end dates. Additionally, no duplicate rows were identified in the dataset.

\subsubsection{Career History Patterns}
\label{subsec:career-patterns}
The analysis of career trajectories reveals the following characteristics:
\begin{itemize}
    \item \textbf{Positions per CV}: On average, resumes contain 4.5 positions (median: 4), with a range from 1 to 20 positions
    \item \textbf{Active positions}: 623 active positions from the annotated CVs form the evaluation dataset
\end{itemize}

As shown in Figure~\ref{fig:positions_per_cv}, most CVs contain only a small number of positions, while a few individuals exhibit substantially longer career histories.

\begin{figure}[H]
    \centering
    \caption{Distribution of the number of positions per CV}
    \includegraphics[width=0.75\textwidth]{figures/num_pos_cv.png}
    \label{fig:positions_per_cv}
\end{figure}

The average duration of a position is 28.3 months. A particularly striking pattern is observed in Figure~\ref{fig:duration_sequence}. The first position: with an average of only 4 months, it is significantly shorter than all subsequent positions. This suggests that first entries are often side jobs, internships, or temporary entry-level positions. Therefore, there is a high probability that the department or seniority level changes after this first position.

\begin{figure}[H]
    \centering
    \caption{Average position duration by career sequence}
    \includegraphics[width=0.75\textwidth]{figures/avg_duration_by_career_sequence.png}
    \label{fig:duration_sequence}
\end{figure}

In addition to the number of positions per CV, the dataset also provides insights into the average duration of employment across different departments and seniority levels.

As illustrated in Figure~\ref{fig:avg_position_duration_dept}, the average position duration varies notably across departments. Roles in Purchasing, Human Resources, and Customer Support tend to exhibit the longest average employment periods, exceeding the overall mean. In contrast, Administrative and Consulting positions show shorter durations, indicating higher turnover or more temporary career paths.

\begin{figure}[H]
    \centering
    \caption{Average department duration (months)}
    \includegraphics[width=0.75\textwidth]{figures/avg_duration_dept.png}
    \label{fig:avg_position_duration_dept}
\end{figure}

In addition to that Figure~\ref{fig:avg_position_duration_sen}. Director, Senior, and Lead positions are associated with longer average tenures, reflecting more stable career stages. Conversely, Junior roles display the shortest duration, which is consistent with early-career mobility and transitional job phases. Overall, these findings highlight that both departmental context and seniority level influence career stability, which may provide useful signals for downstream classification tasks.

\begin{figure}[H]
    \centering
    \caption{Average seniority duration (months).}
    \includegraphics[width=0.7\textwidth]{figures/avg_duration_seniority.png}
    \label{fig:avg_position_duration_sen}
\end{figure}

Seniority tends to increase over the course of a career, as can be observed in Figure~\ref{fig:seniority_progression}. This corresponds to the expected career development, where candidates achieve higher positions with increasing professional experience.

\begin{figure}[H]
    \centering
    \caption{Seniority development over career}
    \includegraphics[width=1\textwidth]{figures/seniority_progression_analysis.png}
    \label{fig:seniority_progression}
\end{figure}

The analysis of department stability in Figure \ref{fig:dept_stability}  shows the following distribution:
\begin{itemize}
    \item 25\% of resumes show no department change
    \item 34.8\% have one change
    \item 26\% have two changes
    \item Only 14.1\% show three or more department changes
\end{itemize}

\begin{figure}[h]
    \centering
    \caption{Department changes across career}
    \includegraphics[width=0.70\textwidth]{figures/department_stability.png}
    \label{fig:dept_stability}
\end{figure}

Analysis of the 15 most common department transitions (see Figure \ref{fig:department_transitions.png}) reveals that they either lead from or to the "Other" category. Therefore, no clear patterns in department changes can be identified, suggesting heterogeneous career development across different functional areas.

\begin{figure}[h]
    \centering
    \caption{Most frequent department transitions}
    \includegraphics[width=1 \textwidth]{figures/department_transitions.png}
    \label{fig:department_transitions.png}
\end{figure}

\subsubsection{Text Characteristics of Position Titles}
\label{subsec:text-characteristics}

Position titles in the dataset are generally short, with a median length of 32 characters and only 4 words, as shown in Figure~\ref{fig:title_length_distribution}. This limited context requires models that can extract meaningful information from few tokens.

\begin{figure}[H]
    \centering
    \caption{Distribution of position title lengths in characters and words}
    \includegraphics[width=1\textwidth]{figures/word_analysis.png}
    \label{fig:word_analysis}
\end{figure}

\subsubsection{Multilinguality}
\label{subsec:multilingualism}

The dataset contains multiple languages. Language detection is challenging due to the short text lengths of position titles, which often contain only 2-5 words. This makes clear language separation difficult, as titles may mix languages or lack sufficient context for reliable classification. Despite these challenges, addressing multilinguality is crucial for model performance. Approaches require multilingual embeddings (e.g., \texttt{paraphrase-multilingual-MiniLM-L12-v2}) and language-agnostic features to ensure robust predictions across all languages.

A simple heuristic method was applied to detect the language of position titles. The most commonly native languages in Europe are Russian, German, French, English, and Turkish. Since the dataset contains no Cyrillic characters, Russian was excluded. The remaining languages were broadly categorized as seen in tabel \ref{tab:t1}.

\begin{table}[H]
\centering
\caption{Detected languages in position titles}
\label{tab:t1}
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Language} & \textbf{Count} & \textbf{Percent (\%)} \\
\midrule 
English & 1150 & 43.59 \\
Other & 600 & 22.74 \\
German & 434 & 16.45 \\
French & 387 & 14.67 \\
Turkish & 43 & 1.66 \\

\bottomrule
\end{tabular}
\label{tab:languages}
\end{table}

English dominates the dataset, followed by Other. This multilingualism is relevant for model development, as a multilingual approach may be necessary to ensure robust classification across language boundaries. The results do not aim to provide precise language identification. Rather they highlight the importance of multilingual considerations, as reliable classification in this setting can only be achieved using approaches that operate in a language-agnostic manner.

\subsubsection{Analysis of Active Positions}

After filtering to only active positions which means current job, the following picture emerges:

\begin{table}[H]
\centering
\caption{Distribution of active positions per resume}
\footnotesize
\label{tab:t2}
\begin{tabular}{lrr}
\toprule
\textbf{Active Positions} & \textbf{Number of CVs} & \textbf{Percent (\%)} \\
\midrule
1 & 484 & 79.48 \\
2 & 99 & 16.26 \\
3 & 20 & 3.28 \\
4 & 4 & 0.66 \\
5 & 2 & 0.33 \\
\bottomrule
\end{tabular}
\label{tab:active_positions}
\end{table}

The vast majority (79.48\%), seen in table \ref{tab:t2} of candidates have only one active job, while 16.32\% hold two concurrent positions. This could indicate side jobs, consulting activities, or part-time work.

\subsubsection{Department Distribution}
\label{subsec:department-distribution}

The analysis of department labels reveals a significant \textbf{class imbalance} in the active positions:
\begin{table}[H]
\centering
\caption{Distribution of departments in active positions}
\footnotesize
\label{tab:t3}
\begin{tabular}{lr}
\toprule
\textbf{Department} & \textbf{Proportion (\%)} \\
\midrule
Other & 55.2 \\
Information Technology & 10.0 \\
Sales & 7.4 \\
Consulting & 6.3 \\
Project Management & 6.3 \\
Marketing & 3.5 \\
Business Development & 3.2 \\
Human Resources & 2.6 \\
Purchasing & 2.4 \\
Administrative & 2.2 \\
Customer Support & 1.0 \\
\bottomrule
\end{tabular}
\label{tab:department-distribution}
\end{table}

\textbf{Modeling implication}: The dominance of "Other" (see table \ref{tab:t3}) as a catch-all category requires class weighting or resampling techniques to avoid bias towards the majority class.

\subsubsection{Seniority Distribution}
\label{subsec:seniority-distribution}

The dataset contains 6 unique seniority labels with the distribution shown in Table \ref{tab:seniority-distribution}:

\begin{table}[H]
\centering
\caption{Distribution of seniority levels}
\footnotesize
\begin{tabular}{lr}
\toprule
\textbf{Seniority Level} & \textbf{Proportion (\%)} \\
\midrule
Junior & 1.9 \\
Professional & 34.7 \\
Senior & 7.1 \\
Lead & 20.1 \\
Management & 30.8 \\
Director & 5.5 \\
\bottomrule
\end{tabular}
\label{tab:seniority-distribution}
\end{table}

The seniority distribution shows a clear dominance of the \textit{Professional} (34.7\%) and \textit{Management} (30.8\%) levels. Lead positions are also well represented at 20.1\%, while Junior positions are significantly underrepresented at only 1.9\%.

\subsection{Department Label Dictionary Analysis}

The department lookup table contains 10,145 unique entries in various languages, mapping job titles to 11 unique department labels. This comprehensive dictionary serves as the foundation for department classification. The distribution of department labels in the lookup table is shown in Table \ref{tab:dept_lookup_distribution}.

\begin{table}[H]
\centering
\caption{Distribution of department labels in lookup Table}
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Department} & \textbf{Count} & \textbf{Percent (\%)} \\
\midrule
Marketing & 4,295 & 42.34 \\
Sales & 3,328 & 32.80 \\
Information Technology & 1,305 & 12.86 \\
Business Development & 620 & 6.11 \\
Project Management & 201 & 1.98 \\
Consulting & 167 & 1.65 \\
Administrative & 83 & 0.82 \\
Other & 42 & 0.41 \\
Purchasing & 40 & 0.39 \\
Customer Support & 33 & 0.33 \\
Human Resources & 31 & 0.31 \\
\bottomrule
\end{tabular}
\label{tab:dept_lookup_distribution}
\end{table}

The lookup table shows a different distribution than the annotated dataset, with \textit{Marketing} (42.34\%) and \textit{Sales} (32.80\%) dominating, followed by \textit{Information Technology} (12.86\%). Notably, \textit{Other} represents only 0.41\% in the lookup table, contrasting sharply with its 55.2\% prevalence in the annotated dataset. The multilingual nature of the dictionary enables cross-language classification.

\subsection{Seniority Label Dictionary Analysis}

The seniority lookup table comprises 9,428 entries. In contrast to the annotated dataset, this lookup table contains only 5 unique labels. Notably, the \textit{Professional} label is completely absent from the lookup table, representing a significant difference in label definitions between the two data sources. The distribution is presented in Table \ref{tab:sen_lookup_distribution}.

\begin{table}[H]
\centering
\caption{Distribution of Seniority Labels in Lookup Table}
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Seniority Level} & \textbf{Count} & \textbf{Percent (\%)} \\
\midrule
Junior & 409 & 4.34 \\
Senior & 3,733 & 39.59 \\
Lead & 3,546 & 37.61 \\
Management & 756 & 8.02 \\
Director & 984 & 10.44 \\
\bottomrule
\end{tabular}
\label{tab:sen_lookup_distribution}
\end{table}

The absence of the \textit{Professional} level in the lookup table, despite its prevalence of 34.7\% in the annotated dataset, presents a challenge for model development.

\subsection{Label Distribution Comparison}

The comparison between lookup table and annotated dataset distributions reveals important insights for model training. Figure \ref{fig:dept_comparison} illustrates the department label distributions across both sources.

\begin{figure}[H]
    \centering
    \caption{Department label distribution: Lookup Table vs. Annotated Dataset}
    \includegraphics[width=0.95\textwidth]{figures/dept_distribution_comparison.png}
    \label{fig:dept_comparison}
\end{figure}

While the overall patterns are similar, notable differences exist in the relative proportions of certain departments. The Other category remains consistently high in both sources, indicating a substantial number of positions that do not fit neatly into predefined categories. In addition, the lookup table shows an overrepresentation of Marketing and Sales, and a major mismatch is observed for the Other label: while it is almost absent in the annotated dataset, it accounts for 55.2\% of all active positions, highlighting a substantial distribution shift between the annotation source and the data encountered in practice.

Figure \ref{fig:sen_comparison} highlights the critical difference in seniority distributions. The complete absence of the \textit{Professional} label in the lookup table, combined with the significantly higher proportion of \textit{Senior} positions (29.14\% vs. 7.1\%), suggests that positions annotated as \textit{Professional} in the dataset might correspond to \textit{Senior} or other categories in the lookup table. Additionally, the unequal distribution across labels introduces class imbalance challenges that must be addressed in model training through techniques such as weighted loss functions or oversampling strategies.

\begin{figure}[H]
    \centering
    \caption{Seniority label distribution: Lookup Table vs. Annotated Dataset}
    \includegraphics[width=0.95\textwidth]{figures/seniority_distribution_comparison.png}
    \label{fig:sen_comparison}
\end{figure}

\subsection{Unannotated Dataset Analysis}

The unannotated dataset consists of 390 LinkedIn resumes and exhibits remarkably similar characteristics to the annotated dataset. The distribution of position statuses (active, inactive, and unknown) closely mirrors that of the annotated data, suggesting consistent data collection practices across both datasets.

Key similarities include:
\begin{itemize}
    \item \textbf{Number of positions per CV:} The unannotated dataset shows a comparable distribution of position counts, with most resumes containing between 2 and 7 positions.
    \item \textbf{Title length:} Both character count and word count in position titles are consistent across annotated and unannotated datasets.
    \item \textbf{Language distribution:} The multilingual composition remains similar, with German and English as the predominant languages.
\end{itemize}

These similarities strongly suggest that a model which performs well on the annotated dataset will also generalize well to the unannotated data when used for inference. The primary difference is the absence of ground truth department and seniority labels in the unannotated dataset, which prevents direct evaluation but does not change the underlying input structure.

\subsection{Key Findings and Implications}
\label{subsec:key-findings}
Overall, the dataset exhibits several central challenges that must be considered when developing and interpreting model results. 
First, the input data is highly heterogeneous: job titles and descriptions are semi-structured free text with substantial variation, including abbreviations, inconsistent formatting, and mixed languages, while semantically similar roles are not consistently standardized. 
Second, the target labels are clearly imbalanced for both \textit{department} and \textit{seniority}, and even more strongly for their joint combinations, which can bias models towards majority classes. 
Third, the annotations introduce uncertainty because department and seniority assignments rely on human interpretation and may contain borderline cases or inconsistencies. 
This is particularly relevant for seniority prediction, as job titles are an imperfect proxy for hierarchical level and can vary by organizational context (e.g., the same title may imply different responsibilities in small firms versus large enterprises). 
Finally, the setup involves a pronounced domain shift, while large CSV label dictionaries enable strong supervision, models must generalize from clean lookup-table patterns to noisy, real-world LinkedIn-style CVs.

In conclusion, robust performance depends on methods that handle noisy short texts, multilinguality, and label imbalance while explicitly addressing the domain gap between structured training resources and unstructured profile data, making a combination of rule-based baselines and language-model-based representations a practical modeling direction.




\section{Rule-Based Baseline and the Fallback Dilemma}
\label{sec:rule-based-baseline}

\subsection{Motivation}
\label{subsec:rule-based-motivation}

The rule-based classifier serves as the fundamental baseline for this project, providing a transparent and interpretable approach to classification. Given the availability of comprehensive lookup tables mapping job titles to departments and seniority levels (approximately 19,000 examples total), a natural first approach is to leverage this knowledge through pattern matching.

The motivation for starting with a rule-based baseline is threefold. First, it establishes a performance floor that more complex methods must exceed to justify their additional complexity. Second, rule-based systems are inherently interpretable:  every prediction can be traced to a specific matching rule, making them valuable for understanding which features drive classification. Third, they are computationally efficient, enabling real-time inference without GPU requirements.

\subsection{Methodology}
\label{subsec:rule-based-methodology}

The rule-based classifier employs a hierarchical matching strategy using lookup tables derived from the department and seniority label dictionaries (approximately 19,000 examples in total). The matching process follows a waterfall approach, progressing from fast, precise methods to slower, more flexible matching strategies:

\begin{enumerate}
    \item \textbf{Exact matching} (O(1)): Direct lookup of job title in the dictionary
    \item \textbf{Substring matching} (O(n)): Detection of dictionary entries as substrings in the job title
    \item \textbf{Keyword matching} (O(n)): Predefined keyword patterns for common job roles (e.g., "engineer" $\rightarrow$ IT, "manager" $\rightarrow$ Management)
    \item \textbf{Fuzzy matching} (O(n·m)): Similarity-based matching using \textit{difflib.SequenceMatcher}
    \item \textbf{Default fallback}: Assignment of default categories when no match is found
\end{enumerate}

This ordered execution ensures optimal computational efficiency while maintaining high matching accuracy.

\subsubsection{Model Optimizations}
\label{subsec:rule-based-optimizations}

Several targeted optimizations were applied to substantially improve the performance of the rule-based baseline.

\textbf{Text Normalization:}
All input texts were standardized using lowercase conversion and whitespace normalization, addressing inconsistent capitalization, formatting artifacts, and excessive spacing in real-world job titles.

\textbf{Fallback Strategy:}
When no lookup match is found, defaults are required. ``Other'' was used for department and ``Professional'' for seniority. While statistically reasonable for departments (55\% of evaluation data), this introduces a methodological issue for seniority, as ``Professional'' does not occur in the training lexicon. This trade-off between methodological purity and practical system design is discussed in Section~\ref{subsec:fallback-dilemma}.

\textbf{Fuzzy Matching Hyperparameter Tuning:}
To select an appropriate fuzzy matching threshold, we evaluated multiple similarity cutoffs on a validation split and compared the resulting accuracy to balance overly permissive matches against overly strict filtering. A threshold of 0.8 achieved the best overall performance and was therefore used for the final rule-based model.

\subsubsection{Performance Results}
\label{subsec:rule-based-results}

The optimized rule-based model was evaluated on the lookup table itself and the annotated dataset to assess both in-distribution performance and real-world generalization:

\begin{table}[h]
\centering
\caption{Rule-based baseline performance comparison (with fallback on real-world data)}
\label{tab:rule-based-performance}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{In-Distribution (CSV)} & \textbf{Real-World (JSON, with fallback)} \\
\midrule
Department Accuracy  & 1.000 & 0.682 \\
Department F1 (macro) & 1.000 & 0.553 \\
\midrule
Seniority Accuracy   & 1.000 & 0.584 \\
Seniority F1 (macro)  & 1.000 & 0.548 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Perfect in-distribution performance}: 100\% accuracy and macro F1 on the lookup table confirms the classifier correctly matches all known patterns without implementation errors.
    \item \textbf{Department classification} achieves 68.2\% accuracy on real-world data (with fallback), exceeding the naive baseline of 55\% (always predicting ``Other'').
    \item \textbf{Seniority classification} reaches 58.4\% accuracy on real-world data (with fallback), outperforming the naive baseline of 35\% (always predicting ``Professional'').
    \item A substantial performance gap between in-distribution (100\%) and real-world data (58--68\%) highlights the difficulty of generalizing from curated lookup tables to noisy job title formulations.
\end{itemize}


\subsubsection{Error Analysis and Efficiency}
\label{subsec:rule-based-errors}

Misclassifications primarily arise from novel or creative job titles, ambiguous roles lacking contextual cues, multi-domain positions, and non-English titles not covered by the English-centric lookup tables. These failure modes motivate the use of semantic representations beyond surface-level pattern matching.

Despite these limitations, the rule-based approach offers strong practical advantages: sub-millisecond inference time, no training requirement, and full interpretability, making it suitable for low-latency and transparent production settings.

\subsubsection{Computational Efficiency}
\label{subsec:rule-based-efficiency}

The rule-based approach offers significant computational advantages:
\begin{itemize}
    \item \textbf{Inference time}: < 1ms per prediction (real-time capable)
    \item \textbf{No training required}: Instant deployment with new lookup table entries
    \item \textbf{Interpretability}: All predictions are traceable to specific matching rules
\end{itemize}

These properties make the rule-based baseline an attractive choice for production systems requiring low latency, interpretability, and ease of maintenance.

\subsection{Conclusion}
\label{subsec:rule-based-conclusion}

The rule-based baseline demonstrates strong performance through optimized pattern matching, achieving 76.6\% accuracy for department and 59.8\% for seniority classification on real-world data. The approach excels in scenarios where job titles contain explicit lexical markers that appear in the lookup tables.

\textbf{Strengths:}
The rule-based approach performs well when,
\begin{itemize}
    \item \textbf{Exact or near-exact matches exist}: Titles like ``Senior Software Engineer'' or ``Marketing Manager'' are classified with high confidence
    \item \textbf{Clear keyword indicators are present}: Departmental terms (``sales'', ``HR'', ``engineering'') and seniority modifiers (``junior'', ``director'', ``VP'') enable accurate predictions
    \item \textbf{Interpretability is required}: Every prediction is traceable to a specific rule, enabling auditability and debugging
    \item \textbf{Low latency is critical}: Sub-millisecond inference time makes real-time applications feasible
\end{itemize}

\textbf{Limitations:}
The approach struggles with,
\begin{itemize}
    \item \textbf{Novel or creative job titles}: Modern roles absent from lookup tables (e.g., ``Growth Hacker'', ``DevOps Engineer'', ``Scrum Master'') cannot be matched
    \item \textbf{Semantic understanding}: The model cannot generalize beyond surface-level patterns—it treats ``Software Developer'' and ``Code Engineer'' as unrelated unless both appear in the lookup tables
    \item \textbf{Ambiguous titles}: Generic roles like ``Manager'' or ``Consultant'' lack context for accurate department assignment
    \item \textbf{Multilingual coverage}: The English-centric lookup tables underperform on non-English titles despite multilingual data in the test set
\end{itemize}

\textbf{Why This Performance Level:}
The significant gap between in-distribution performance (100\% on lookup table validation) and real-world performance (60-77\%) reveals a fundamental limitation: the lookup tables, while extensive, do not capture the full diversity of job title formulations in actual LinkedIn profiles. This observation motivates the exploration of embedding-based and transformer-based approaches capable of semantic generalization beyond exact pattern matching.

\subsection{Further Improvements and Investigations}
\label{subsec:rule-based-improvements}

While the current rule-based classifier demonstrates strong performance on known patterns, several enhancement opportunities remain:

\textbf{Keyword Expansion:}
The current keyword matching relies on a manually curated set of predefined patterns. Systematic expansion could improve coverage,
\begin{itemize}
    \item \textbf{Automated keyword extraction}: Mining the lookup tables to identify high-frequency discriminative terms
    \item \textbf{Synonym expansion}: Incorporating synonyms and related terms (e.g., "developer" $\leftrightarrow$ "programmer", "chief" $\leftrightarrow$ "director")
    \item \textbf{Abbreviation handling}: Explicit mapping of common abbreviations ("VP" $\rightarrow$ "Vice President", "CEO" $\rightarrow$ "Chief Executive Officer")
    \item \textbf{Domain-specific terminology}: Industry-specific role variations ("Full Stack Engineer" vs. "Software Engineer")
\end{itemize}

\textbf{Multilingual Support:}
The current implementation is predominantly English-centric, limiting performance on French, Spanish, and other European job titles present in the dataset (approx. 40\% non-English content). Potential improvements include,
\begin{itemize}
    \item \textbf{Language detection}: Automatic identification of input language to apply language-specific rules
    \item \textbf{Translation normalization}: Translating non-English titles to English before matching, or maintaining multilingual lookup tables
    \item \textbf{Language-agnostic keywords}: Identifying cognates and internationally recognized terms ("Manager", "Director", "Engineer")
    \item \textbf{Cross-lingual fuzzy matching}: Adapting similarity thresholds based on linguistic distance
\end{itemize}

\textbf{Context-Aware Matching:}
Current matching considers only job titles in isolation. Additional context could improve disambiguation,
\begin{itemize}
    \item \textbf{Company information}: Organization type and industry could inform department assignment (e.g., "Consultant" in IT company vs. consulting firm)
    \item \textbf{Career trajectory analysis}: Temporal patterns in job history could constrain seniority predictions
    \item \textbf{Title component parsing}: Decomposing compound titles into seniority prefix + function + department suffix
\end{itemize}

\textbf{Hybrid Rule-ML Approaches}
Combining rule-based matching with machine learning could leverage strengths of both paradigms,
\begin{itemize}
    \item \textbf{Confidence-based delegation}: Using ML models only when rule-based confidence is low
    \item \textbf{Active learning for lookup table expansion}: Identifying high-confidence ML predictions to add to lookup tables
    \item \textbf{Ensemble methods}: Combining rule-based predictions with embedding similarity scores
\end{itemize}

These enhancements would address the identified limitations while maintaining the computational efficiency and interpretability advantages of the rule-based approach.

\subsection{Methodological Considerations: The Fallback Dilemma}
\label{subsec:fallback-dilemma}
The choice of fallback defaults highlights a fundamental tension in applied machine learning between methodological rigor and practical system design.
\textbf{The Data Leakage Issue:}
Using ``Professional'' as the seniority fallback constitutes severe data leakage because,
\begin{itemize}
    \item The training data (CSV lexicons) contains \textbf{zero} ``Professional'' examples—this class does not exist in training
    \item The training lexicon has only 5 seniority classes: Senior (39.6\%), Lead (37.6\%), Director (10.4\%), Management (8.0\%), Junior (4.3\%)
    \item The evaluation data contains ``Professional'' as the most common class (34.7\%)
    \item The choice was informed entirely by inspection of the evaluation distribution, not derived from training data
    \item This gives the rule-based approach an unfair advantage: unmatched titles default to a class that only exists in the evaluation set
\end{itemize}
\textbf{Impact Quantification:}
To assess the impact, we can estimate how many predictions rely on the fallback,
\begin{itemize}
    \item Approximately 20-30\% of evaluation job titles have no direct lookup table match
    \item Of these unmatched titles, the fallback contributes to the final prediction
    \item Based on actual comparison, the fallback contributes approximately 15 percentage points to seniority accuracy and 40 percentage points to department accuracy
    \item Without this informed fallback, rule-based accuracy drops from 68.2\% to 28.2\% for departments and from 58.4\% to 43.1\% for seniority
\end{itemize}
\textbf{Two Versions Evaluated:}
To transparently assess the impact of this methodological choice, we implemented and compared two versions of the rule-based approach,
\begin{enumerate}
    \item \textbf{Basic (No Fallback)}: Returns ``None''/``Unclassified'' when no match is found, avoiding data leakage but producing incomplete predictions
    \item \textbf{Enhanced (With Fallback)}: Uses informed defaults (``Other'' for department, ``Professional'' for seniority), introducing data leakage but providing complete coverage
\end{enumerate}
The performance comparison reveals the substantial impact of the fallback strategy:

\begin{table}[h]
\centering
\caption{Impact of the fallback strategy on rule-based model performance}
\label{tab:fallback-impact}
\begin{tabular}{l l cc}
\toprule
\textbf{Task} & \textbf{Version} & \textbf{Accuracy} & \textbf{F1 Macro} \\
\midrule
Department & Basic (No Fallback)      & 0.282 & 0.444 \\
Department & Enhanced (With Fallback) & 0.682 & 0.553 \\
\midrule
Seniority  & Basic (No Fallback)      & 0.431 & 0.389 \\
Seniority  & Enhanced (With Fallback) & 0.584 & 0.548 \\
\bottomrule
\end{tabular}
\end{table}

The reported results in this paper use the \textit{Enhanced} version for practical reasons (complete prediction coverage), but readers should be aware that approximately half of the department performance and one-quarter of the seniority performance stems from the informed fallback strategy rather than successful matching.
\textbf{The Practical Necessity:}
Despite being methodologically problematic, fallback defaults are \textit{required} for production rule-based systems,
\begin{itemize}
    \item Unlike supervised models that can predict any class, rule-based systems must handle the ``no match'' case explicitly
    \item Real-world systems cannot return ``null'' or ``unknown''—stakeholders require a prediction for every input
    \item The choice is not whether to have a fallback, but which class to use as the default
\end{itemize}
\textbf{Alternative Approaches Considered:}
Several alternatives exist, each with trade-offs,
\begin{enumerate}
    \item \textbf{Most frequent training class}: Methodologically pure, but may not reflect test distribution (``Senior'' in training vs. ``Professional'' in evaluation)
    \item \textbf{Uniform random selection}: Unbiased but performs poorly on imbalanced data
    \item \textbf{``Unknown'' category}: Honest but impractical for downstream applications
    \item \textbf{Confidence-based rejection}: Defer to human review when confidence is low (not feasible for all use cases)
\end{enumerate}
\textbf{Fair Comparison with Other Models:}
Crucially, supervised learning models (TF-IDF, Feature Engineering, DistilBERT) do \textit{not} face this dilemma,
\begin{itemize}
    \item They are trained on the lookup tables, which contain the ``Professional'' class (albeit rarely)
    \item They learn class distributions from training data and naturally generalize to unseen inputs
    \item Their predictions for novel job titles are based on learned patterns, not informed defaults
    \item When comparing models, the rule-based approach's informed fallback provides a slight unfair advantage
\end{itemize}
\textbf{Disclosure and Transparency:}
In the interest of scientific integrity, we fully disclose this methodological compromise. The rule-based results should be interpreted with the understanding that,
\begin{itemize}
    \item The seniority performance (59.8\% accuracy) benefits from an informed default
    \item A methodologically pure version would score approximately 50-53\%
    \item This advantage is \textit{specific to rule-based systems} and does not apply to supervised models
    \item The choice reflects real-world engineering constraints rather than methodological oversight
\end{itemize}
This discussion illustrates that production ML systems often require pragmatic compromises that would be unacceptable in pure research settings. The key is \textit{transparency}: documenting these decisions allows readers to make informed comparisons and assess generalizability.

\section{Embedding-Based Baseline}
\label{sec:embedding-baseline}

\subsection{Motivation}
\label{subsec:embedding-motivation}

Having established a rule-based baseline, the next logical step is to explore whether semantic understanding can improve generalization beyond exact pattern matching. The rule-based approach struggles with novel job titles and requires exact or near-exact matches in the lookup tables. An embedding-based approach offers the promise of semantic generalization: if ``Software Developer'' and ``Programmer'' have similar meanings, their vector representations should be close in embedding space, enabling the model to make correct predictions even when the exact phrasing differs.

The motivation for attempting zero-shot embedding classification is twofold. First, it provides an upper bound on what can be achieved without supervised training on LinkedIn CVs—if semantic similarity alone suffices, more complex approaches may be unnecessary. Second, the multilingual nature of the dataset (approximately 40\% non-English) suggests that language-agnostic semantic representations could handle German, French, and Spanish titles more gracefully than English-centric keyword matching.

\subsection{Methodology}
\label{subsec:embedding-methodology}

The embedding-based classifier employs pre-trained multilingual sentence transformers to encode job titles and labels into dense vector representations. Classification is performed through cosine similarity matching between input embeddings and label prototype embeddings.

\paragraph{Model Selection}
The approach utilizes the \texttt{paraphrase-multilingual-MiniLM-L12-v2} sentence transformer model:
\begin{itemize}
    \item \textbf{Embedding dimension}: 384-dimensional dense vectors
    \item \textbf{Multilingual support}: Covers 50+ languages including English, German, French, and Spanish
    \item \textbf{Model size}: 118M parameters (~470MB)
\end{itemize}

\paragraph{Input Text Enrichment}
To maximize semantic information, input text is enriched beyond simple job titles by combining title, company, and job description (first 200 characters). This rich context provides additional signals for disambiguation.

\paragraph{Multi-Prototype Label Representations}
Instead of using single prototype embeddings per label, the approach employs K-Means clustering (k=3) to create multiple prototypes per label. This strategy captures intra-label diversity, accommodating different semantic variations within the same category (e.g., "Software Engineer" and "Backend Developer" both belong to IT but have different embeddings).

\paragraph{Prediction Process}
Classification uses the maximum cosine similarity across all prototypes for each label. The predicted label is the one with the highest similarity score.

\subsection{Performance Results}
\label{subsec:embedding-results}

The embedding-based model was evaluated on the same annotated LinkedIn CV dataset used for the rule-based baseline:

\begin{table}[h]
\centering
\caption{Embedding-based baseline performance}
\label{tab:embedding-performance}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.247 & 0.287 \\
Seniority & 0.364 & 0.238 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Underperformance vs. rule-based}: Department accuracy (24.7\%) and seniority accuracy (36.4\%) are substantially lower than the rule-based baseline (28.2\% and 43.1\% respectively on comparable metrics without fallback defaults)
    \item \textbf{Below naive baseline}: Performance is worse than simply predicting the most common class
    \item \textbf{Semantic understanding limitations}: Pre-trained embeddings struggle to capture domain-specific job title semantics
    \item \textbf{Generalization potential}: Unlike rule-based matching, embedding approach can theoretically handle completely novel job titles through semantic similarity
\end{itemize}

\subsection{Error Analysis}
\label{subsec:embedding-errors}

Analysis of misclassifications reveals systematic failure patterns:

\paragraph{Semantic Ambiguity}
The model struggles with titles that are semantically similar across different departments. For example, "Project Manager" could belong to IT, Consulting, or Project Management, and "Analyst" appears across Finance, IT, Marketing, and Consulting.

\paragraph{Multilingual Performance}
Despite using a multilingual model, English job titles achieve higher accuracy (~50\%) than German/French/Spanish titles (~40\%), indicating cross-lingual similarity challenges.

\paragraph{Surface Similarity Issues}
The model sometimes matches on word overlap rather than contextual understanding. For example, "Senior Engineer" might be incorrectly classified based on the word "Senior" rather than the full context.

\subsection{Conclusion}
\label{subsec:embedding-conclusion}

The embedding-based zero-shot approach demonstrates that pre-trained general-purpose embeddings are insufficient for this domain-specific classification task, achieving only 24.7\% accuracy for departments and 36.4\% for seniority.

\paragraph{Why It Underperforms.}
Several factors contribute to the disappointing results:
\begin{itemize}
    \item \textbf{Centroid collapse}: Averaging thousands of job title embeddings to create class prototypes loses discriminative information, resulting in generic representations
    \item \textbf{Domain mismatch}: The sentence transformer was trained on general text, not job titles—it understands that ``Manager'' and ``Director'' are related but cannot capture domain-specific nuances
    \item \textbf{Low-dimensional discrimination}: The 384-dimensional embedding space, while powerful for general semantic tasks, lacks the specificity needed to distinguish between subtle job title variations
\end{itemize}

\paragraph{Where It Could Succeed.}
The embedding approach would  excel in scenarios with:
\begin{itemize}
    \item \textbf{Fine-tuned embeddings}: Domain adaptation on job title data could dramatically improve performance
    \item \textbf{Larger, more diverse training data}: More examples per class would create more robust prototypes
    \item \textbf{Hybrid integration}: Using embeddings as features alongside other signals rather than as the sole classification mechanism
\end{itemize}

\paragraph{Key Insight.}
This experiment reveals an important lesson: powerful pre-trained models do not automatically solve domain-specific problems. Task-specific adaptation—whether through fine-tuning, feature engineering, or hybrid approaches—is necessary when the target domain differs significantly from the pre-training corpus. This motivates our exploration of supervised learning approaches in subsequent sections.

\section{TF-IDF + Logistic Regression (Lexicon-Supervised)}
\label{sec:tfidf-logreg}

\subsection{Motivation}
\label{subsec:tfidf-motivation}

Having explored zero-shot embedding similarity, the next logical step is to investigate whether classical machine learning with carefully designed text features can outperform both rule-based matching and embedding approaches. The TF-IDF (Term Frequency-Inverse Document Frequency) representation offers a middle ground: it captures word importance beyond simple keyword matching while remaining interpretable and computationally efficient.

The key constraint for this approach is lexicon-supervised learning: the model is trained exclusively on the lookup tables (approximately 19,000 labeled examples) without access to any LinkedIn CV data during training. This ensures a fair comparison with zero-shot methods and provides insight into how well traditional machine learning techniques can generalize across the domain gap.

\subsection{Methodology}
\label{subsec:tfidf-methodology}

The classifier combines TF-IDF vectorization with regularized Logistic Regression, employing both word-level and character-level n-grams to capture semantic and morphological patterns.

\paragraph{Feature Extraction}
The approach uses a hybrid feature space combining:
\begin{itemize}
    \item \textbf{Word n-grams (1-2)}: Captures semantic meaning through unigrams and bigrams (e.g., ``software engineer'', ``senior manager'')
    \item \textbf{Character n-grams (3-5)}: Handles morphological variations and typos, particularly useful for German compound words (e.g., ``entwickler'' = developer)
    \item \textbf{TF-IDF weighting}: Down-weights common terms while emphasizing discriminative words
\end{itemize}

Feature dimensions are capped at 5,000 word features and 3,000 character features for department classification to prevent overfitting, with slightly reduced dimensions (3,000 words, 2,000 characters) for seniority due to the simpler task structure.

\paragraph{Model Training}
Logistic Regression with L2 regularization (C=1.0) is trained on the vectorized lookup table entries. The regularization parameter was selected through cross-validation to balance bias and variance, prioritizing generalization over perfect in-distribution fit.

\paragraph{Text Preprocessing}
Minimal preprocessing is applied:
\begin{itemize}
    \item Lowercase normalization
    \item Whitespace cleanup
    \item Removal of common job title suffixes (``(m/w/d)'', ``(f/m/x)'')
\end{itemize}

This light preprocessing preserves informative variations while standardizing format inconsistencies.

\subsection{Performance Results}
\label{subsec:tfidf-results}

The TF-IDF + Logistic Regression approach was evaluated on the annotated LinkedIn CVs:

\begin{table}[h]
\centering
\caption{TF-IDF + Logistic Regression performance}
\label{tab:tfidf-performance}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.268 & 0.393 \\
Seniority & 0.474 & 0.430 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Competitive seniority performance}: 47.4\% accuracy for seniority exceeds both rule-based (43.1\%) and embedding (36.4\%) approaches
    \item \textbf{Modest department performance}: 26.8\% accuracy, similar to embedding but below rule-based
    \item \textbf{Strong macro F1 for departments}: 0.393 indicates more balanced performance across classes than raw accuracy suggests
    \item \textbf{Generalization advantage}: Outperforms embeddings despite using similar training data, demonstrating the value of discriminative learning
\end{itemize}

\subsection{Conclusion}
\label{subsec:tfidf-conclusion}

The TF-IDF + Logistic Regression approach demonstrates that classical machine learning with appropriate feature engineering can compete with modern embedding methods, achieving the best seniority classification performance among all non-transformer approaches.

\paragraph{Strengths.}
The approach excels at:
\begin{itemize}
    \item \textbf{Seniority classification}: Explicit seniority keywords (``senior'', ``junior'', ``director'', ``lead'') are weighted heavily by TF-IDF, enabling accurate predictions
    \item \textbf{Robustness to distribution shift}: Regularized linear models cannot memorize complex patterns, paradoxically making them better at generalizing when training and test distributions differ
    \item \textbf{Multilingualcharacter support}: Character n-grams capture morphological patterns in German titles (e.g., ``leiter'' = lead, ``entwickler'' = developer)
    \item \textbf{Interpretability}: Feature weights reveal which terms drive predictions, enabling model debugging and validation
\end{itemize}

\paragraph{Limitations.}
The model struggles with:
\begin{itemize}
    \item \textbf{Department classification}: The ``Other'' class and ambiguous titles reduce performance, as TF-IDF cannot distinguish context-dependent meanings
    \item \textbf{Novel terminology}: Emerging job titles not present in lookup tables (e.g., ``Growth Hacker'', ``DevOps Engineer'') lack corresponding features
    \item \textbf{Semantic reasoning}: Cannot infer that ``Software Developer'' and ``Programmer'' are synonyms without explicit evidence in training data
\end{itemize}

\paragraph{Why This Performance Level.}
The strong seniority performance reflects the inherent task structure: seniority levels have consistent lexical markers across languages and industries (``Senior'', ``Director'', ``Lead'', ``VP''). TF-IDF naturally assigns high weights to these discriminative terms. In contrast, department classification requires understanding subtle contextual differences—``Manager'' could indicate Sales, IT, HR, or Other depending on surrounding words—which TF-IDF's bag-of-words representation cannot capture.

The key insight is that classical ML with appropriate features often outperforms zero-shot methods when explicit discriminative signals exist. This motivates our exploration of supervised transformer models that can learn task-specific representations.


\section{Feature Engineering with Random Forest}
\label{sec:feature-engineering}

\subsection{Motivation}
\label{subsec:feature-eng-motivation}

Having observed that all previous approaches struggle with the domain gap between clean lookup tables and messy real-world LinkedIn data, we hypothesized that the problem lies not in model sophistication but in feature quality. Text-based features (TF-IDF, embeddings) are sensitive to phrasing variations and domain shift. What if we could engineer domain-invariant features that transfer robustly from training to test data?

The motivation for feature engineering is that career trajectories exhibit patterns that transcend job title wording. For instance, someone with 10 years of experience and three previous positions is likely senior, regardless ofwhether their title says ``Senior Engineer'' or ``Lead Developer.'' Similarly, keywords like ``sales,'' ``engineer,'' or ``analyst'' indicate departments more reliably than exact title matches.

\subsection{Methodology}
\label{subsec:feature-eng-methodology}

\paragraph{Career-Based Feature Extraction}

Rather than relying solely on text embeddings, we engineered domain-invariant features that capture career trajectory patterns:

\begin{itemize}
    \item \textbf{Title structure features}: Word count, character length, presence of seniority indicators (Jr., Sr., Lead, etc.)
    \item \textbf{Keyword density}: Frequency of domain-specific terms (technical, business, management keywords)
    \item \textbf{Career trajectory}: Number of previous positions, tenure patterns, career progression indicators
    \item \textbf{Organization context}: Company name patterns, industry indicators
    \item \textbf{Temporal features}: Position duration, career gaps, employment continuity
\end{itemize}

\paragraph{Model Configuration}

The Random Forest classifier was configured with:
\begin{itemize}
    \item 200 estimators with max depth of 15
    \item Balanced class weights to address imbalanced department distribution
    \item Stratified 5-fold cross-validation for hyperparameter selection
\end{itemize}

\subsection{Performance Results}
\label{subsec:feature-eng-results}

\begin{table}[h]
\centering
\caption{Feature Engineering performance on real-world evaluation set}
\label{tab:feature-eng-performance}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.282 & 0.249 \\
Seniority & 0.473 & 0.437 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding}: Feature Engineering achieved a \textbf{Department F1 score of 0.249} and a \textbf{Seniority F1 score of 0.437} when evaluated on the full real-world dataset (including the dominant "Other" class). While the model captures structural signals, its performance for functional domains is heavily influenced by the 'Other' category distribution in real-world CVs.

\subsection{Conclusion}
\label{subsec:feature-eng-conclusion}

The Feature Engineering approach demonstrates a fundamental insight: while domain knowledge encoded in structural features (like tenure and position count) provides a robust floor for seniority prediction, text-based department classification remains sensitive to vocabulary shift.

\paragraph{Strengths.}
The approach excels because:
\begin{itemize}
    \item \textbf{Career trajectory robustness}: Career patterns transfer better than text patterns across domains for seniority evaluation.
    \item \textbf{Generalization floor}: Features like ``number of previous positions'' or ``years of experience'' provide consistent signals across data sources, achieving 0.437 F1 for seniority.
    \item \textbf{Interpretability}: Feature importance analysis reveals that seniority keywords and career duration dominate the model's decision-making process.
\end{itemize}

\paragraph{Limitations.}

The model struggles with:
\begin{itemize}
    \item \textbf{Department performance with "Other"}: The F1 score is 0.249 when the "Other" class is included, indicating that structural features alone cannot easily distinguish specific functional domains from the "Other" majority when the lexicon is clean.
    \item \textbf{Feature engineering effort}: Requires manual design, limiting scalability to new classification schemas compared to end-to-end models.
    \item \textbf{Missing semantic understanding}: Does not capture the full semantic depth of job titles that larger transformers might learn.
\end{itemize}

\paragraph{Why This Performance Level.}

Departments are determined by function-specific vocabulary. While keyword density features capture some of this, they lack the flexibility of n-gram or transformer representations. Seniority, being a more structural concept (tenure, level), is better suited for this feature-based approach than the semantic task of department classification.

\paragraph{Key Lesson.}
This result challenges the common assumption that ``more complex models are always better.'' When training and test distributions differ substantially, simpler models with domain-adapted features often outperform end-to-end learned representations. Deep learning excels at extracting patterns from similar data distributions; feature engineering excels at encoding human knowledge that generalizes across distribution shifts.


\section{Pseudo-Labeling (Semi-Supervised Learning)}
\label{sec:pseudo-labeling}
\subsection{Motivation}
\label{subsec:pseudo-motivation}
Standard supervised learning is limited by the availability of labeled training data. In our setting, we have access to high-quality labels from the lookup tables but face a domain gap when applying these models to LinkedIn CVs. However, we also possess a large dataset of unannotated LinkedIn profiles (\texttt{linkedin-cvs-not-annotated.json}) that significantly overlaps in distribution with our target evaluation set.
Pseudo-labeling is a semi-supervised technique that leverages this unlabeled data. The core idea is to train a model on the labeled data (CSV's), use it to predict labels for the unlabeled data (generating "pseudo-labels"), and then retrain the model on the combined dataset. This approach hypothesizes that the model can bootstrap its own performance by learning from confident predictions on the target distribution, potentially bridging the domain gap by exposing the model to the specific linguistic patterns of LinkedIn profiles.
\subsection{Methodology}
\label{subsec:pseudo-methodology}
Our pseudo-labeling pipeline consists of three phases:
\paragraph{Phase 1: Initial Training}
We first established a reference performance by training a transformer-based model on the structured lookup table data (methodology detailed in Section~\ref{sec:distilbert-models}). This supervised model serves as the teacher for the pseudo-labeling pipeline, providing initial estimates that are then refined through exposure to unannotated LinkedIn data.
\paragraph{Phase 2: Label Generation and Filtering}
We applied this teacher model to the 390 unannotated LinkedIn CVs. To avoid reinforcing errors, we filtered the predictions based on confidence scores. Only predictions where the model assigned a probability greater than 0.9 were retained as high-quality pseudo-labels. This threshold ensures that we only incorporate samples where the model is highly certain, reducing the risk of "confirmation bias" where the model learns its own mistakes.
\paragraph{Phase 3: Retraining}
Finally, we combined the original labeled lookup table data with the newly generated pseudo-labeled examples and retrained the DistilBERT model from scratch. This expanded training set aims to combine the reliability of the ground truth labels with the distributional relevance of the LinkedIn data.
\subsection{Performance Results}
\label{subsec:pseudo-results}
The pseudo-labeling approach was evaluated on the annotated hold-out set:
\begin{table}[h]
\centering
\caption{Pseudo-Labeling performance}
\label{tab:pseudo-performance}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.444 & 0.438 \\
Seniority & 0.483 & 0.437 \\
\bottomrule
\end{tabular}
\end{table}
\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{High Department Generalization}: The department classification achieved an F1 of 0.438, which substantially outperforms the structural feature-engineering approach (F1=0.249) and is comparable to the rule-based baseline without manual fallback logic (F1=0.444).
    \item \textbf{Improved Seniority Adaptation}: For seniority, the approach achieved an F1 of 0.437, aligning closely with the performance of curated feature engineering (F1=0.437) and significantly exceeding the basic rule-based matching (F1=0.389).
\end{itemize}
\subsection{Conclusion}
\label{subsec:pseudo-conclusion}
Pseudo-labeling yielded consistently positive results across both tasks, establishing a robust middle ground between rigid keyword matching and complex feature engineering. By bridging the domain gap through target-distribution exposure, the model adapted to real-world LinkedIn patterns more effectively than models trained solely on structured lexicons. This highlights a key advantage of semi-supervised techniques: they allow the system to learn the specific linguistic patterns of the target domain even when the primary training source is clean but incomplete. For production systems, pseudo-labeling offers a viable path to improved generalization without the need for additional manual annotation.

\section{DistilBERT Fine-Tuning Approaches}
\label{sec:distilbert-models}
\subsection{Motivation}
\label{subsec:distilbert-motivation}
After exploring feature engineering, we turned to transformer models to investigate whether large-scale pre-trained language models could learn task-specific representations that outperform hand-crafted features. Due to the multilingual nature of the dataset (German, English, and additional languages) and the requirement for efficient inference, we selected \textbf{DistilBERT} in its multilingual variant (\texttt{distilbert-base-multilingual-cased}).
DistilBERT is based on the principle of \textit{knowledge distillation}, where a smaller and more efficient model (the student) is trained to reproduce the behavior of a larger teacher model (e.g., BERT). Instead of learning from hard labels alone, the student is guided by so-called \textit{soft targets}, i.e., probability distributions produced by the teacher model, which transfer generalized linguistic knowledge. DistilBERT reduces the number of parameters by approximately 40\% and accelerates inference by up to 60\%, while retaining around 97\% of the original performance of BERT. This makes it particularly suitable for processing short text fragments such as job titles efficiently.
The primary challenge identified in earlier approaches is severe class imbalance, particularly for department classification where the ``Other'' class comprises 55\% of the dataset. This imbalance causes models to over-predict the majority class, resulting in poor macro F1 scores. The motivation for this comprehensive experiment is to systematically evaluate five different strategies for addressing class imbalance, ranging from simple class weighting to hierarchical two-stage classification.
\subsection{Methodology}
\label{subsec:distilbert-methodology}
\paragraph{Model Architecture.}
All experiments used \texttt{distilbert-base-multilingual-cased} with the following specifications:
\begin{itemize}
    \item 134M parameters (6 transformer layers)
    \item Supports 104 languages (critical for multilingual job titles)
    \item 40\% smaller and 60\% faster than BERT-base while retaining 97\% performance
    \item Maximum sequence length: 64 tokens (sufficient for job titles)
\end{itemize}
The model represents input sequences through a combination of token, segment, and positional embeddings. Unlike unidirectional architectures, DistilBERT learns context-dependent representations via the self-attention mechanism by simultaneously considering both left and right context of each token. This is especially critical for distinguishing job titles whose meaning may vary depending on context (e.g., ``Manager'' in IT versus Sales).
\paragraph{Training Infrastructure.}
Model training was conducted using the \texttt{Transformers} library provided by HuggingFace. The common baseline setup consisted of:
\begin{itemize}
    \item 80/20 train--test split with fixed random seed for reproducibility
    \item \texttt{AutoTokenizer} with maximum sequence length of 64 tokens and truncation enabled
    \item \textbf{Optimizer}: AdamW with weight decay 0.01
    \item \textbf{Learning rate schedule}: Linear warmup (6\% of steps) followed by linear decay
    \item \textbf{Early stopping}: Patience 2-3 epochs, monitoring macro F1
    \item \textbf{Best model selection}: Load checkpoint with highest validation F1
\end{itemize}
\paragraph{Class Imbalance Strategies.}
We systematically evaluated five different approaches to handle class imbalance:
\textbf{Approach 1: Baseline (Standard Fine-Tuning)}
Separate models for \textit{Department} and \textit{Seniority} prediction were trained without any explicit adjustment for class imbalance:
\begin{itemize}
    \item Learning rate: $2 \times 10^{-5}$ with linear decay
    \item Batch size: 64, up to 20 epochs with early stopping (patience=3)
    \item No special imbalance handling
\end{itemize}
This configuration served as a reference point for subsequent optimizations.
\textbf{Approach 2: Class Balancing (Weighted Loss)}
To reduce the dominance of majority classes (e.g., ``Other'' in the Department task), a custom \texttt{WeightedTrainer} was implemented. Class weights were incorporated into the standard cross-entropy loss, resulting in a weighted formulation:
\begin{equation}
\mathcal{L}_{WCE} = - \sum_{c=1}^{C} w_c \, y_c \log(p_c),
\end{equation}
where $y_c$ denotes the one-hot encoded true label, $p_c$ the predicted probability for class $c$, and $w_c$ a weight factor computed as:
\begin{equation}
w_c = \frac{N}{K \cdot n_c}
\end{equation}
where $N$ is total samples, $K$ is number of classes, and $n_c$ is samples in class $c$. Misclassifications of minority classes were thus penalized more strongly, ensuring that underrepresented categories exerted greater influence during gradient updates.
\textbf{Approach 3: Oversampling (Custom Strategy)}
The most effective method proved to be a custom oversampling strategy. Minority class instances were duplicated in order to align class distributions with the median class size:
\begin{itemize}
    \item Target: All classes reach median count (approximately 620 per class)
    \item Training set expansion: 8,116 $\rightarrow$ approximately 6,800 samples
\end{itemize}
Unlike undersampling, which risks discarding valuable information, oversampling allows the model to learn more robust patterns for rare categories.
\textbf{Approach 4: Combined Approach}
A hybrid strategy combining oversampling with weighted loss was evaluated to determine whether additional improvements could be achieved through simultaneous balancing techniques.
\textbf{Approach 5: Two-Stage Hierarchical Classification}
A hierarchical classification approach was tested for the Department prediction task, decomposing the problem into two stages:
\textit{Stage 1: Binary Classification (Other vs. NotOther)}
\begin{itemize}
    \item Separate ``Other'' (55\% of data) from all specific departments
    \item Custom \texttt{WeightedTrainer} with class weights
    \item Validation accuracy: 99.95\%, F1: 0.998+
\end{itemize}
\textit{Stage 2: Multi-class Classification (Specific Departments)}
Only instances predicted as ``NotOther'' are forwarded to Stage 2, where a multi-class classifier assigns the input to one of the remaining specific departments (e.g., Sales, Marketing, Purchasing, Administrative):
\begin{itemize}
    \item Classify samples predicted as ``NotOther'' into 10 specific departments
    \item \texttt{FocalLoss} ($\gamma=2.0$) to emphasize difficult examples during training
    \item Validation accuracy: 99.25\%, F1: 0.998+
\end{itemize}
Stage 2 employs \textit{focal loss} to down-weight well-classified samples:
\begin{equation}
\mathcal{L}_{FL}(p_t) = -\alpha (1 - p_t)^{\gamma} \log(p_t),
\end{equation}
where $p_t$ denotes the predicted probability of the true class, $\alpha$ is a balancing factor, and $\gamma$ controls the strength of focusing on misclassified instances.
\textit{Confidence-Based Thresholding:} A confidence threshold sweep between 0.2 and 0.8 was performed. The optimal threshold (TH2=0.7) was determined via grid search—samples with Stage 2 confidence below threshold are reclassified as ``Other''.
\textbf{Hyperparameter Selection:}
Since accuracy can be misleading in imbalanced settings, the \textbf{macro F1-score} was defined as the primary evaluation metric (\texttt{metric\_for\_best\_model}). Key hyperparameters were optimized:
\begin{itemize}
    \item \textbf{Learning Rate:} Values in the range of $1 \times 10^{-5}$ to $2 \times 10^{-5}$ were tested. Small learning rates are essential during fine-tuning to avoid catastrophic forgetting.
    \item \textbf{Batch Size:} 32 and 64 were evaluated. Larger batches yield more stable gradient estimates.
    \item \textbf{Epochs:} Maximum 20, with early stopping dynamically shortening training.
\end{itemize}
Based on experimental results, the \textbf{oversampling strategy (Approach 3)} was identified as the most effective configuration. The final hyperparameters differed slightly between tasks:
\noindent\textbf{Department Model:}
\begin{itemize}
    \item Learning Rate: $2 \times 10^{-5}$, Batch Size: 64, Epochs: 20 (early stopping)
    \item Strategy: Custom oversampling to median class size
\end{itemize}
\noindent\textbf{Seniority Model:}
\begin{itemize}
    \item Learning Rate: $1 \times 10^{-5}$, Batch Size: 64, Epochs: 20 (early stopping)
    \item Strategy: Custom oversampling
\end{itemize}
The reduced learning rate for the Seniority task suggests that distinguishing seniority levels required more fine-grained weight adjustments.
\subsection{Performance Results}
\label{subsec:distilbert-results}
\begin{table}[h]
\centering
\caption{DistilBERT comparison across all approaches}
\label{tab:distilbert-comparison}
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Approach} & \textbf{Dept F1 (macro)} & \textbf{Sen F1 (macro)} \\
\midrule
Baseline & 0.327 & 0.404 \\
Class Balancing & 0.343 & -- \\
Oversampling & 0.344 & \textbf{0.414} \\
Combined & 0.346 & -- \\
Two-Stage v2 & \textbf{0.534} & -- \\
\bottomrule
\end{tabular}
\end{table}
\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Significant Seniority Drop}: Unlike in-distribution results, seniority F1 macro drops to approximately 0.40--0.41 in real-world evaluation due to the domain gap (missing "Professional" label).
    \item \textbf{High department accuracy}: Two-Stage v2 reached 68.1\% accuracy, achieving the highest F1 macro (0.534) for this task.
    \item \textbf{Oversampling benefit}: Shows a slight improvement in department F1 over baseline, though the hierarchical approach remains vastly superior.
    \item \textbf{Class weighting vs. Data Level}: The marginal differences suggest that architectural changes (Two-Stage) are more effective than loss weighting for this specific label distribution mismatch.
\end{itemize}
Overall, while the oversampling approach provides a modest improvement over the standard baseline for department classification, the two-stage hierarchical architecture remains the superior configuration for this task, effectively balancing predictive power for specific domains with the ability to manage the dominant "Other" category.
\subsection{Conclusion}
\label{subsec:distilbert-conclusion}
The DistilBERT experiments demonstrate that transformer models excel at seniority classification but struggle with department classification, even with sophisticated imbalance-handling techniques.

\textbf{Strengths:}
DistilBERT performs exceptionally well when:
\begin{itemize}
    \item \textbf{High in-distribution capability}: The model learns the lexicon patterns with nearly 100\% accuracy.
    \item \textbf{Multilingual support}: The multilingual DistilBERT handles German, French, and Spanish titles without explicit language-specific engineering.
    \item \textbf{Efficient Inference}: The distilled architecture allows for high throughput, suitable for production screening.
\end{itemize}

\paragraph{Limitations:}
The model struggles with,
\begin{itemize}
    \item \textbf{Missing labels in training}: The model fails to recognize the "Professional" class (34.7\% of evaluation data) because it was entirely absent from the clean lexicons.
    \item \textbf{Domain gap}: Real-world test performance drops significantly compared to in-distribution validation, revealing a lack of robustness to novel label distributions.
    \item \textbf{Department complexity}: Even with two-stage classification, the 55\% "Other" class remains a significant challenge for balanced performance.
\end{itemize}

\textbf{Why This Performance Pattern:}
Seniority classification benefits from two factors: (1) explicit lexical markers that appear consistently across domains, and (2) reasonable class balance enabling the model to learn all classes. Department classification, by contrast, requires understanding context-dependent terminology (``Manager'' means different things in different departments) while handling extreme class imbalance—a combination that proves challenging even for state-of-the-art transformers.

\textbf{Key Insight:}
The fundamental problem is not model capacity but the domain gap between training data (clean, structured CSV lookup tables) and test data (noisy, real-world LinkedIn profiles). Transformers trained on clean data overfit to the training distribution's patterns, performing poorly when the test distribution differs. This explains why feature engineering—which explicitly encodes domain-invariant knowledge—outperforms transformers for department classification despite having far fewer parameters.

\textbf{Practical Recommendation:}
For production systems facing similar domain shift challenges: (1) Use Two-Stage Transformer models for department classification to handle majority class dominance and hierarchical complexity, (2) Use transformers for tasks with consistent lexical patterns and balanced classes, (3) Reserve feature engineering for specific scenarios where lexical overlap is minimal but structural features are strong.


\section{Experiments}
\label{sec:experiments}

This section presents the systematic comparison of all implemented approaches, using consistent evaluation protocols and metrics.

\subsection{Experimental Setup}
\label{subsec:experimental-setup}

\paragraph{Training Data}
Models were trained on the provided lookup tables:
\begin{itemize}
    \item Department: 10,145 text-label pairs across 11 classes
    \item Seniority: 9,428 text-label pairs across 5 classes
\end{itemize}

\paragraph{Evaluation Data}
All models were evaluated on the annotated LinkedIn CVs (623 active positions), representing real-world distribution with significant domain shift from training data.

\paragraph{Metrics}
\begin{itemize}
    \item \textbf{Accuracy}: Overall correct predictions / total predictions
    \item \textbf{Macro F1}: Unweighted average F1 across all classes (treats minority classes equally)
    \item \textbf{Weighted F1}: F1 weighted by class frequency (reflects overall performance)
\end{itemize}

\subsection{Quantitative Results}
\label{subsec:quantitative-results}

\begin{table}[h]
\centering
\caption{Comprehensive comparison of all approaches on evaluation set}
\label{tab:final-comparison}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Dept Acc} & \textbf{Dept F1} & \textbf{Sen Acc} & \textbf{Sen F1} \\
\midrule
\multicolumn{5}{l}{\textit{Baseline Approaches}} \\
Rule-Based (Basic) & 0.282 & 0.444 & 0.431 & 0.389 \\
Embedding Zero-Shot & 0.247 & 0.287 & 0.364 & 0.238 \\
Hybrid (Rule+Embed) & 0.449 & 0.404 & 0.465 & 0.434 \\
Pseudo-Labeling & 0.444 & 0.438 & 0.483 & 0.437 \\
\midrule
\multicolumn{5}{l}{\textit{Classical ML}} \\
TF-IDF + LogReg & 0.268 & 0.393 & 0.474 & 0.430 \\
Feature Engineering & 0.282 & 0.249 & 0.473 & \textbf{0.437} \\
\midrule
\multicolumn{5}{l}{\textit{Transformer-based (DistilBERT)}} \\
Baseline & 0.278 & 0.327 & 0.451 & 0.404 \\
Class Balancing & 0.274 & 0.343 & -- & -- \\
Oversampling & 0.276 & 0.344 & 0.464 & 0.414 \\
Combined & 0.268 & 0.346 & -- & -- \\
Two-Stage v2 & \textbf{0.681} & \textbf{0.534} & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{enumerate}
    \item \textbf{Department Classification}: Two-Stage DistilBERT achieves the best F1 (0.534) and accuracy (68.1\%) overall, effectively handling the "Other" class.
    \item \textbf{Seniority Classification}: Feature Engineering (Improved) outperforms deep learning models (F1=0.443) by being more resilient to labels missing from the training data (e.g., "Professional").
    \item \textbf{The Majority Class Challenge}: All models struggle with the weighted Macro F1 when the 55\% "Other" class is included, highlighting the need for more representative training data for non-target roles.
\end{enumerate}

\subsection{Error Analysis}
\label{subsec:error-analysis}

\subsubsection{Department Classification Errors}

The confusion matrix reveals systematic error patterns:

\begin{itemize}
    \item \textbf{``Other'' confusion}: Many specific departments incorrectly classified as ``Other'' and vice versa
    \item \textbf{IT vs. Consulting overlap}: Technical consultants frequently misclassified
    \item \textbf{Business Development vs. Sales}: Similar terminology causes boundary confusion
    \item \textbf{Administrative}: Low recall due to diverse job titles (``Office Manager'', ``Executive Assistant'', etc.)
\end{itemize}

\subsubsection{Seniority Classification Errors}

\begin{itemize}
    \item \textbf{Management vs. Lead confusion}: Organizational hierarchy varies by company size
    \item \textbf{Senior vs. Professional boundary}: Context-dependent distinction not captured by title alone
    \item \textbf{Director underrepresentation}: Only 5.5\% of training data, causing lower recall
\end{itemize}

\subsubsection{Domain Shift Analysis}

The significant performance gap between in-distribution (lookup table validation) and real-world (annotated CVs) evaluation reveals domain shift:

\begin{table}[h]
\centering
\caption{In-distribution vs. Real-world performance gap}
\footnotesize
\label{tab:domain-shift}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{In-Dist Acc} & \textbf{Real-World Acc} & \textbf{Gap} \\
\midrule
Rule-Based & 1.000 & 0.302 & -69.8\% \\
DistilBERT Baseline (Seniority) & 0.999 & 0.451 & -54.8\% \\
Feature Engineering & 0.941 & 0.282 & -65.9\% \\
\bottomrule
\end{tabular}
\end{table}

Feature Engineering shows the smallest domain shift gap, explaining its superior real-world performance despite lower in-distribution accuracy, indicating that it overfits the least. 



\section{Discussion and Analysis}
\label{sec:discussion}

\subsection{Model Comparison and Performance Analysis}
\label{subsec:model-comparison}

The experimental results reveal several counter-intuitive findings that merit deeper analysis.

\paragraph{Why Two-Stage DistilBERT Leads Department Classification}

The Two-Stage DistilBERT approach (F1=0.535) outperformed the other aproches, if we don't account for the Rule Based fallback. The success of the hierarchical strategy is attributed to:

\begin{itemize}
    \item \textbf{Separation of Concerns}: Identifying whether a title is a "target" department before classifying the specific department reduces noise.
    \item \textbf{Semantic Depth}: Transformers capture nuances in LinkedIn titles (multilingual, creative) that fixed keyword features struggle to normalize.
    \item \textbf{Generalization}: Despite the domain shift, the transformer's broader linguistic priors provide a more robust baseline than rigid feature-based sets when dealing with the high diversity of the "Other" class.
\end{itemize}

\paragraph{Why Transformers Struggle with Unseen Labels (Seniority)}
\label{subsec:transformers-struggle}

The experimental results show that DistilBERT Baseline (F1=0.404) is outperformed by Feature Engineering (F1=0.443) for seniority classification. This observation reveals a critical vulnerability of supervised transformers to distribution shift in labels:

\begin{itemize}
    \item \textbf{Missing "Professional" Label}: The clean training lexicons contain zero examples of the "Professional" class, which accounts for 34.7\% of the evaluation data.
    \item \textbf{Over-Confidence in Seen Classes}: Since the model was only exposed to classes like "Senior", "Junior", and "Lead" during training, it attempts to force real-world "Professional" profiles into these categories based on lexical overlap, leading to significant misclassification.
    \item \textbf{Lack of Structural Awareness}: Unlike Feature Engineering, which uses "years of experience" or "number of positions" as robust proxies, the transformer relies almost exclusively on title-text patterns. In the absence of specific keywords it was trained on, its semantic generalization fails to bridge the gap to the missing label.
\end{itemize}

\subsection{Final Evaluation and Metrics Interpretation}
\label{subsec:metrics-interpretation}
To correctly interpret the feasibility of the proposed system, stakeholders must look beyond top-line accuracy numbers.
\paragraph{The Primacy of Macro F1}
For department classification, \textbf{Accuracy is a misleading metric}. Due to the 55\% prevalence of the "Other" category in the department classification, a trivial "dummy" classifier predicting "Other" for every input would achieve 55\% accuracy but offer zero business value.
\begin{itemize}
    \item A Macro F1 of 0.535(Two-Stage DistilBERT) implies that while the model faces challenges with the dominant "Other" class, it maintains the highest discriminative power among all tested variants.
    \item This metric confirms that identifying specific departments remains the primary bottleneck for the career pathing system.
\end{itemize}
\paragraph{Seniority Reliability Factors}
For seniority, the moderate Macro F1 (0.443 for Feature Engineering, 0.404 for DistilBERT) reflects the inherent difficulty of the domain gap. The primary bottleneck is the "Professional" class mismatch between training and test sets. While predictive accuracy for seen classes remains high, the overall system reliability in a real-world LinkedIn setting is hampered by the inability of models trained purely on lexicons to recognize classes that are dominant in the wild but absent in the training source. This suggests that future iterations must incorporate target-domain samples into the training loop, possibly through semi-supervised or human-in-the-loop strategies.

\subsection{The DistilBERT Iteration Journey}
\label{subsec:distilbert-journey}

Our systematic exploration of five DistilBERT configurations demonstrates the scientific methodology applied throughout this project:

\begin{enumerate}
    \item \textbf{Baseline} $\rightarrow$ Established performance floor (Dept F1=0.327)
    \item \textbf{Class Balancing} $\rightarrow$ Marginal improvement for departments
    \item \textbf{Oversampling} $\rightarrow$ Better trade-off for minority departments
    \item \textbf{Combined} $\rightarrow$ No additive benefit
    \item \textbf{Two-Stage} $\rightarrow$ Breakthrough for department classification (F1=0.534)
\end{enumerate}

This iterative refinement illustrates that class imbalance in NLP requires problem-specific solutions beyond standard techniques.

\subsection{Impact of Preprocessing and Feature Choices}
\label{subsec:preprocessing-impact}

Several preprocessing decisions significantly impacted performance:

\begin{itemize}
    \item \textbf{Text normalization}: Lowercasing and whitespace normalization improved rule-based matching by 15-20\%
    \item \textbf{Fallback strategies}: The choice of default class (``Other'' for department, ``Professional'' for seniority) has substantial impact on accuracy
    \item \textbf{Sequence length}: 64 tokens sufficient for job titles; longer context (job descriptions) provided minimal benefit
\end{itemize}

\subsection{Limitations and Threats to Validity}
\label{subsec:limitations}

\paragraph{Data Limitations}
\begin{itemize}
    \item \textbf{Annotation quality}: Human labelers may disagree on borderline cases, especially for seniority
    \item \textbf{Class definition ambiguity}: The ``Other'' department is a catch-all with heterogeneous contents
    \item \textbf{Sample size}: 623 evaluation samples limit statistical power for minority classes
\end{itemize}

\paragraph{Methodological Limitations}
\begin{itemize}
    \item \textbf{Single evaluation set}: Results may not generalize to other LinkedIn data distributions
    \item \textbf{No temporal validation}: Job title conventions evolve; models may become outdated
    \item \textbf{Limited hyperparameter search}: Computational constraints prevented exhaustive optimization
    \item \textbf{Data leakage via informed defaults}: The rule-based fallback approach uses ``Professional'' as the seniority fallback, a class that does not exist in the training data (0\%) but represents 34.7\% of evaluation data. Comparison of Basic (no fallback) vs. Enhanced (with fallback) versions shows this provides an unfair advantage for accuracy. While practically necessary for production systems, this constitutes methodological data leakage that should be considered when comparing across approaches. Supervised models do not face this issue as they learn to predict from the training distribution, but consequently struggle when that distribution omits major real-world classes. See Section~\ref{subsec:fallback-dilemma} for detailed discussion.
\end{itemize}

\paragraph{External Validity}
\begin{itemize}
    \item \textbf{Cultural/regional bias}: Dataset reflects specific geographic and industry distributions
    \item \textbf{LinkedIn-specific formatting}: Models may not transfer to other resume formats
\end{itemize}

\section{Recommended System Design}
\label{sec:system-design}
Based on the extensive empirical evaluation and error analysis presented in this report, we propose a hybrid system architecture for the final production deployment. This design selects the optimal model for each task by balancing predictive performance, robustness to domain shift, and operational constraints.
\paragraph{Department Prediction: Two-Stage DistilBERT}
We recommend the \textbf{Two-Stage DistilBERT} approach (Section \ref{sec:distilbert-models}) for department classification.
\begin{itemize}
    \item \textbf{Justification}: This model achieved the highest Macro F1 score (0.535) and accuracy (68.5\%) in the standardized real-world evaluation. Its hierarchical design effectively addresses the dominance of the "Other" class by first performing structural filtering before specific domain assignment.
    \item \textbf{Operational}: While requiring GPU acceleration for optimal inference throughput, the model's predictive power significantly outweighs the simpler feature-based approaches (F1: 0.535 vs 0.249).
    \item \textbf{Integration}: We recommend using the Pseudo-Labeling model (Section \ref{sec:pseudo-labeling}) as a high-confidence alternative. This model achieved the second-best Macro F1 (0.438) and demonstrates improved generalization by learning directly from target-domain distributions. This replaces the problematic rule-based fallback mentioned in Section \ref{subsec:fallback-dilemma}.
\end{itemize}
\paragraph{Seniority Prediction: Feature Engineering (Improved)}
We recommend the \textbf{Feature Engineering} approach (Section \ref{sec:feature-engineering}) for seniority classification.
\begin{itemize}
    \item \textbf{Justification}: This model achieved the highest Macro F1 (0.443) on the full evaluation set. Its use of structural features (years of experience, counts of previous roles) makes it more resilient to the "Professional" label mismatch than text-only transformers.
    \item \textbf{Operational}: Highly efficient and interpretable, making it ideal for large-scale screening where explanation of seniority level is required.
    \item \textbf{Limitations}: While more robust than DistilBERT, it still suffers from the training-set missing labels; however, its failure mode is more predictable than the semantic misalignments of the transformer.
\end{itemize}
\paragraph{Approaches Not Recommended}
We explicitly advise against deploying:
\begin{itemize}
    \item \textbf{Zero-Shot Embeddings}: With performance below the naive baseline, general-purpose embeddings lack the domain specificity required for this task.
    \item \textbf{Pure System-Wide Transformers}: Using DistilBERT for \textit{both} tasks is suboptimal. It adds unnecessary compute cost for the department task where it performs worse than the simpler Random Forest model.
\end{itemize}

\section{Conclusion and Summary of Key Findings}
\label{sec:conclusion}

This capstone project developed and systematically compared multiple approaches for predicting career domain and seniority from LinkedIn CVs. Through extensive experimentation across 12 distinct methods, we identified optimal solutions for each classification task.

\subsection{Summary of Results}

\paragraph{Department Classification}
\begin{itemize}
    \item \textbf{Best Accuracy}: Two-Stage DistilBERT (68.5\%)
    \item \textbf{Best F1 Score}: Two-Stage DistilBERT (0.535)
    \item \textbf{Recommendation}: Use Two-Stage DistilBERT for primary classification, Pseudo-Labeling for high-confidence scenarios
\end{itemize}

\paragraph{Seniority Classification}
\begin{itemize}
    \item \textbf{Best Accuracy}: Feature Engineering (47.3\%)
    \item \textbf{Best F1 Score}: Feature Engineering (0.443)
    \item \textbf{Recommendation}: Use Feature Engineering for its robustness to distribution shifts in labels.
\end{itemize}

\subsection{Key Insights}

\begin{enumerate}
    \item \textbf{Hierarchical models outperform single-stage approaches when dealing with extreme majority classes}. The Two-Stage DistilBERT approach achieved the best department metrics by isolating the "Other" class.
    
    \item \textbf{Class imbalance requires problem-specific solutions}. Standard techniques (class weights, oversampling) had minimal impact; the Two-Stage hierarchical approach effectively addressed the 55\% ``Other'' class dominance.
    
    \item \textbf{Simpler models can outperform complex ones}. DistilBERT Baseline was the best seniority classifier, outperforming class-balanced and oversampled variants.
    
    \item \textbf{Accuracy and F1 measure different aspects}. The choice between Two-Stage (high accuracy) and Feature Engineering (high F1) depends on the application requirements.
\end{enumerate}

\subsection{Practical Recommendations}

For production deployment, we recommend:
\begin{itemize}
    \item \textbf{Department}: Two-Stage DistilBERT model for optimal semantic and hierarchical processing
    \item \textbf{Seniority}: DistilBERT Baseline for optimal performance
    \item \textbf{Fallback strategy}: Pseudo-Labeling as a high-performance alternative for department prediction
\end{itemize}

\subsection{Future Work}

Several directions could improve performance:
\begin{itemize}
    \item \textbf{Data augmentation}: Synthetic job title generation using LLMs
    \item \textbf{Multi-task learning}: Joint department-seniority prediction with shared representations
    \item \textbf{Active learning}: Iteratively labeling high-uncertainty samples to expand training data
    \item \textbf{Cross-lingual transfer}: Leveraging multilingual models for non-English CVs
\end{itemize}

\section{GenAI Usage Documentation}
\label{sec:genai-usage}

In accordance with project requirements, we document the use of Generative AI tools throughout this project.

\subsection{Tools Used}
\begin{itemize}
    \item \textbf{GitHub Copilot}: Code completion and boilerplate generation
    \item \textbf{Claude (Anthropic)}: Code review, debugging assistance, and documentation
    \item \textbf{ChatGPT}: Research questions and conceptual explanations
\end{itemize}

\subsection{Use Cases}
\begin{enumerate}
    \item \textbf{Code Generation}: Trainer class implementations (WeightedTrainer, FocalTrainer), evaluation utilities
    \item \textbf{Debugging}: Resolving dependency conflicts, fixing tensor shape mismatches
    \item \textbf{Documentation}: LaTeX formatting, report structure suggestions
    \item \textbf{Research}: Understanding Focal Loss, class imbalance strategies
\end{enumerate}

\subsection{Verification Process}
All AI-generated code was:
\begin{itemize}
    \item Reviewed for correctness and alignment with project goals
    \item Tested through unit tests and end-to-end notebook execution
    \item Validated against expected outputs and metrics
\end{itemize}

Complete GenAI usage documentation is available in the repository at \texttt{docs/genai\_usage.md}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The bibliography is inserted automatically
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\lhead{}
\printbibliography
\addcontentsline{toc}{section}{\bibname}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\section{Appendix A}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Affidavit must be adapted 
%% in declaration.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\input{declaration.tex}

\end{document}
