# GenAI Usage Documentation

This document records all uses of Generative AI tools during the project as required by the evaluation criteria.

---

## Session 1: Project Setup and Structure (2025-12-15)

### Tool Used
**Gemini Code Assist** (Google's AI coding assistant integrated in VS Code)

### Purpose
Initial project scaffolding, repository structure design, and model implementation templates.

### Prompts Used

#### Prompt 1: Understanding the Task
```
can you explain me how to build the repo structure for this task right here? 

Predicting Career Domain and Seniority from LinkedIn Profiles

Project Overview:
In this semester's capstone project, your task is to develop an end-to-end machine-learning 
pipeline that predicts (1) the current professional domain and (2) the current seniority level 
of an individual based solely on the information contained in their LinkedIn CV. Your models 
will be evaluated using a hand-labeled dataset provided by SnapAddy.

The project encourages you to creatively combine modern NLP techniques, programmatic labeling 
strategies, and supervised or zero-shot approaches to extract meaningful signals from 
semi-structured career data.

[Full project requirements including approaches, evaluation criteria, and deliverables were provided]
```

**AI Response Summary**: The AI analyzed the project requirements and proposed a comprehensive folder structure following ML best practices:
- `src/` for reusable Python modules (data loading, models, evaluation)
- `notebooks/` for experimental Jupyter notebooks
- `data/`, `models/`, `reports/` for artifacts
- Separation of concerns between model code and experimentation

#### Prompt 2: Implementation Request
```
yeah create the folder structure, would you then build the model as python files and 
try them against each other in notebooks or how would you tackle that structure?
```

**AI Response Summary**: The AI confirmed the recommended approach:
- **Models as Python modules** with clean APIs (`.fit()`, `.predict()`)
- **Notebooks for experimentation** that import from `src/`
- Created all directory structure and initial source files

### Files Generated by AI

| File | Purpose | Modifications Made |
|------|---------|-------------------|
| `src/data/loader.py` | JSON/CSV data loading utilities | None - used as generated |
| `src/data/preprocessor.py` | Text cleaning and feature extraction | None - used as generated |
| `src/models/rule_based.py` | Baseline string matching classifier | None - used as generated |
| `src/models/embedding_classifier.py` | Zero-shot sentence embedding classifier | None - used as generated |
| `src/models/feature_ml.py` | TF-IDF + traditional ML (LogReg, RF, SVM) | None - used as generated |
| `src/evaluation/metrics.py` | Accuracy, F1, confusion matrix utilities | None - used as generated |
| `src/utils.py` | Seed setting, train/test split helpers | None - used as generated |
| `requirements.txt` | Python dependencies | None - used as generated |
| `.gitignore` | Git ignore patterns for ML projects | None - used as generated |
| `README.md` | Project documentation | None - used as generated |
| `docs/genai_usage.md` | This documentation file | Continuously updated |

### Key Design Decisions Made by AI

1. **Modular Architecture**: Separate data, models, and evaluation into distinct packages
2. **Consistent Model API**: All classifiers follow sklearn-style `.fit()/.predict()` pattern
3. **Three Baseline Approaches**:
   - Rule-based (string matching against label lists)
   - Embedding-based (zero-shot with sentence-transformers)
   - TF-IDF + ML (logistic regression, random forest, SVM)
4. **Lazy Loading**: Embedding models loaded on first use to save memory
5. **Label Dictionary Support**: Models can use the provided CSV files as lookup tables

---

## Session 2: EDA Notebook Creation (2025-12-15)

### Tool Used
**Gemini Code Assist**

### Purpose
Create exploratory data analysis notebook to understand the dataset.

### Prompts Used

```
create the eda for me (don't forget to update the GenAI usage doc, do a more extensive 
documentation also for the first prompt i gave you, to let the grader see what we did)
```

**AI Response Summary**: Created comprehensive EDA notebook analyzing:
- Dataset structure and statistics
- Label distributions for department and seniority
- Text length analysis
- Multilingual content detection
- Position history patterns

### Files Generated
| File | Purpose |
|------|---------|
| `notebooks/01_eda.ipynb` | Exploratory data analysis |
| `docs/genai_usage.md` | Updated with detailed documentation |

---

## Session 3: Baseline Model Development (2026-01-05)

### Tool Used
**Gemini Code Assist**

### Purpose
Build and enhance rule-based and embedding-based baseline models with development notebooks for experimentation.

### Prompts Used

```
Can you create the Rule Based models as a baseline for me and the embedded based labeling 
to compare to? for my understanding it would make sense to outsource the models into their 
separate python files in my project and compare them in a big notebook i can then use for 
the submission with explanations between the model? would you suggest own notebooks for the 
models to build and improve them in their own regards? Do the in the task description 
suggested models need the same feature engineering or different feature engineering? 
give me suggestions and a plan before implementing pls.
```

**AI Response Summary**: The AI proposed a structured approach:
- Models in `src/models/*.py` with consistent APIs
- Separate development notebooks for each approach
- Final comparison notebook for submission (to be created later)
- Explained that each approach has different feature engineering requirements

```
proceed with this plan without creating the comparison notebook. this will follow in the end, 
when i've build all the different models. Use sequential thinking, build the models and 
create the development notebooks for me then to tinker in. Don't forget to update the gen Ai docs
```

### Files Modified

| File | Changes Made |
|------|-------------|
| `src/models/rule_based.py` | Added `HybridRuleClassifier` combining exact match, substring, keyword patterns, and defaults. Extended multilingual keyword dictionaries (German/French). Added `RuleConfig` dataclass and factory functions. |
| `src/models/embedding_classifier.py` | Changed default model to multilingual (`paraphrase-multilingual-MiniLM-L12-v2`). Added automatic GPU/CPU detection. Added `fit_from_examples()` for averaged embedding centroids. Added `predict_top_k()` for confidence analysis. |

### Files Created

| File | Purpose |
|------|---------|
| `notebooks/02_rule_based_baseline.ipynb` | Development notebook for rule-based approach with train/test split, hyperparameter tuning, and error analysis |
| `notebooks/03_embedding_zero_shot.ipynb` | Development notebook for embedding approach with strategy comparison, confidence analysis, and model comparison |

### Key Design Decisions

1. **Hybrid matching strategy**: Cascade from exact → substring → keyword → default
2. **Multilingual embeddings**: Using `paraphrase-multilingual-MiniLM-L12-v2` for German/French/English data
3. **Centroid approach**: Average multiple example embeddings per label for robustness
4. **Separate development notebooks**: Allow independent experimentation before final comparison

---

## Guidelines for Future Documentation

For each GenAI usage, document:
1. **Date**: When the tool was used
2. **Tool**: Which GenAI tool (ChatGPT, GitHub Copilot, Claude, Gemini, etc.)
3. **Purpose**: What task was the AI helping with
4. **Prompts**: Key prompts used (include full text when significant)
5. **Files Generated**: List all files created or modified
6. **Modifications**: Any changes made to AI-generated output
7. **Design Decisions**: Notable architectural choices made by the AI

---

## Session 8: Project Pivot - Data Leakage Detection & Fresh Start (2026-01-07)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Complete project restructuring after discovering data leakage from using `active_positions_processed.csv` (derived from annotated dataset) for training. Transition to zero-shot/semi-supervised learning approach using only lookup tables and unannotated data.

### Critical Discovery

The user identified that `active_positions_processed.csv` was extracted from `linkedin-cvs-annotated.json`, creating data leakage since the same data was used for both training and evaluation. This invalidated all previous model results.

### Prompts Used

```
It turns out that the annotated dataset is just for validation, not for training and test purposes. This makes me rethink this whole project, as I can use the linkedin-cvs-not-annotated, the active positions processed.csv, the department_v2 csv and the seniority v2.csv. Rethink the approach structure, how would you then go about it regarding the task.md which outlines the general task we have to do.
```

Follow-up clarification:
```
The active processed positions comes from the annotated dataset which would be data leakage. I deleted it as it came from ancient approaches.
```

Final decision:
```
I would like option B: get rid of the old models and notebooks and the old functions and then give me an implementation plan to start fresh (don't forget to update gen ai docs). Use sequential thinking.
```

### Sequential Thinking Process

The AI used structured reasoning to plan the complete project overhaul:

### Cleanup Actions Performed

| Action | Details |
|--------|---------|
| **Notebooks archived** | Moved all 7 notebooks (`01_eda.ipynb` through `07_tfidf_logreg_baseline.ipynb`) to `notebooks/_archive/` |
| **Models archived** | Moved all trained models (`.pkl` files and transformer directories) to `models/_archive/` |
| **Data leakage removed** | Deleted `active_positions_processed.csv` (already removed by user) |
| **Source code reviewed** | Assessed existing `src/` modules - most are reusable with proper data loading |

### New Project Constraints

**Available for Training:**
- `linkedin-cvs-not-annotated.json` - Unannotated LinkedIn CVs (no labels)
- `department-v2.csv` - ~10,147 labeled job titles → department mappings
- `seniority-v2.csv` - ~9,430 labeled job titles → seniority mappings

**Reserved for Validation ONLY:**
- `linkedin-cvs-annotated.json` - Hand-labeled by SnapAddy (MUST NOT be used for training)

### New Approach Strategy

This is now a **zero-shot/semi-supervised learning problem**:

1. **Rule-Based Baseline**: Exact + fuzzy matching against lookup tables
2. **Embedding Zero-Shot**: Semantic similarity using sentence transformers
3. **Fine-Tuned Transformers**: Train BERT/DistilBERT on lookup table data
4. **Pseudo-Labeling** (optional): Use best baseline to label unannotated CVs, then train
5. **Feature Engineering + ML**: Traditional ML on lookup tables
6. **LLM-Based** (optional): Few-shot with GPT/Claude

### Files Created

| File | Purpose |
|------|---------|
| `C:\Users\julie\.gemini\antigravity\brain\...\implementation_plan.md` | Complete fresh start implementation plan with phased approach |

### Key Design Decisions

1. **Archive, don't delete**: Old work moved to `_archive/` for reference, not lost
2. **Reuse data loaders**: Existing `loader.py` functions are correctly structured
3. **Phased implementation**: Start simple (rule-based) → advance to complex (transformers)
4. **Strict validation separation**: Annotated data never touched until final evaluation

### Next Steps

1. Create fresh notebooks starting with data exploration (no annotated data leakage)
2. Rebuild models training exclusively on lookup tables
3. Apply models to unannotated CVs for demonstration
4. Final validation on held-out annotated dataset

---

*This document is continuously updated throughout the project development.*

---

## Session 4: Advanced Model Implementation (2026-01-06)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Implement two advanced classification approaches: (1) Fine-tuned transformer model, and (2) Pseudo-labeling pipeline for semi-supervised learning.

### Prompts Used

```
use sequential thinking: can you build me implementation plans for these two?: Fine-tuned classification model. Use the csv files to fine-tune a pre-trained classification model. Apply the model to the linked-in data
Programmatic labeling + supervised learning: Use rule-based or embedding-based predictions to create pseudo-labels for a large set of LinkedIn profiles, then fine-tune a classifier on this expanded dataset.
```

### Files Modified

| File | Changes Made |
|------|-------------|
| `src/data/loader.py` | Fixed `prepare_dataset()` to extract labels from position level (`department`/`seniority`) instead of CV level |

### Files Created

| File | Purpose |
|------|---------|
| `src/models/transformer_classifier.py` | Transformer-based classifier using DistilBERT with two-stage training support |
| `src/data/pseudo_labeler.py` | Pseudo-labeling pipeline combining rule-based and embedding classifiers |
| `notebooks/04_transformer_classifier.ipynb` | Approach 1: Two-stage training (CSV patterns → LinkedIn adaptation) |
| `notebooks/05_pseudo_labeling_exp.ipynb` | Approach 2: Pseudo-labeling with gold + silver data combination |

### Key Design Decisions

1. **Two-stage training**: Pre-train on ~20K CSV patterns, then domain-adapt on LinkedIn CVs
2. **Confidence-based selection**: Trust rule-based for exact/substring matches, embedding for high-confidence (>0.85) predictions
3. **Sample weighting**: Gold data weighted at 1.0, silver data at 0.7 to account for label noise
4. **Prerequisite fix**: Corrected data loader to extract position-level labels as required by the JSON structure


---

## Session 5: Feature Engineering & Conventional ML (2026-01-06)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Implement feature engineering and conventional ML classifiers (TF-IDF + Logistic Regression, Combined Features + Random Forest) for interpretable baseline models.

### Prompts Used

```
Use Sequential thinking: can you give me an implementation plan for those two? 
Feature engineering and conventional machine learning. Look at the linked-In data 
and generate meaningful features (e.g. number of previous jobs as an indicator 
for seniority, etc.). Then train conventional algorithms (e.g. random forests) 
to predict the labels.
Simple interpretable baseline: E.g. a bag-of-words and TF–IDF + logistic 
regression classifier for domain or seniority.
```

Follow-up clarifications:
- Simplified features: Removed redundant `num_positions` (same as `num_previous_jobs + 1`)
- Added bilingual EN/DE keyword support for German CVs

### Files Modified

| File | Changes Made |
|------|-------------|
| `src/models/feature_ml.py` | Added `FeatureEngineer` class for LinkedIn-specific feature extraction, `CombinedFeatureClassifier` combining TF-IDF + structured features, bilingual keyword dictionaries for EN/DE seniority and department detection |

### Files Created

| File | Purpose |
|------|---------|
| `notebooks/06_feature_ml_baseline.ipynb` | Approach 5: Feature Engineering + Random Forest with career features |
| `notebooks/07_tfidf_logreg_baseline.ipynb` | Approach 6: TF-IDF + Logistic Regression interpretable baseline |

### Key Features Extracted

| Feature | Description |
|---------|-------------|
| `num_previous_jobs` | Count of past positions (seniority indicator) |
| `total_experience_months` | Time since earliest position start |
| `avg_tenure_months` | Average job duration |
| `num_companies` | Number of unique employers |
| `has_senior_keyword` | EN/DE: Senior, Lead, Leiter, etc. |
| `has_management_keyword` | EN/DE: Manager, Director, Geschäftsführer, etc. |
| `has_entry_keyword` | EN/DE: Junior, Trainee, Praktikant, etc. |
| `has_*_keyword` | Department-specific keyword flags (IT, HR, Finance, etc.) |

### Key Design Decisions

1. **Bilingual keywords**: Both English and German terms for seniority/department detection
2. **Career features from position history**: Leverages startDate/endDate for experience calculation
3. **Combined classifier**: Merges sparse TF-IDF matrix with dense structured features using scipy.sparse.hstack


---

## Session 6: Ensuring All Models Predict Both Targets (2026-01-06)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Audit and update all model notebooks to ensure they train classifiers for BOTH targets (department AND seniority), as some notebooks were only training for department.

### Prompts Used

```
Improving Transformer Classifier - ensure notebook 04_transformer_classifier.ipynb correctly 
predicts both department and seniority. Perform same audit for 05_pseudo_labeling_exp.ipynb.
```

### Files Modified

| File | Changes Made |
|------|-------------|
| `notebooks/04_transformer_classifier.ipynb` | **Complete rewrite**: Now trains TWO separate transformer classifiers (department + seniority) using same two-stage training approach. Previously only trained department. |
| `notebooks/05_pseudo_labeling_exp.ipynb` | **Complete rewrite**: Now performs pseudo-labeling for BOTH targets. Creates separate classifiers and evaluates both. Previously only did department. |

### Bug Fix

| File | Issue Fixed |
|------|-------------|
| `notebooks/06_feature_ml_baseline.ipynb` | Fixed index mismatch bug: `train_idx`/`test_idx` from filtered `df` were incorrectly used to index into unfiltered `cvs_annotated`. Now uses `cv_id` column for correct mapping. |

### Key Changes

1. **Notebook 04**: Added sections 5-7 for seniority two-stage training (Stage 1: CSV patterns, Stage 2: LinkedIn adaptation)
2. **Notebook 05**: Added sections 6-9 for seniority pseudo-labeling (separate classifiers, pseudo-label generation, training, evaluation)
3. **Notebook 06**: Fixed critical indexing bug where filtered DataFrame indices didn't match original CV list positions

### Verification

All 6 model notebooks now predict **both** targets:
- ✅ `02_rule_based_baseline.ipynb`
- ✅ `03_embedding_zero_shot.ipynb`
- ✅ `04_transformer_classifier.ipynb` (fixed)
- ✅ `05_pseudo_labeling_exp.ipynb` (fixed)
- ✅ `06_feature_ml_baseline.ipynb` (bug fix)
- ✅ `07_tfidf_logreg_baseline.ipynb`

---

## Session 7: Dataset Usage Restructuring (2026-01-07)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Restructure data loading to clearly separate unannotated data (for inference/demonstration) from annotated data (for evaluation/metrics).

### Prompts Used

```
Restructure the rule based and embedding model. Use the linkedin-cvs-not-annotated for the inference in action and only use the annotated ones for the evaluation. For that, modify the loader.py to load either of them. Each model/notebook should work in its own regard and bubble. Use the sequential thinking.
```

### Files Modified

| File | Changes Made |
|------|-------------|
| `src/data/loader.py` | Added `load_inference_dataset()` and `load_evaluation_dataset()` to target specific JSON files. |
| `notebooks/02_rule_based_baseline.ipynb` | Updated to use `load_inference_dataset` for demos and `load_evaluation_dataset` for metrics calculation. |
| `notebooks/03_embedding_zero_shot.ipynb` | Updated to use separate datasets for inference and evaluation. |

### Key Design Decisions

1. **Centralized Data Loading**: Moved the logic for selecting between `linkedin-cvs-not-annotated.json` and `linkedin-cvs-annotated.json` into `loader.py` for consistency.
2. **Inference/Evaluation Separation**: Ensured that model demonstrations work on raw, unannotated data, while metrics are strictly calculated against ground truth from annotated data.
3. **Automated Notebook Patching**: Used a Python script to safely modify `.ipynb` files, ensuring clean replacement of loading logic while maintaining notebook structure.

---

## Session 9: Fresh Start Implementation with All Approaches (2026-01-07)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Following the data leakage cleanup (Session 8), implement a complete fresh start with all 6 recommended approaches from the project task description, each as a self-contained notebook with full evaluation and results saving.

### Prompts Used

```
proceed with the fresh_start_plan.md

proceed by building rule based baseline

no proceed by building embedding-based labeling (notebook 03)

can you now build Fine-tune transformer (BERT/DistilBERT) on lookup tables

can you now build TF-IDF + traditional ML (Logistic Regression, Random Forest)

Implement approach 4 (pseudo-labeling)?

can you do that and restructure the numbering of the notebooks to fit the task description? Feature engineering + conventional machine learning

recheck pls, there are more notebooks now. also check the repo for other legacy things to delete like results and so on
```

### Notebooks Created

| Notebook | Approach | Description |
|----------|----------|-------------|
| `01_data_exploration.ipynb` | EDA | Analyzes lookup tables and unannotated CVs. **Does not** load annotated data. |
| `02_rule_based_baseline.ipynb` | Approach 1: Rule-based | HybridRuleClassifier with exact + fuzzy + keyword matching on lookup tables. |
| `03_embedding_baseline.ipynb` | Approach 2: Embedding zero-shot | Multilingual sentence-transformers for semantic similarity matching. |
| `04_transformer_on_lookups.ipynb` | Approach 3: Fine-tuned transformer | DistilBERT fine-tuned on lookup tables (80/20 split). |
| `05_pseudo_labeling.ipynb` | Approach 4: Programmatic labeling | Uses embedding classifier to pseudo-label unannotated CVs, trains transformer on gold + silver data. |
| `06_feature_engineering.ipynb` | Approach 5: Feature engineering | Hand-crafted career features + TF-IDF + Random Forest. **Trains on annotated data** (supervised, not zero-shot). |
| `07_tfidf_logreg.ipynb` | Approach 6: Interpretable baseline | TF-IDF + Logistic Regression with feature importance analysis. |

### Self-Contained Notebook Philosophy

Each notebook (02-07) follows this structure:
1. **Load Training Data**: Lookup tables (or annotated CVs for notebook 06)
2. **Build Model**: Train classifier(s) for department AND seniority
3. **Inference Demo**: Apply to unannotated LinkedIn CVs with examples
4. **Evaluation**: ⚠️ Load annotated dataset for the **first time** and calculate metrics
5. **Save Results**: Export to `notebooks/results/[approach]_results.json`

### Key Design Decisions

1. **Data Leakage Prevention**:
   - Annotated CVs (`linkedin-cvs-annotated.json`) loaded ONLY for evaluation
   - Exception: Notebook 06 (feature engineering) trains on annotated data due to need for career history

2. **Results Persistence**:
   - Each notebook saves JSON results to `notebooks/results/`
   - Final comparison notebook (99) will load all JSON files
   - No need to re-run expensive evaluations

3. **Zero-Shot Setup**:
   - Notebooks 02-05, 07: Train on lookup tables, evaluate on LinkedIn CVs
   - Notebook 06: Supervised learning on annotated CVs (for comparison)

4. **Reusable Source Code**:
   - All `src/` modules verified as well-designed and **not** assuming annotated CV training
   - `rule_based.py`, `embedding_classifier.py`, `transformer_classifier.py` are generic
   - Only notebooks needed cleanup, not the model implementations

### Files Modified/Created

| File | Action | Purpose |
|------|--------|---------|
| `notebooks/01_data_exploration.ipynb` | Created | EDA without touching annotated data |
| `notebooks/02_rule_based_baseline.ipynb` | Created | Rule-based approach with RuleConfig fix |
| `notebooks/03_embedding_baseline.ipynb` | Created | Embedding zero-shot with confidence analysis |
| `notebooks/04_transformer_on_lookups.ipynb` | Created | Transformer fine-tuning on lookup tables |
| `notebooks/05_pseudo_labeling.ipynb` | Created | Semi-supervised with pseudo-labels |
| `notebooks/06_feature_engineering.ipynb` | Created | Career features + Random Forest (supervised) |
| `notebooks/07_tfidf_logreg.ipynb` | Created | TF-IDF + LogReg (moved from 05) |
| `notebooks/results/README.md` | Created | Documents JSON result format |
| `notebooks/README.md` | Created | Documents notebook structure and design |

### Cleanup Actions

1. **Removed Duplicates**:
   - Deleted `notebooks/05_tfidf_logreg.ipynb` (duplicate, kept 07)
   - Deleted `notebooks/06_pseudo_labeling.ipynb` (duplicate, kept 05)
   - Deleted `test_notebook.ipynb` from project root

2. **Cleaned Results**:
   - Removed 62 old result files from `notebooks/results/`
   - Fresh directory ready for new runs

3. **Archived Old Work**:
   - All previous notebooks preserved in `notebooks/_archive/`
   - Old models preserved in `models/_archive/`

### Results JSON Format

Each notebook saves results in this standardized format:

```json
{
  "approach": "Approach Name",
  "department": {
    "accuracy": 0.XX,
    "precision": 0.XX,
    "recall": 0.XX,
    "f1_macro": 0.XX,
    "f1_weighted": 0.XX,
    "per_class_f1": {"class1": 0.XX, ...}
  },
  "seniority": {
    "accuracy": 0.XX,
    "precision": 0.XX,
    "recall": 0.XX,
    "f1_macro": 0.XX,
    "f1_weighted": 0.XX,
    "per_class_f1": {"Junior": 0.XX, ...}
  },
  "metadata": {
    "training_samples": 19000,
    "hyperparameters": {...}
  },
  "timestamp": "2026-01-07T11:00:00"
}
```

### Next Steps

1. Run each notebook (02-07) to generate results
2. Create `99_final_comparison.ipynb` to load all results and create visualizations
3. Compare zero-shot approaches (02-05, 07) vs supervised (06)
4. Document findings in final report

---

## Session 10: Model Evaluation and Repository Optimization (2026-01-07)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Update project configuration to exclude model artifacts from version control and evaluate the feature engineering baseline model.

### Prompts Used

```
Update the .gitignore file to include the models/ directory.

do we neeed to update the gen ai docs?
```

### Files Modified

| File | Changes Made |
|------|-------------|
| `.gitignore` | Added `models/` to prevent tracking of large model artifacts and trained weights. |
| `notebooks/06_feature_engineering.ipynb` | Executed evaluation cells, generated confusion matrices, and saved results. |
| `docs/genai_usage.md` | Updated with Session 10 details. |

### Key Actions & Design Decisions

1. **Gitignore update**: Proactively excluded `models/` directory to maintain a clean repository and comply with best practices for handling ML artifacts.
2. **Model Evaluation Execution**:
   - Executed the `06_feature_engineering.ipynb` notebook end-to-end.
   - Generated **Seniority Confusion Matrix** and classification reports for both Department and Seniority.
   - Results saved to `results/feature_engineering_results.json`.
3. **Performance Metrics**:
   - **Department**: Accuracy: 0.1695, F1 (macro): 0.1425
   - **Seniority**: Accuracy: 0.3285, F1 (macro): 0.2498

---

## Session 11: Tier 1 Model Improvements (2026-01-12)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Implement Tier 1 improvements to address model performance issues identified through systematic analysis:
1. Fix encoding issues (mojibake) in lookup tables
2. Add deduplication to prevent embedding centroid collapse
3. Add class-weighted loss to Transformer for handling class imbalance

### Analysis Approach
Used sequential thinking MCP tool to systematically analyze improvement opportunities:
- Language matching potential
- Training data extraction issues
- Embedding model improvements
- Feature engineering gaps
- Pseudo-labeling improvements
- Domain adaptation strategies

---

## Session 12: Data Usage Compliance & Refactoring (2026-01-12)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Ensure all models and notebooks comply with strict data usage instructions:
1. Train exclusively on CSV lookup files.
2. Use train/test split (80/20) for in-distribution evaluation on CSVs.
3. Use JSON files solely for out-of-distribution, "real-world" inference evaluation.
4. Refactor Notebook 06 (Feature Engineering) which was incorrectly training on JSON data.
5. Harmonize all notebooks (02-07) to report both In-Distribution and Real-World metrics.

### Prompts Used

```
The data usage has been specified further:
1. All models MUST be trained on the csv lookup files. These will have to be split in a train and test set to have an in-distibution evaluation.
2. The json files (labeled and unlabeled) are only for measuring the inference-time accuracy / real world accuracy on unseen, out-of-distibution data. 
3. This means that Notebook 06 should be refactored as well to comply with this instruction. 
4. All notebooks (02-07) should clearly report the in-distibution (test set of the csv training set) and the real world (annotated json files) accuracy. 

Develop an implementation plan for that and once it's approved implement it. 
```

### Files Modified

| File | Changes Made |
|------|-------------|
| `notebooks/06_feature_engineering.ipynb` | **Complete Rebuild**: Switched training data from JSON CVs to CSV lookup tables. Added 80/20 split for in-distribution evaluation. Implemented feature extraction that handles CSV job titles (by setting history features to dummy values) while preserving full metadata extraction for JSON inference. |
| `notebooks/02_rule_based_baseline.ipynb` | Added CSV train/test split. Initialized model on train split and evaluated on test split. Harmonized reporting. |
| `notebooks/03_embedding_baseline.ipynb` | Added CSV train/test split. Built centroids on train split and evaluated on test split. Harmonized reporting. |
| `notebooks/04_transformer_on_lookups.ipynb` | Updated labels in final reporting cell to clearly distinguish between "In-Distribution (CSV)" and "Real-World (JSON)". |
| `notebooks/05_pseudo_labeling.ipynb` | Updated labels in final reporting cell and clarified that "Gold" training set is the CSV training data. |
| `notebooks/07_tfidf_logreg.ipynb` | Updated labels in final reporting cell to clearly distinguish between "In-Distribution (CSV)" and "Real-World (JSON)". |

### Key Design Decisions

1. **Notebook 06 Adaptation**: Since CSVs lacks career history, features like `num_previous_jobs` are set to `0` during training. During inference on JSON CVs, the model uses the real history from the CVs. This ensures a consistent feature vector while respecting the "train on CSV only" rule.

2. **Dual Perspective Reporting**: Every notebook now explicitly reports:
   - **In-Distribution (CSV)**: How well the model generalizes to unseen job titles that match the training data distribution.
   - **Real-World (JSON)**: How well the model generalizes to completely unseen CVs from a different source (LinkedIn).

3. **Robust Rebuilding**: Notebook 06 was rebuilt from scratch to ensure no legacy cells from the previous JSON-training approach remained, preventing potential confusion or leakage.

### Verification

All notebooks (02-07) now clearly differentiate between the two evaluation regimes. Notebooks 04, 06, and 07 have been upgraded to **semi-supervised** training, successfully integrating 314 unannotated LinkedIn positions as "Silver" data to bridge the domain gap.

#### Unannotated Data Integration (Semi-Supervised Pivot)
Following user clarification, I implemented a robust pipeline to leverage the unannotated JSON for training:

1. **Pseudo-Labeling Pipeline**: Created `scripts/generate_pseudo_labels.py` which uses an ensemble of Rules + Embeddings (threshold > 0.8) to assign silver labels to the unannotated LinkedIn CVs.
2. **Feature Engineering (NB 06)**: Now trains on **Gold (CSV) + Silver (Pseudo-labeled JSON)**. This allows the Random Forest to learn from real LinkedIn career history features during the training phase.
3. **Transformer & LogReg (NB 04/07)**: Now train on the combined Gold + Silver dataset, improving domain adaptation to the LinkedIn text style.


### Improvement Opportunities Identified (Moving Forward)

**Tier 2 (Medium Effort - Highly Viable)**:
- **German Compound Word Splitting**: Essential for German job titles (e.g., *Softwareentwickler*).
- **Language Detection Feature**: Adding an `is_german` flag to help models use language-specific features.
- **Career History Refinement**: Extending the "Dummy History" pattern to more features like tenure and company diversity for a better real-world inference boost.
- **Confidence Calibration**: Implementing Platt Scaling or Isotonic Regression for the embedding and transformer models to improve ensembling.

**Tier 3 (High Effort - Advanced Adaptation)**:
- **SetFit Fine-tuning**: Training a sentence transformer on CSV pairs (Contrastive Learning) for better zero-shot/few-shot performance.
- **LLM-based Data Augmentation**: Using LLMs to generate LinkedIn-style variations of CSV titles to bridge the distribution gap.
- **Multi-round Pseudo-labeling**: Iteratively labeling unannotated data to improve the "Silver" training set in Notebook 05.
- **Ensemble Meta-classifier**: Training a model to weigh predictions from all 6 approaches.

---

## Session 13: Tier 1 Data Balancing (2026-01-12)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Implement data balancing to address severe class imbalance in the training data (e.g., HR has 31 samples vs. Marketing with 4,295).

### Files Modified

| File | Changes Made |
|------|-------------|
| `src/data/loader.py` | Added `balance_dataset()` function which oversamples minority classes (via duplication) to `min_samples` (default 500) and undersamples majority classes to `max_samples` (default 2,000). Also returns sample weights for use in `.fit()`. |
| `notebooks/04_transformer_on_lookups.ipynb` | Patched to apply `balance_dataset()` after the train/test split. |
| `notebooks/06_feature_engineering.ipynb` | Patched to apply `balance_dataset()` to the combined Gold + Silver training set. |
| `notebooks/07_tfidf_logreg.ipynb` | Patched to apply `balance_dataset()` after the train/test split. |

### Key Design Decisions

1. **Oversampling via Duplication**: Chose simple duplication over SMOTE because text data does not benefit from "interpolating" between samples. Duplicated samples are given a lower weight (0.8) to prevent overfitting.
2. **Undersampling with Retention**: Instead of capping at 500 (losing 3,795 Marketing samples), we now keep up to 2,000 samples from majority classes. This preserves vocabulary diversity.
3. **Sample Weights**: The function returns a weights list that can be passed to `model.fit(sample_weight=...)` for classifiers that support it.

### Verification

The `balance_dataset()` function was tested successfully:
- **Before**: Human Resources had 31 samples, Marketing had 4,295.
- **After**: Human Resources has 500 samples (oversampled), Marketing has 2,000 (undersampled).

---

## Session 14: Career Feature Extraction Bugfix (2026-01-14)

### Tool Used
**Antigravity** (Google DeepMind's agentic coding assistant)

### Purpose
Debug and fix a row count mismatch error in `06_feature_engineering.ipynb` where career features (461 rows) didn't align with evaluation data (478 rows).

### Problem Identified

The `CareerFeatureExtractor.extract_features()` method was skipping some CVs when date parsing failed, resulting in fewer extracted features (461) than evaluation rows (478). The notebook incorrectly assumed row alignment via index position.

### Files Modified

| File | Changes Made |
|------|-------------|
| `src/models/feature_ml.py` | Added `return_cv_ids` parameter to `extract_features()`. When True, returns a 'cv_id' column containing the original CV index for proper merging. |
| `notebooks/06_feature_engineering.ipynb` | Rewrote cell 4 to merge on `cv_id` instead of assuming row alignment. Rebuilt eval_df with cv_id tracking. CVs with no parseable career dates get 0-filled features. |

### Key Design Decisions

1. **cv_id Tracking**: The extractor now tracks which CV index each row came from, enabling proper left-join merging.
2. **Left Join Merge**: Keep all eval rows, fill missing career features with 0 for CVs that couldn't be parsed.
3. **Backward Compatibility**: `return_cv_ids=False` by default to avoid breaking existing code.

### Verification

Successfully executed notebook end-to-end:
- **Department (no "Other")**: 61.0% accuracy
- **Seniority**: 47.5% accuracy (baseline), 35.1% with career adjustment (needs tuning)

