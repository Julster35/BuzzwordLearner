\subsection{Embedding-Based Baseline}
\label{sec:embedding-baseline}

This section presents a zero-shot embedding-based classification approach using sentence transformers for department and seniority prediction. Unlike the rule-based baseline that relies on exact pattern matching, this approach leverages semantic similarity to generalize beyond known job titles.

\subsubsection{Methodology}
\label{subsec:embedding-methodology}

The embedding-based classifier employs pre-trained multilingual sentence transformers to encode job titles and labels into dense vector representations. Classification is performed through cosine similarity matching between input embeddings and label prototype embeddings.

\paragraph{Model Selection}
The approach utilizes the \texttt{paraphrase-multilingual-MiniLM-L12-v2} sentence transformer model:
\begin{itemize}
    \item \textbf{Embedding dimension}: 384-dimensional dense vectors
    \item \textbf{Multilingual support}: Covers 50+ languages including English, German, French, and Spanish
    \item \textbf{Model size}: 118M parameters (~470MB)
\end{itemize}

\paragraph{Input Text Enrichment}
To maximize semantic information, input text is enriched beyond simple job titles by combining title, company, and job description (first 200 characters). This rich context provides additional signals for disambiguation.

\paragraph{Multi-Prototype Label Representations}
Instead of using single prototype embeddings per label, the approach employs K-Means clustering (k=3) to create multiple prototypes per label. This strategy captures intra-label diversity, accommodating different semantic variations within the same category (e.g., "Software Engineer" and "Backend Developer" both belong to IT but have different embeddings).

\paragraph{Prediction Process}
Classification uses the maximum cosine similarity across all prototypes for each label. The predicted label is the one with the highest similarity score.

\subsubsection{Performance Results}
\label{subsec:embedding-results}

The embedding-based model was evaluated on the same annotated LinkedIn CV dataset used for the rule-based baseline:

\begin{table}[h]
\centering
\caption{Embedding-based baseline performance}
\label{tab:embedding-performance}
\begin{tabular}{lcc}
\toprule
\textbf{Task} & \textbf{Accuracy} & \textbf{F1 (macro)} \\
\midrule
Department & 0.451 & 0.301 \\
Seniority & 0.458 & 0.360 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Underperformance vs. rule-based}: Department accuracy (45.1\%) and seniority accuracy (45.8\%) are lower than the rule-based baseline (76.6\% and 59.8\% respectively)
    \item \textbf{Better than naive baseline}: Still exceeds naive baselines (Department: 55\% for "Other", Seniority: 35\% for "Professional")
    \item \textbf{Semantic understanding limitations}: Pre-trained embeddings struggle to capture domain-specific job title semantics
    \item \textbf{Generalization potential}: Unlike rule-based matching, embedding approach can handle completely novel job titles through semantic similarity
\end{itemize}

\subsubsection{Error Analysis}
\label{subsec:embedding-errors}

Analysis of misclassifications reveals systematic failure patterns:

\paragraph{Semantic Ambiguity}
The model struggles with titles that are semantically similar across different departments. For example, "Project Manager" could belong to IT, Consulting, or Project Management, and "Analyst" appears across Finance, IT, Marketing, and Consulting.

\paragraph{Multilingual Performance}
Despite using a multilingual model, English job titles achieve higher accuracy (~50\%) than German/French/Spanish titles (~40\%), indicating cross-lingual similarity challenges.

\paragraph{Surface Similarity Issues}
The model sometimes matches on word overlap rather than contextual understanding. For example, "Senior Engineer" might be incorrectly classified based on the word "Senior" rather than the full context.

\subsubsection{Comparison with Rule-Based Baseline}
\label{subsec:embedding-vs-rule}

\begin{table}[h]
\centering
\caption{Embedding vs. Rule-Based performance comparison}
\label{tab:embedding-vs-rule}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Rule-Based} & \textbf{Embedding} & \textbf{Difference} \\
\midrule
Department Accuracy & 0.766 & 0.451 & -0.315 \\
Department F1 (macro) & 0.612 & 0.301 & -0.311 \\
Seniority Accuracy & 0.598 & 0.458 & -0.140 \\
Seniority F1 (macro) & 0.539 & 0.360 & -0.179 \\
\bottomrule
\end{tabular}
\end{table}

The substantial performance gap indicates that pre-trained general-purpose embeddings are insufficient for domain-specific classification. Exact/fuzzy matching of known patterns (rule-based) outperforms semantic similarity (embedding) in this task.

\subsubsection{Advantages and Limitations}
\label{subsec:embedding-pros-cons}

\paragraph{Advantages}
\begin{itemize}
    \item \textbf{Zero-shot capability}: Can classify completely novel job titles without retraining
    \item \textbf{Multilingual support}: Handles multiple languages with single model
    \item \textbf{Semantic generalization}: Captures synonyms and related terms automatically
    \item \textbf{No manual rule maintenance}: Unlike keyword lists, embeddings adapt automatically
\end{itemize}

\paragraph{Limitations}
\begin{itemize}
    \item \textbf{Domain mismatch}: General-purpose embeddings poorly aligned with job title semantics
    \item \textbf{Lower accuracy}: Significant underperformance vs. rule-based matching
    \item \textbf{Computational cost}: 50-200x slower than rule-based inference
    \item \textbf{Higher memory requirements}: ~500MB model size vs. ~5MB for lookup tables
\end{itemize}

\subsubsection{Future Improvements}
\label{subsec:embedding-improvements}

Several enhancement opportunities were identified:

\paragraph{Fine-Tuning on Job Title Data}
Domain adaptation through fine-tuning could significantly improve performance by training embeddings to maximize similarity within label classes and better distinguish between different job title categories.

\paragraph{Hybrid Rule-Embedding Approach}
Combining strengths of both methods by using rule-based matching for high-confidence exact matches and applying embedding similarity for novel/ambiguous titles. Expected performance would fall between individual baselines (50-60\% accuracy).

\paragraph{Transformer-Based Classification}
Moving beyond embeddings to fine-tuned BERT/DistilBERT models with classification heads instead of similarity matching could provide 10-20\% accuracy improvement over the zero-shot embedding approach.

These enhancements form the foundation for subsequent experiments exploring fine-tuned transformer models and hybrid approaches in the following sections.

